{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa54415",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/aw/anaconda3/lib/python3.11/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /home/aw/anaconda3/lib/python3.11/site-packages/openmm/../../../libOpenMM.so.8.0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_505/442843799.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mopenmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenmm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openmm/__init__.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_openmm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mopenmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenmm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVec3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtsintegrator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMTSIntegrator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMTSLangevinIntegrator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAMDIntegrator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAMDForceGroupIntegrator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDualAMDIntegrator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openmm/openmm.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion_info\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_swig_python_version_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Import the low-level C/C++ module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__package__\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\".\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_openmm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0m_openmm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: /home/aw/anaconda3/lib/python3.11/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /home/aw/anaconda3/lib/python3.11/site-packages/openmm/../../../libOpenMM.so.8.0)"
     ]
    }
   ],
   "source": [
    "from openmm.app import *\n",
    "from openmm import *\n",
    "from openmm.unit import *\n",
    "\n",
    "print('Loading...')\n",
    "pdb = PDBFile('./data_sets/AF-A0A2W7I3V2-F1-model_v4.pdb')\n",
    "forcefield = ForceField('amber99sb.xml', 'tip3p.xml')\n",
    "modeller = Modeller(pdb.topology, pdb.positions)\n",
    "print('Adding hydrogens...')\n",
    "modeller.addHydrogens(forcefield)\n",
    "print('Adding solvent...')\n",
    "modeller.addSolvent(forcefield, model='tip3p', padding=1*nanometer)\n",
    "print('Minimizing...')\n",
    "system = forcefield.createSystem(modeller.topology, nonbondedMethod=PME)\n",
    "integrator = VerletIntegrator(0.501*picoseconds)\n",
    "simulation = Simulation(modeller.topology, system, integrator)\n",
    "simulation.context.setPositions(modeller.positions)\n",
    "simulation.minimizeEnergy(maxIterations=100)\n",
    "print('Saving...')\n",
    "positions = simulation.context.getState(getPositions=True).getPositions()\n",
    "PDBFile.writeFile(simulation.topology, positions, open('output.pdb', 'w'))\n",
    "print('Done')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#https://openstructure.org/promod3/3.3/gettingstarted/\n",
    "#http://njohner.github.io/ost_pymodules/align_traj_on_density.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a05244e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying RCSB Search using the following parameters:\n",
      " {\"query\": {\"type\": \"group\", \"logical_operator\": \"and\", \"nodes\": [{\"type\": \"group\", \"logical_operator\": \"or\", \"nodes\": [{\"type\": \"terminal\", \"service\": \"text\", \"parameters\": {\"attribute\": \"rcsb_entity_source_organism.taxonomy_lineage.name\", \"operator\": \"exact_match\", \"value\": \"LX95_01242\"}}, {\"type\": \"terminal\", \"service\": \"text\", \"parameters\": {\"attribute\": \"rcsb_entity_source_organism.taxonomy_lineage.name\", \"operator\": \"exact_match\", \"value\": \"Homo sapiens\"}}]}, {\"type\": \"terminal\", \"service\": \"text\", \"parameters\": {\"operator\": \"greater\", \"attribute\": \"rcsb_entry_info.resolution_combined\", \"value\": 4}}]}, \"request_options\": {\"return_all_hits\": true}, \"return_type\": \"entry\"} \n",
      "\n",
      "\n",
      " ['1AOS', '1CC0', '1D3E', '1D3I', '1DGI', '1H3Y', '1J89', '1JEW', '1JL4', '1M11']\n"
     ]
    }
   ],
   "source": [
    "#blue biotech\n",
    "from sgrna_designer.design import design_sgrna_tiling_library\n",
    "\n",
    "target_transcripts = ['ENST00000381577', 'ENST00000644969'] # [PDL1, BRAF]\n",
    "#how do you share a datamodel between python and javascript\n",
    "#goal increase biofuel yield of algae -> make this super easy\n",
    "#as a user/scientist/child - i would like to buy algae, plants and so on that are slightly modified in cool ways\n",
    "#as a scientist - i would like to modify genes in as custom a way as possible to support industrial workflows in blue biotech = algae + fish\n",
    "\n",
    "#purple popcorn\n",
    "#purple trees\n",
    "#better algae\n",
    "\n",
    "#LX95_01242\n",
    "#for each organism\n",
    "#list a dropdrown of changes\n",
    "genes = {\n",
    "    'algae': {'property': 'glowing', 'name': 'asdfasdf'}\n",
    "}\n",
    "#data base for sequences for each organism\n",
    "#each organism has\n",
    "    # genome\n",
    "    # potential edits - genes\n",
    "    #lots of this seems to be just lots of joins - like AF - 500gb of genetic databases\n",
    "    #some of this seems to be \"computational chemistry??\" - how different molecules fit together \n",
    "    #other part would be good ui + random other shit thats probably easy to find through lots of iteration and talking to people\n",
    "def queryDataBase(gene):\n",
    "    gene = 'PZW41561.1'\n",
    "    url = 'https://www.ebi.ac.uk/ebisearch/search?query=${gene}&requestFrom=ebi_index&db=allebi'\n",
    "    search_results = request(url)\n",
    "    page = request(search_results[0])\n",
    "    fastas = request(page.find('link:href=\"fasta\"'))\n",
    "    return fasta\n",
    "\n",
    "def findCorrectPlaceToPutGeneInNucleus():\n",
    "    #use a libary \n",
    "    #do sequence alignmnet\n",
    "    #just put it at the end of two sequences between the telomereres in a gene that isnt activated\n",
    "    #or replace with a gene you dont want\n",
    "    #use some sort of routing building block viusalization on the client \n",
    "    #let people configure and play with different DNA configurations as if they were legos\n",
    "    #3d viuslaizatioon of all different possibiliies\n",
    "    #show different results of dna\n",
    "    pass \n",
    "\n",
    "def designSGRNA(prevSeq, nextSeq):\n",
    "    pass\n",
    "#using the existing genome on the organism, find a good place to put the gene - based on genomics data and some sort of fancy simulation                                              \n",
    "#TBD - add a prompt to synthesize or look up a protein from another organism and use an LLM to magic that\n",
    "#given a protein or enzyme\n",
    "#look up the gene\n",
    "#with the gene, look up the fasta\n",
    "#knowing what the intended new structure is of the genome, design an SGRNA to clip and modify the genome of the algae\n",
    "pdb = PDBFile('./data_sets/AF-A0A2W7I3V2-F1-model_v4.pdb')\n",
    "\n",
    "#LX95_01242\n",
    "#https://www.ebi.ac.uk/ena/browser/api/fasta/PZW41561.1?download=true\n",
    "from pypdb.clients.search.search_client import perform_search_with_graph\n",
    "from pypdb.clients.search.search_client import ReturnType\n",
    "from pypdb.clients.search.search_client import QueryGroup, LogicalOperator\n",
    "from pypdb.clients.search.operators import text_operators\n",
    "\n",
    "# SearchOperator associated with structures with under 4 Angstroms of resolution\n",
    "under_4A_resolution_operator = text_operators.ComparisonOperator(\n",
    "       value=4,\n",
    "       attribute=\"rcsb_entry_info.resolution_combined\",\n",
    "       comparison_type=text_operators.ComparisonType.GREATER)\n",
    "\n",
    "# SearchOperator associated with entities containing 'Mus musculus' lineage\n",
    "is_mus_operator = text_operators.ExactMatchOperator(\n",
    "            value=\"LX95_01242\",\n",
    "            attribute=\"rcsb_entity_source_organism.taxonomy_lineage.name\")\n",
    "\n",
    "# SearchOperator associated with entities containing 'Homo sapiens' lineage\n",
    "is_human_operator = text_operators.ExactMatchOperator(\n",
    "            value=\"Homo sapiens\",\n",
    "            attribute=\"rcsb_entity_source_organism.taxonomy_lineage.name\")\n",
    "\n",
    "# QueryGroup associated with being either human or `Mus musculus`\n",
    "is_human_or_mus_group = QueryGroup(\n",
    "    queries = [is_mus_operator, is_human_operator],\n",
    "    logical_operator = LogicalOperator.OR\n",
    ")\n",
    "\n",
    "# QueryGroup associated with being ((Human OR Mus) AND (Under 4 Angstroms))\n",
    "is_under_4A_and_human_or_mus_group = QueryGroup(\n",
    "    queries = [is_human_or_mus_group, under_4A_resolution_operator],\n",
    "    logical_operator = LogicalOperator.AND\n",
    ")\n",
    "\n",
    "return_type = ReturnType.ENTRY\n",
    "\n",
    "results = perform_search_with_graph(\n",
    "  query_object=is_under_4A_and_human_or_mus_group,\n",
    "  return_type=return_type)\n",
    "print(\"\\n\", results[:10]) # Huzzah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a19c8643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "Adding hydrogens...\n",
      "Adding solvent...\n",
      "Minimizing...\n",
      "Saving...\n",
      "Done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e67765b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Topology; 3 chains, 13897 residues, 43554 atoms, 29825 bonds>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation.topology\n",
    "modeller.topology\n",
    "#pdb.topology\n",
    "# import MDAnalysis as mda\n",
    "# u = mda.Universe('./templates/vite-project/public/graphite.pdb')\n",
    "# from py3dmol import vizinterfaces as p3d\n",
    "# viz = p3d.MdaViz(u)\n",
    "# viz.make_animation()\n",
    "# viz.animate()\n",
    "# protein = u.select_atoms('not resname HOH')\n",
    "# pviz = p3d.MdaViz(protein,width='400px',height='400px')\n",
    "# pviz.make_animation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d685eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8748d9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269787e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c952b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9277ef2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b286584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request  # import main Flask class and request object\n",
    "import random\n",
    "app1 = Flask(__name__)  # create the Flask app\n",
    "from flask import jsonify\n",
    "from Bio import SeqIO\n",
    "\n",
    "filename = 'maize_pseudohap.fasta'\n",
    "filename = 'symA3_37.fasta'\n",
    "\n",
    "import json\n",
    "seq = []\n",
    "def hasGenome(filename):\n",
    "    count = 0\n",
    "    with open(filename, \"r\") as fasta_file:\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            if count > 10: break\n",
    "            #print(record.id)   # Prints the sequence header (identifier)\n",
    "            seq.append(str(record.seq))  # Prints the sequence\n",
    "            count += 1\n",
    "        with open('/home/awahab/llm-testing/templates/vite-project/public/kmers.json', 'w') as writable:\n",
    "            json.dump(seq, writable)\n",
    "            writable.close()\n",
    "        print('writing to file ' + './templates/vite-project/public/kmers.json')\n",
    "hasGenome(filename)\n",
    "from Bio import Align\n",
    "aligner = Align.PairwiseAligner()\n",
    "print(aligner)\n",
    "\n",
    "from Bio.Align.Applications import ClustalwCommandline\n",
    "alignments = pairwise2.align.globalxx(seq1, seq2) \n",
    "\n",
    "\n",
    "align = AlignIO.read(\"/path/to/biopython/sample/opuntia.aln\", \"clustal\")\n",
    "print(align)\n",
    "# @app1.route('/status', methods=['GET'])\n",
    "# def a_live():\n",
    "#     print('shit', hasGenome(filename)[0])\n",
    "#     return jsonify(hasGenome(filename))\n",
    "\n",
    "# @app1.route('/predict', methods=['GET'])\n",
    "# def predict():\n",
    "#     demo=random.randint(2000, 5000)    \n",
    "#     return str(demo)\n",
    "\n",
    "# app1.run(port=5000)  # run app in debug mode on port 5000\n",
    "\n",
    "\n",
    "#actually make it so when user requests \n",
    "\n",
    "# i want this tree to be purple\n",
    "\n",
    "# create a dropdrown of other colors it can be \n",
    "#dont have a prompt -> just creates toggles for shit that peple want \n",
    "# actually changes the shader to be those colors ??!??!?! ?! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a94642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.listdir('./templates/vite-project/public/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://genome.jgi.doe.gov/portal/pages/dynamicOrganismDownload.jsf?organism=Chlremi1\n",
    "#design fusion of two proteins\n",
    "#https://observablehq.com/d/124e11318fe98788\n",
    "#canu -d SLA-04_q100_Canu_assembly -p SLA-04 genomeSize=55m -nanopore-raw /home/calvinc/SLA-04_fast5_original/fastq_skipped+passed/*.fastq\n",
    "#https://www.google.com/search?q=algae+gneome+.fasta&sca_esv=558593241&sxsrf=AB5stBilP6j3p89gvHZ0GXMjNzQtcR3Q-A%3A1692557692181&ei=fGHiZMHPCoKrqtsPo7GTkAM&ved=0ahUKEwjB_YPg9OuAAxWClWoFHaPYBDIQ4dUDCBE&uact=5&oq=algae+gneome+.fasta&gs_lp=Egxnd3Mtd2l6LXNlcnAiE2FsZ2FlIGduZW9tZSAuZmFzdGEyBxAhGKABGApI_gpQJ1jECHAAeAKQAQCYAZUBoAHHBaoBAzQuM7gBA8gBAPgBAcICBBAAGEfCAgcQABgNGIAEwgIGEAAYFhgewgIIEAAYFhgeGA_CAgUQIRigAeIDBBgAIEGIBgGQBgU&sclient=gws-wiz-serp#ip=1\n",
    "#https://chempert.uni.lu/transcriptionalresponse/RID16890\n",
    "\n",
    "#!wget https://marinegenomics.oist.jp/symb/download/symA3_37.fasta.gz\n",
    "# !ls | grep symA3\n",
    "# !gunzip symA3_37.fasta.gz\n",
    "#run experiments\n",
    "\n",
    "#convolutions on nucoleotide basepairs -> lookign for patterns \n",
    "# make algae do more cool stuff -> increase yield, disease resitant, less water, faster thing, longer lifespan  \n",
    "#https://www.kaggle.com/competitions/open-problems-multimodal/data\n",
    "\n",
    "\n",
    "\n",
    "#4 step pipeline -> each step is programmable -> different workflows -> this one is for a different lab and a different problem\n",
    "#do a different problem -> crispr for plants, or make it generalizable across lots of workflows \n",
    "#\n",
    "#1 day\n",
    "# assemble routing structure for RNA to connect to\n",
    "#s guide rna\n",
    "#ribosome viewer\n",
    "#transcriptome design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04e8db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mudatasets\n",
    "# mdata = mudatasets.load(\"brain3k_multiome\", full=True)\n",
    "# mdata.var_names_make_unique()\n",
    "import mudatasets\n",
    "mdata = mudatasets.load(\"brain3k_multiome\", full=True)\n",
    "#mdata.var_names_make_unique()\n",
    "mdata\n",
    "#https://cpa-tools.readthedocs.io/en/latest/tutorials/GSM.html\n",
    "#https://thedataquarry.com/posts/vector-db-4/\n",
    "#invent something in blue biotech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c950cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://support.10xgenomics.com/single-cell-multiome-atac-gex/datasets/1.0.0/pbmc_granulocyte_sorted_10k\n",
    "# #https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.2/5k_pbmc_protein_v3\n",
    "# import os\n",
    "\n",
    "# import muon as mu\n",
    "# import muon.atac as ac\n",
    "# import muon.prot as pt\n",
    "\n",
    "# import matplotlib\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# from glob import glob\n",
    "\n",
    "# data_dir = \"./\"\n",
    "\n",
    "# for file in _(f\"{data_dir}/GSM5123951*\"):\n",
    "#     print(file)\n",
    "    \n",
    "# get_h5_file = lambda root, s, w: f\"{root}/{s}_X066-MP0C1{w}_leukopak_perm-cells_tea_200M_cellranger-arc_filtered_feature_bc_matrix.h5\"\n",
    "\n",
    "\n",
    "# meta = {}\n",
    "# for file in glob(f\"{data_dir}/GSM5123951*\"):\n",
    "#     tokens = os.path.basename(file).split(\"_\")\n",
    "#     meta[\n",
    "#         tokens[0]       # GSM5123951\n",
    "#     ] = tokens[1][-2:]  # W3\n",
    "\n",
    "# meta\n",
    "# s, w = list(meta.items())[0]\n",
    "\n",
    "\n",
    "# def reArrangingOrigamiBlocks():\n",
    "#     return 'aligning em microscopy images'\n",
    "#     return 'particle images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdata.obs[\"sample\"] = s\n",
    "# mdata.obs[\"well\"] = w\n",
    "\n",
    "# mdata = mu.read_10x_h5(\n",
    "#     get_h5_file(data_dir, s, w)\n",
    "# )\n",
    "\n",
    "# mdata.update()\n",
    "# mdata.var_names_make_unique()\n",
    "\n",
    "# plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# mu.pl.embedding(mdata, basis=\"X_wnn_umap\", color=list(map(\n",
    "#     lambda x: \"prot:\" + x,\n",
    "#     [\n",
    "#         \"CD4\", \"CD45RA\", \"CD45RO\",  # CD4 T cells\n",
    "#         \"CD8a\",                     # CD8 T cells\n",
    "#         \"KLRG1\",                    # NK cells\n",
    "#         \"CD14\", \"CD16\",             # monocytes\n",
    "#         \"CD19\", \"CD21\", \"IgD\",      # B cells\n",
    "#     ]\n",
    "# )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca32b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "results_file = 'write/pbmc3k.h5ad'  # the file that will store the analysis results\n",
    "adata = sc.read_10x_mtx(\n",
    "    'data/filtered_gene_bc_matrices/hg19/',  # the directory with the `.mtx` file\n",
    "    var_names='gene_symbols',                # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)      \n",
    "sc.pp.highly_variable_genes(adata, \n",
    "                                layer=None, \n",
    "                                n_top_genes=200, \n",
    "                                min_disp=0.5, \n",
    "                                max_disp=1, \n",
    "                                min_mean=0.0125, \n",
    "                                max_mean=3, \n",
    "                                span=0.3, \n",
    "                                n_bins=20, \n",
    "                                flavor='seurat_v3', \n",
    "                                subset=False, \n",
    "                                inplace=True, \n",
    "                                batch_key=None, \n",
    "                                check_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, find\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas \n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "import torch, numpy as np, pandas as pd\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "rowGeneExpression2 = defaultdict(dict)\n",
    "import math\n",
    "import torch\n",
    "pandas.set_option('mode.use_inf_as_na', True)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "import scanpy as sc\n",
    "from torch import tensor\n",
    "import torch, numpy as np, pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch import tensor\n",
    "import torch, numpy as np, pandas as pd\n",
    "import torch.optim as optim\n",
    "from fastai.data.transforms import RandomSplitter\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix, tril\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "from fastprogress.fastprogress import master_bar \n",
    "np.set_printoptions(linewidth=140)\n",
    "torch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\n",
    "pd.set_option('display.width', 140)\n",
    "#one ='DatlingerBock2021.h5ad'\n",
    "#one = 'AissaBenevolenskaya2021.h5ad'\n",
    "#one = 'AissaBenevolenskaya2021.h5ad'\n",
    "folders = '/home/awahab/llm-testing/data_sets/'\n",
    "#one = 'AdamsonWeissman2016_GSM2406675_10X001.h5ad' #sigmoid returns nan in 0th frame\n",
    "one ='DatlingerBock2017.h5ad'\n",
    "one = 'AissaBenevolenskaya2021.h5ad'\n",
    "one = 'SrivatsanTrapnell2020_sciplex2.h5ad'\n",
    "one ='DatlingerBock2017.h5ad'\n",
    "#one = 'AdamsonWeissman2016_GSM2406675_10X001.h5ad'\n",
    "#one = 'AissaBenevolenskaya2021.h5ad'\n",
    "#one = 'XieHon2017.h5ad'\n",
    "#one = 'SrivatsanTrapnell2020_sciplex2.h5ad'\n",
    "def readFiles():\n",
    "    adata = sc.read_h5ad(folders + one)\n",
    "    one ='DatlingerBock2017.h5ad'\n",
    "    return adata\n",
    "adata = sc.read_h5ad(folders + one)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata, \n",
    "                                layer=None, \n",
    "                                n_top_genes=200, \n",
    "                                min_disp=0.5, \n",
    "                                max_disp=1, \n",
    "                                min_mean=0.0125, \n",
    "                                max_mean=3, \n",
    "                                span=0.3, \n",
    "                                n_bins=20, \n",
    "                                flavor='seurat_v3', \n",
    "                                subset=False, \n",
    "                                inplace=True, \n",
    "                                batch_key=None, \n",
    "                                check_values=True)\n",
    "\n",
    "sc.pp.pca(adata)\n",
    "found = find(adata.X)\n",
    "torch.manual_seed(440)\n",
    "#adata.obs.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n",
    "#adata.obs = adata.iloc[:5000]\n",
    "#adata.obs= adata.obs[adata.obs.iloc[:5000]]\n",
    "#adata.obs.iloc[:5000]\n",
    "#adata.var_names\n",
    "var_df = adata.var\n",
    "df = adata.obs#.iloc[:5000]\n",
    "df = df.drop(columns=['nperts'])\n",
    "df['percent_mito'] = 1\n",
    "def getMode(l): \n",
    "    return max(set(l), key=l.count)\n",
    "#sc.pp.filter_cells(adata, min_counts=None, min_genes=None, max_counts=None, max_genes=10, inplace=True, copy=False)\n",
    "#sc.pp.filter_genes(adata, min_counts=None, min_cells=None, max_counts=None, max_cells=None, inplace=True, copy=False)\n",
    "#sc.pp.highly_variable_genes(adata, layer=None, n_top_genes=None, min_disp=0.5, max_disp=inf, min_mean=0.0125, max_mean=3, span=0.3, n_bins=20, flavor='seurat', subset=False, inplace=True, batch_key=None, check_values=True)\n",
    "#sc.pp.regress_out(adata, keys, n_jobs=None, copy=False)\n",
    "#cell perturbation is defined as molecular response or gene expression that is different to what is \"normal\"\n",
    "from IPython.display import IFrame\n",
    "#check for expression values that are equal from crispr\n",
    "#join with gene ontology\n",
    "#this is a program\n",
    "#input an adata file\n",
    "#outputs a list of cell-IDs and the genes perturbed \n",
    "#and then what that gene does \n",
    "#and what interactions may occur with those perturbations \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder1 = LabelEncoder()\n",
    "label_encoder2 = LabelEncoder()\n",
    "label_encoder3 = LabelEncoder()\n",
    "#df['chembl-ID'] = label_encoder1.fit_transform(df['chembl-ID'])\n",
    "df['perturbation_2'] = label_encoder1.fit_transform(df['perturbation_2'])\n",
    "df['target_2'] = label_encoder2.fit_transform(df['target'])\n",
    "cool_columns = 'ncounts ngenes percent_mito percent_ribo'.split(' ')\n",
    "for key in cool_columns:\n",
    "    ct = adata.obs[adata.obs['perturbation'] == 'control'][key].std()\n",
    "    pt = adata.obs[adata.obs['perturbation'] != 'control'][key].std()\n",
    "    print(key, '      ctrl =   ', ct, '   pert = ', pt)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#https://github.com/scipy/scipy/blob/v1.11.2/scipy/sparse/_compressed.py#L1158-L1165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('../')\n",
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "\n",
    "# import scanpy as sc\n",
    "# import torch\n",
    "# import scarches as sca\n",
    "# from scarches.dataset.trvae.data_handling import remove_sparsity\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import gdown\n",
    "\n",
    "# from absl import flags\n",
    "# flags.DEFINE_string(\"flags\", None, \"flag for host IP address\")\n",
    "\n",
    "# sc.settings.set_figure_params(dpi=200, frameon=False)\n",
    "# sc.set_figure_params(dpi=200)\n",
    "# sc.set_figure_params(figsize=(4, 4))\n",
    "# torch.set_printoptions(precision=3, sci_mode=False, edgeitems=7)\n",
    "\n",
    "# target_conditions = [\"10X\"]\n",
    "# source_adata = adata[~adata.obs.study.isin(target_conditions)].copy()\n",
    "# target_adata = adata[adata.obs.study.isin(target_conditions)].copy()\n",
    "# print(source_adata)\n",
    "# print(target_adata)\n",
    "\n",
    "# sca.models.SCVI.setup_anndata(source_adata, batch_key=\"batch\")\n",
    "\n",
    "# vae = sca.models.SCVI(\n",
    "#     source_adata,\n",
    "#     n_layers=2,\n",
    "#     encode_covariates=True,\n",
    "#     deeply_inject_covariates=False,\n",
    "#     use_layer_norm=\"both\",\n",
    "#     use_batch_norm=\"none\",\n",
    "# )\n",
    "\n",
    "# early_stopping_kwargs = {\n",
    "#     \"early_stopping_metric\": \"elbo\",\n",
    "#     \"save_best_state_metric\": \"elbo\",\n",
    "#     \"patience\": 10,\n",
    "#     \"threshold\": 0,\n",
    "#     \"reduce_lr_on_plateau\": True,\n",
    "#     \"lr_patience\": 8,\n",
    "#     \"lr_factor\": 0.1,\n",
    "# }\n",
    "# vae.train(n_epochs=500, frequency=1, early_stopping_kwargs=early_stopping_kwargs)\n",
    "\n",
    "\n",
    "# reference_latent = sc.AnnData(vae.get_latent_representation())\n",
    "# reference_latent.obs[\"cell_type\"] = source_adata.obs[\"final_annotation\"].tolist()\n",
    "# reference_latent.obs[\"batch\"] = source_adata.obs[\"batch\"].tolist()\n",
    "# sc.pp.neighbors(reference_latent, n_neighbors=8)\n",
    "# sc.tl.leiden(reference_latent)\n",
    "# sc.tl.umap(reference_latent)\n",
    "# sc.pl.umap(reference_latent,\n",
    "#            color=['batch', 'cell_type'],\n",
    "#            frameon=False,\n",
    "#            wspace=0.6,\n",
    "#            )\n",
    "\n",
    "# ref_path = 'ref_model/'\n",
    "# vae.save(ref_path, overwrite=True)\n",
    "\n",
    "# model = sca.models.SCVI.load_query_data(\n",
    "#     target_adata,\n",
    "#     ref_path,\n",
    "#     freeze_dropout = True,\n",
    "# )\n",
    "\n",
    "# query_latent = sc.AnnData(model.get_latent_representation())\n",
    "# query_latent.obs['cell_type'] = target_adata.obs[\"final_annotation\"].tolist()\n",
    "# query_latent.obs['batch'] = target_adata.obs[\"batch\"].tolist()\n",
    "\n",
    "# model.train(n_epochs=500, frequency=1, early_stopping_kwargs=early_stopping_kwargs, weight_decay=0)\n",
    "# sc.pp.neighbors(query_latent)\n",
    "# sc.tl.leiden(query_latent)\n",
    "# sc.tl.umap(query_latent)\n",
    "# plt.figure()\n",
    "# sc.pl.umap(\n",
    "#     query_latent,\n",
    "#     color=[\"batch\", \"cell_type\"],\n",
    "#     frameon=False,\n",
    "#     wspace=0.6,\n",
    "# )\n",
    "\n",
    "# surgery_path = 'surgery_model'\n",
    "# model.save(surgery_path, overwrite=True)\n",
    "\n",
    "#pathing errors -> conda installed with sudo\n",
    "\n",
    "# from absl import flags\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "# from absl import flags\n",
    "# from absl.flags import FLAGS\n",
    "\n",
    "# flags.DEFINE_string('dataroot',\"D:\\College\",'path to root folder of dataset')\n",
    "\n",
    "\n",
    "# import scarches as sca\n",
    "#reinstall. abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45167ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "import torch, numpy as np, pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "from fastai.data.transforms import RandomSplitter\n",
    "from collections import defaultdict\n",
    "\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_columns] = df[numerical_columns].fillna(0)\n",
    "numerical_values = df.select_dtypes(include=[int, float]).values.tolist()\n",
    "numerical_values\n",
    "rowGeneExpression = defaultdict(int)\n",
    "hv_genes = set(list(var_df[var_df['highly_variable'] == True].index))\n",
    "normal_genes = (list(adata.var_names))\n",
    "high_variance_columns = set([ i for i,val in enumerate(normal_genes) if val in hv_genes ])\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_columns] = df[numerical_columns].fillna(0)\n",
    "sums = []\n",
    "column_averages = defaultdict(list)\n",
    "rowGeneExpression = defaultdict(int)\n",
    "rows, columns, vals = found\n",
    "high_variance = set(high_variance_columns)\n",
    "row_id = 0\n",
    "control_variables = set(['ctrl', 'control', '*'])\n",
    "dependent_variables = list(df['perturbation'].map(lambda val: 0 if val in control_variables else 1).values)\n",
    "geneValues = defaultdict(int)\n",
    "columnMode = defaultdict(list)\n",
    "geneAverages = defaultdict(int)\n",
    "geneOccurences = defaultdict(int)\n",
    "geneVariance = defaultdict(list)\n",
    "cell_variance_score = defaultdict(int)\n",
    "\n",
    "row_variance = [] \n",
    "c,g,v = found\n",
    "\n",
    "cell_variance_score = {}\n",
    "for i in range(df.shape[0]): cell_variance_score[i]= 0\n",
    "\n",
    "for cell,gene,val in zip(c,g,v):\n",
    "    if gene not in high_variance_columns: continue\n",
    "    geneValues[gene] += val\n",
    "    geneOccurences[gene] += 1\n",
    "    columnMode[gene].append(val)\n",
    "    \n",
    "for k in dict(geneValues):\n",
    "    geneAverages[k] =  geneValues[k] / geneOccurences[k]\n",
    "    \n",
    "for k in dict(geneValues): columnMode[k] = getMode(columnMode[k])\n",
    "    \n",
    "for cell, gene, val in zip(c,g,v):\n",
    "    if gene not in high_variance_columns: continue\n",
    "    geneVariance[gene].append(abs(val - geneAverages[gene]))# ** 2\n",
    "    \n",
    "    \n",
    "for k in dict(geneAverages):  \n",
    "    geneVariance[k] = max(set(geneVariance[k]), key=geneVariance[k].count)\n",
    "\n",
    "geneModes = defaultdict(list)\n",
    "\n",
    "for cell, gene, val in zip(c,g,v):\n",
    "    if gene not in high_variance_columns: continue\n",
    "    geneModes[gene].append(abs(val))# ** 2\n",
    "\n",
    "for val in geneModes: geneModes[val] = max(set(geneModes[val]), key=geneModes[val].count)\n",
    "\n",
    "num_cells = len(df.select_dtypes(include=[int, float]).values.tolist())\n",
    "    \n",
    "mini_cell_var = defaultdict(list)\n",
    "for cell, gene, val in zip(c,g,v):\n",
    "    if gene not in high_variance_columns: continue\n",
    "    columnColor = geneAverages[gene]\n",
    "    cellColorForGene = val\n",
    "    threshold = columnColor\n",
    "    if (cellColorForGene - columnColor) < 0:\n",
    "        mini_cell_var[cell].append(cellColorForGene - columnColor)\n",
    "        cell_variance_score[cell] += abs(cellColorForGene - columnColor)\n",
    "      \n",
    "for key in mini_cell_var: mini_cell_var[key] = max(mini_cell_var[key])\n",
    "        \n",
    "df['geneVarianceScore'] = cell_variance_score.values()\n",
    "numerical_values = df.select_dtypes(include=[int, float]).values.tolist()\n",
    "independent_variables = pd.DataFrame(numerical_values)\n",
    "\n",
    "vals += .01\n",
    "t_dep = tensor([float(i) for i in dependent_variables]) # pertrubations\n",
    "t_indep = tensor(numerical_values, dtype=torch.float)\n",
    "\n",
    "n_coeff = t_indep.shape[1]\n",
    "\n",
    "vals,indices = t_indep.max(dim=0)\n",
    "t_indep = t_indep / vals\n",
    "trn_split,val_split=RandomSplitter(seed=42)(independent_variables)\n",
    "\n",
    "trn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\n",
    "trn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\n",
    "\n",
    "indep_cols =  df.select_dtypes(include=[int, float]).columns.tolist()\n",
    "indep_cols\n",
    "\n",
    "len([item for item in list(t_dep) if item.item() == 0])\n",
    "len([item for item in list(t_dep) if item.item() > .5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_variance_score= defaultdict(int)\n",
    "for cell, gene, val in zip(c,g,v):\n",
    "    if gene not in high_variance_columns: continue\n",
    "    columnColor = geneAverages[gene]\n",
    "    cellColorForGene = val\n",
    "    threshold = columnColor\n",
    "    if abs(cellColorForGene) > columnColor and columnColor < 1:\n",
    "        #mini_cell_var[cell].append(cellColorForGene - columnColor)\n",
    "        cell_variance_score[cell] += abs(cellColorForGene - columnColor)\n",
    "l = cell_variance_score.values()   \n",
    "avg = sum(l) / len(l)\n",
    "avg = 0\n",
    "import random\n",
    "cvs = cell_variance_score.values()\n",
    "mini_cell_var.values()\n",
    "total_guess = len([item for key, item in enumerate(cvs) if item > avg])\n",
    "correct_guess = len([item for key, item in enumerate(cvs) if item > avg and dependent_variables[key] == 1])\n",
    "perb_total =  len([item for key, item in enumerate(dependent_variables) if dependent_variables[key] == 1])\n",
    "print(f'correct{correct_guess}, total_guess{total_guess}, perb_total {perb_total}, accuracy {correct_guess / total_guess}')\n",
    "print(f'precision {total_guess / perb_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c75d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = []\n",
    "test = defaultdict(int)\n",
    "for i in high_variance_columns:\n",
    "    m=adata.X.getcol(i).todense()\n",
    "    mode = getMode(m.tolist()[0]) \n",
    "    avg = sum(m.tolist()[0]) / len(m.tolist()[0])\n",
    "    pert_and_above_zero = len([i for k, i in enumerate(m.tolist()) if i[0] > 0 and dependent_variables[k] > 0])\n",
    "    not_pert_and_above_zero = len([i for k, i in enumerate(m.tolist()) if i[0] > 0 and dependent_variables[k] < 1])\n",
    "    above_zero = len([i for k, i in enumerate(m.tolist()) if i[0] > 0])\n",
    "    eq_zero = len([i for k, i in enumerate(m.tolist()) if i[0] == 0])\n",
    "    test[i] = above_zero\n",
    "    cellCounts = 5904\n",
    "    if (above_zero > 30): continue # 90%\n",
    "    for key,element in enumerate(m.tolist()):\n",
    "        if element[0] > 0: count.append(key)\n",
    "print(len(set(count)))\n",
    "count = set(count)\n",
    "print(len([x for row, x in enumerate(count) if dependent_variables[x] > 0]),len([x for row, x in enumerate(count) if dependent_variables[x] < 1]))\n",
    "print(len([x for row, x in enumerate(count) if dependent_variables[x] > 0]) / len([x for row, x in enumerate(count)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb0b6c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "category_indices = df.groupby('perturbation').apply(lambda x: x.index.tolist() )\n",
    "most_cells = category_indices[2]\n",
    "\n",
    "most_cell_indices = []\n",
    "for i in most_cells:\n",
    "    most_cell_indices.append(adata.obs.index.get_loc(i))\n",
    "\n",
    "a=most_cell_indices[0]\n",
    "b=most_cell_indices[10]\n",
    "\n",
    "b_matrix = adata.X.getrow(b).todense().tolist()[0]\n",
    "a_matrix = adata.X.getrow(a).todense().tolist()[0]\n",
    "\n",
    "a_matrix\n",
    "print(len(most_cells))\n",
    "sum(a_matrix), sum(b_matrix)\n",
    "count = {}\n",
    "        \n",
    "distance = defaultdict(int)\n",
    "indicesAbove = defaultdict(list)\n",
    "\n",
    "for row in range(5904):\n",
    "    m = adata.X.getrow(row).todense().tolist()[0]\n",
    "    for k in high_variance_columns:\n",
    "        if (geneAverages[k]) < m[k] and m[k] < 100:\n",
    "            distance[k] += m[k]\n",
    "            indicesAbove[row].append(k)\n",
    "            \n",
    "distance_max = max(list(distance.values()))\n",
    "\n",
    "for k in distance:\n",
    "    if distance[k] == distance_max: print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scipy.stats.zscore(adata.X.getcol(0).todense().tolist())\n",
    "indicesAbove = dict(indicesAbove)\n",
    "# for cell,gene,val in zip(c,g,v):\n",
    "#     if gene not in high_variance_columns: continue\n",
    "#     geneValues[gene] += val\n",
    "#     geneOccurences[gene] += 1\n",
    "#     columnMode[gene].append(val)\n",
    "\n",
    "timesAbove = defaultdict(int)\n",
    "geneAboveMeanOccurances = defaultdict(list)\n",
    "\n",
    "for row in dict(indicesAbove): \n",
    "    for column in indicesAbove[row]: \n",
    "        geneAboveMeanOccurances[column].append(row)\n",
    "        \n",
    "prob_perts = {} \n",
    "\n",
    "filteredGeneCellLists = defaultdict(list)\n",
    "\n",
    "threshold = 30\n",
    "\n",
    "for geneList in geneAboveMeanOccurances:\n",
    "    cellsWithGene = geneAboveMeanOccurances[geneList]\n",
    "    if  threshold < len(cellsWithGene) and len(cellsWithGene) < 100:\n",
    "        filteredGeneCellLists[geneList] = cellsWithGene\n",
    "\n",
    "cellToGeneEmbedding = [[] for i in range(5904)]\n",
    "\n",
    "for column in filteredGeneCellLists:\n",
    "    cellList = filteredGeneCellLists[column]\n",
    "    for cellRow in cellList:\n",
    "        cellToGeneEmbedding[cellRow].append(column)\n",
    "    \n",
    "cellToGeneEmbedding\n",
    "\n",
    "cellCount = 0\n",
    "for cellList in list(filteredGeneCellLists.values()):\n",
    "    cellCount += len(cellList)\n",
    "    \n",
    "totalCells = []\n",
    "for key in (filteredGeneCellLists.keys()):\n",
    "    cellList = filteredGeneCellLists[key]\n",
    "    totalCells += cellList\n",
    "    for cell in cellList:\n",
    "        gene = adata.var.iloc[cell].name\n",
    "        row = df.iloc[cell]\n",
    "        \n",
    "len(set(totalCells))\n",
    "\n",
    "len([item for key, item in enumerate(t_dep) if item.item() > .5 and key in totalCells])\n",
    "total = defaultdict(int)\n",
    "for row in range(500):\n",
    "    total[row] += sum(adata.X.getrow(row).data)\n",
    "avg = sum(list(total.values())) / 500\n",
    "\n",
    "counter = 0\n",
    "for key, item in enumerate(list(total.values())):\n",
    "    if item > avg:\n",
    "        counter += 1\n",
    "        \n",
    "counter\n",
    "count_per_category = df.groupby('perturbation').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort genes by perturbation so diagram makes a clear line from top left to bottom right\n",
    "# solve the adata roblem -> cosine -> predict molecular response\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "category_indices = df.groupby('perturbation').apply(lambda x: x.index.tolist() )\n",
    "most_cells = [category_indices[2][0] , category_indices[1][0]]\n",
    "\n",
    "most_cell_indices = []\n",
    "for i in most_cells:\n",
    "    most_cell_indices.append(adata.obs.index.get_loc(i))\n",
    "    \n",
    "cell_group_indices = [df.index.get_loc(i) for i in most_cells]\n",
    "similarities_list = []\n",
    "\n",
    "for i in cell_group_indices:\n",
    "    for j in cell_group_indices:\n",
    "        A =  adata.X.getrow(i)\n",
    "        A_sparse = adata.X.getrow(j)\n",
    "        similarities = cosine_similarity(A_sparse, A)\n",
    "        if similarities[0][0] > .5:\n",
    "            similarities_list.append((i,j, similarities[0][0]))\n",
    "\n",
    "    \n",
    "    \n",
    "print('pairwise dense output:\\n {}\\n'.format(similarities))\n",
    "\n",
    "#also can output sparse matrices\n",
    "similarities_sparse = cosine_similarity(A_sparse,dense_output=False)\n",
    "similarities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Replace NaN values with 0 only in numerical columns\n",
    "df[numerical_columns] = df[numerical_columns].fillna(0)\n",
    "\n",
    "numerical_values = df.select_dtypes(include=[int, float]).values.tolist()\n",
    "numerical_values\n",
    "rowGeneExpression = defaultdict(int)\n",
    "\n",
    "hv_genes = set(list(var_df[var_df['highly_variable'] == True].index))\n",
    "normal_genes = (list(adata.var_names))\n",
    "\n",
    "high_variance_columns = set([ i for i,val in enumerate(normal_genes) if val in hv_genes ])\n",
    "\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Replace NaN values with 0 only in numerical columns\n",
    "df[numerical_columns] = df[numerical_columns].fillna(0)\n",
    "\n",
    "sums = []\n",
    "\n",
    "column_averages = defaultdict(list)\n",
    "rowGeneExpression = defaultdict(int)\n",
    "rows, columns, vals = found\n",
    "high_variance = set(high_variance_columns)\n",
    "row_id = 0\n",
    "\n",
    "embedLayer = []\n",
    "for i in high_variance_columns:\n",
    "        intermediate = []\n",
    "        for i in adata.X.getcol(i).toarray():\n",
    "            intermediate.append(i[0])\n",
    "        embedLayer.append(intermediate)\n",
    "\n",
    "mat_for_embed = np.random.rand(t_dep.shape[0], 200)\n",
    "for key,col in enumerate(list(high_variance_columns)[:200]):\n",
    "    m= adata.X.getcol(col)\n",
    "    m = m.todense().tolist()\n",
    "    for row,val in enumerate(m):\n",
    "        mat_for_embed[row, key] = val[0]\n",
    "\n",
    "a = mat_for_embed\n",
    "plt.imshow(a[:80], cmap='nipy_spectral_r', interpolation='nearest')\n",
    "plt.show()\n",
    "#adata.X.A[0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86216ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://douglaslab.org/tutorials/files/Install-Cadnano-macOS-v4.pdf\n",
    "# # # http# ! pip install biomart\n",
    "# # mito_gene_names = sc.queries.mitochondrial_genes(\"hsapiens\")\n",
    "# # mito_ensembl_ids = sc.queries.mitochondrial_genes(\"hsapiens\", attrname=\"ensembl_gene_id\")\n",
    "# # mito_gene_names_fly = sc.queries.mitochondrial_genes(\"dmelanogaster\", chromosome=\"mitochondrion_genome\")\n",
    "# # import scanpy as sc\n",
    "# # sc.queries.enrich(['KLF4', 'PAX5', 'SOX2', 'NANOG'], org=\"hsapiens\")\n",
    "# # sc.queries.enrich({'set1':['KLF4', 'PAX5'], 'set2':['SOX2', 'NANOG']}, org=\"hsapiens\")\n",
    "# # pbmcs = sc.datasets.pbmc68k_reduced()\n",
    "# # sc.tl.rank_genes_groups(pbmcs, \"bulk_labels\")\n",
    "# # sc.queries.enrich(pbmcs, \"CD34+\")\n",
    "# # # pbmcs\n",
    "# # category_indices = df.groupby('perturbation').apply(lambda x: x.index.tolist() )\n",
    "# # df.index.get_loc('TACTTGACCCCN')\n",
    "# # allRows = defaultdict(int)\n",
    "# # categories = df['perturbation'].unique()\n",
    "# # for i, group in enumerate(category_indices):\n",
    "# #     for row in group:\n",
    "# #         allRows[categories[i]] += 1\n",
    "        \n",
    "# # df.groupby('perturbation')\n",
    "\n",
    "# # categories\n",
    "# # len(category_indices)\n",
    "\n",
    "# # groupCellCounts = list(allRows.values())\n",
    "\n",
    "# # nonZerosInColumn = list(test.values())\n",
    "\n",
    "# # for k,v in enumerate(groupCellCounts):\n",
    "# #     cellCount = nonZerosInColumn[k]\n",
    "# # # from ipywidgets import interact\n",
    "# # # trn_xs = [1,2,3,4,5]\n",
    "# # # conts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]\n",
    "\n",
    "# #just get it working - improve it now\n",
    "# from torch import nn\n",
    "# import torch\n",
    "# def conv(ni, nf, ks=3, stride=1, act=True):\n",
    "#     res = nn.Conv1d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n",
    "#     if act: res = nn.Sequential(res, nn.ReLU())\n",
    "#     return res\n",
    "\n",
    "# def deconv(ni, nf, ks=3, act=True):\n",
    "#     layers = [\n",
    "#     #    nn.UpsamplingNearest2d(scale_factor=2),\n",
    "#               nn.Conv2d(ni, nf, stride=1, kernel_size=ks, padding=ks//2)\n",
    "#     ]\n",
    "#     if act: layers.append(nn.ReLU())\n",
    "#     return nn.Sequential(*layers)\n",
    "\n",
    "# #data /= torch.max(data , 1)\n",
    "# #sort them by cluster and back\n",
    "# finishDemoBy6 = nn.Sequential(\n",
    "#     #nn.RNN(200, 200),\n",
    "#     torch.nn.Linear(200, 200),\n",
    "#     nn.Tanhshrink(),\n",
    "#     #nn.PairwiseDistance(p=2),\n",
    "#     conv(5905,5905, 3),       \n",
    "#     nn.AvgPool1d(101, stride=1),\n",
    "#     conv(5905,5905),\n",
    "#     nn.BatchNorm1d(100),\n",
    "#     nn.AvgPool1d(51, stride=1),\n",
    "#     #conv(5905,5905), \n",
    "#     nn.AvgPool1d(48, stride=1),\n",
    "#     nn.Sigmoid()\n",
    "# ).to('cuda:0')\n",
    "\n",
    "# num_input_channels = 3\n",
    "# c_hid=16\n",
    "# latent_dim = 64\n",
    "# finishDemoBy6= nn.Sequential(\n",
    "#         nn.Conv2d(1, c_hid, kernel_size=2, padding=1, stride=2),  # 32x32 => 16x16\n",
    "#            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "#            nn.Conv2d(c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 16x16 => 8x8\n",
    "#            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "#            nn.Conv2d(2 * c_hid, 3, kernel_size=3, padding=0, stride=1),  # 8x8 => 4x4\n",
    "#            #nn.Flatten(),  # Image grid to single feature vector\n",
    "# #             nn.Linear(2 * 16 * c_hid, 3),\n",
    "# )\n",
    "\n",
    "# opt = optim.SGD(finishDemoBy6.parameters(), lr=0.01)\n",
    "# loss_function2 = torch.nn.MSELoss()\n",
    "# opt = optim.SGD(finishDemoBy6.parameters(), lr=0.01)\n",
    "# loss_function2 = torch.nn.MSELoss()\n",
    "# data = torch.Tensor(mat_for_embed).cuda()\n",
    "# for i in range(50):\n",
    "#     encodedOutput = (finishDemoBy6(data))\n",
    "# #     loss = loss_function2(encodedOutput.sum(1), t_dep.cuda())\n",
    "# #     opt.zero_grad()  # 3\n",
    "# #     loss.backward()\n",
    "#     opt.step()\n",
    "# encodedOutput.to('cuda:0')\n",
    "# encodedOutput\n",
    "# Z = adata.X.A\n",
    "# encodedOutput\n",
    "# https://www.frontiersin.org/articles/10.3389/fpls.2023.1091588/full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7519afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert 4 == 24\n",
    "#! pip install pybiomart\n",
    "#! pip install gprofiler-official\n",
    "import biomart\n",
    "import pybiomart\n",
    "\n",
    "mito_gene_names = sc.queries.mitochondrial_genes(\"hsapiens\")\n",
    "mito_ensembl_ids = sc.queries.mitochondrial_genes(\"hsapiens\", attrname=\"ensembl_gene_id\")\n",
    "mito_gene_names_fly = sc.queries.mitochondrial_genes(\"dmelanogaster\", chromosome=\"mitochondrion_genome\")\n",
    "import scanpy as sc\n",
    "sc.queries.enrich(['KLF4', 'PAX5', 'SOX2', 'NANOG'], org=\"hsapiens\")\n",
    "sc.queries.enrich({'set1':['KLF4', 'PAX5'], 'set2':['SOX2', 'NANOG']}, org=\"hsapiens\")\n",
    "pbmcs = sc.datasets.pbmc68k_reduced()\n",
    "sc.tl.rank_genes_groups(pbmcs, \"bulk_labels\")\n",
    "sc.queries.enrich(pbmcs, \"CD34+\")\n",
    "# pbmcs\n",
    "category_indices = df.groupby('perturbation').apply(lambda x: x.index.tolist() )\n",
    "df.index.get_loc('TACTTGACCCCN')\n",
    "allRows = defaultdict(int)\n",
    "categories = df['perturbation'].unique()\n",
    "for i, group in enumerate(category_indices):\n",
    "    for row in group:\n",
    "        allRows[categories[i]] += 1\n",
    "annot = sc.queries.biomart_annotations(\n",
    "        \"hsapiens\",\n",
    "        [\"ensembl_gene_id\", \"start_position\", \"end_position\", \"chromosome_name\"],\n",
    "    ).set_index(\"ensembl_gene_id\")\n",
    "adata.var[annot.columns] = annot\n",
    "\n",
    "#adata.var[annot.columns]\n",
    "annot.columns\n",
    "#https://www.nature.com/articles/s41467-022-29268-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "       \n",
    "df.groupby('perturbation')\n",
    "\n",
    "categories\n",
    "len(category_indices)\n",
    "\n",
    "groupCellCounts = list(allRows.values())\n",
    "\n",
    "nonZerosInColumn = list(allRows.values())\n",
    "\n",
    "# for k,v in enumerate(groupCellCounts):\n",
    "#     cellCount = nonZerosInColumn[k]\n",
    "# # from ipywidgets import interact\n",
    "# # trn_xs = [1,2,3,4,5]\n",
    "# # conts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]\n",
    "\n",
    "\n",
    "# # interact(nm=conts, split=15.5)(iscore);\n",
    "#just get it working - improve it now\n",
    "# num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2),  # 32x32 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(),  # Image grid to single feature vector\n",
    "            nn.Linear(2 * 16 * c_hid, latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "from torch import nn\n",
    "import torch\n",
    "def conv(ni, nf, ks=3, stride=1, act=True):\n",
    "    res = nn.Conv1d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n",
    "    if act: res = nn.Sequential(res, nn.ReLU())\n",
    "    return res\n",
    "\n",
    "def deconv(ni, nf, ks=3, act=True):\n",
    "    layers = [\n",
    "    #    nn.UpsamplingNearest2d(scale_factor=2),\n",
    "              nn.Conv2d(ni, nf, stride=1, kernel_size=ks, padding=ks//2)\n",
    "    ]\n",
    "    if act: layers.append(nn.ReLU())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "c_hid=16\n",
    "latent_dim = 3\n",
    "kernel_size = (1, 5)\n",
    "finishDemo= nn.Sequential(\n",
    "            nn.Conv2d(1, c_hid, kernel_size=(1, 5)),  # 32x32 => 16x16\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=kernel_size),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(c_hid, 2 * c_hid, kernel_size=kernel_size),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=kernel_size),\n",
    "            nn.LeakyReLU(), #L\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(2 * c_hid, 1, kernel_size=kernel_size),  # 8x8 => 4x4\n",
    "            nn.Linear(180, 3)\n",
    ").to('cuda:0')\n",
    "##add some matrix math on latent representation\n",
    "## decode back ???\n",
    "opt = optim.SGD(finishDemo.parameters(), lr=0.01)\n",
    "loss_function2 = torch.nn.MSELoss()\n",
    "data = torch.Tensor([[mat_for_embed]]).to('cuda')\n",
    "\n",
    "for i in range(5):\n",
    "    encodedOutput = (finishDemo(data.to('cuda:0')))\n",
    "    loss = loss_function2(encodedOutput.squeeze().sum(1), t_dep.cuda())\n",
    "    opt.zero_grad()  # 3\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "encodedOutput.to('cuda:0')\n",
    "encodedOutput = (finishDemo(data))\n",
    "tensor = encodedOutput\n",
    "tensor = tensor.squeeze()\n",
    "tensor\n",
    "#print(tensor.shape)\n",
    "#spongebob\n",
    "#tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e5802",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.sum(1)\n",
    "#simplify model, use a decoder, use another simple function on ('latent representation') to figure out stochastic relationships between columns and \n",
    "#mean, variance, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for latent_dim in [64, 128, 256, 384]:\n",
    "#     model_ld, result_ld = train_cifar(latent_dim)\n",
    "#     model_dict[latent_dim] = {\"model\": model_ld, \"result\": result_ld}\n",
    "#     print(model_dict)\n",
    "# model = model_dict[128][\"model\"]\n",
    "# model = model_dict[256][\"model\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "from Bio.SeqUtils import GC \n",
    "records = [len(rec) for rec in SeqIO.parse(filename, \"fasta\")] \n",
    "pylab.xlabel(\"Genes\") \n",
    "pylab.ylabel(\"GC Percentage\") \n",
    "pylab.grid()\n",
    "pylab.hist(records,bins=5) \n",
    "gc = sorted(GC(rec.seq) for rec in SeqIO.parse(filename, \"fasta\"))\n",
    "pylab.plot(gc) \n",
    "#https://marinegenomics.oist.jp/symb/viewer/download?project_id=37\n",
    "#https://compeau.cbd.cmu.edu/teaching/great-ideas-in-computational-biology/\n",
    "#https://pubs.acs.org/doi/10.1021/acssynbio.1c00329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af8be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "c_hid=16\n",
    "latent_dim = 3\n",
    "kernel_size = (1, 5)\n",
    "finishDemo= nn.Sequential(\n",
    "            nn.Conv2d(1, c_hid, kernel_size=(1, 5)),  # 32x32 => 16x16\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=kernel_size),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(c_hid, 2 * c_hid, kernel_size=kernel_size),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=kernel_size),\n",
    "            nn.LeakyReLU(), #L\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(2 * c_hid, 1, kernel_size=kernel_size),  # 8x8 => 4x4\n",
    "            #nn.Linear(180, 3)\n",
    ").to('cuda:0')\n",
    "\n",
    "# buf = numpy.fromfile( dataFile, dtype=np.uint8, count=16384, offset=offs)\n",
    "\n",
    "nucleotides = []\n",
    "# with open('./maize_pseudohap.fasta.gz', 'r') as f:\n",
    "#     for line in f:\n",
    "#         nucleotides.append(line)\n",
    "def read_fasta(filename):\n",
    "    \"\"\"\n",
    "    Read a FASTA file and return a dictionary with sequence headers as keys and sequences as values.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        sequences = {}\n",
    "        header = None\n",
    "        sequence = []\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            line = line.strip()  # Remove whitespace\n",
    "            count+= 1\n",
    "            print(line)\n",
    "            if (count> 100):\n",
    "                break\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "            if line.startswith(\">\"):  # Header line\n",
    "                if header:  # If there's already a previous header, save the sequence\n",
    "                    sequences[header] = ''.join(sequence)\n",
    "                    sequence = []\n",
    "                header = line[1:]  # Exclude the \">\" symbol\n",
    "            else:  # Sequence line\n",
    "                sequence.append(line)\n",
    "        \n",
    "        # Save the last sequence\n",
    "        if header:\n",
    "            sequences[header] = ''.join(sequence)\n",
    "        \n",
    "    return sequences\n",
    "\n",
    "\n",
    "    \n",
    "from Bio import SeqIO\n",
    "\n",
    "filename = 'maize_pseudohap.fasta'\n",
    "filename = 'symA3_37.fasta'\n",
    "count = 0\n",
    "\n",
    "seq = []\n",
    "with open(filename, \"r\") as fasta_file:\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        if count > 100: break\n",
    "        #print(record.id)   # Prints the sequence header (identifier)\n",
    "        seq.append(record.seq)  # Prints the sequence\n",
    "        count += 1\n",
    "s = seq[0]\n",
    "s= str(s)\n",
    "img = []\n",
    "charToI = {\n",
    "    'T': 0,\n",
    "    'A': 1,\n",
    "    'G': 2,\n",
    "    'C': 3,\n",
    "    'N': 4\n",
    "}\n",
    "for char in s:\n",
    "    img.append(charToI[char])\n",
    "import torch\n",
    "tensor = torch.Tensor([[[img]]]).to('cuda:0')\n",
    "#finishDemo(tensor)\n",
    "s= str(s)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d6a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain_imgs = torch.zeros(4, 3, 32, 32)\n",
    "# # Single color channel\n",
    "# plain_imgs[1, 0] = 1\n",
    "# # Checkboard pattern\n",
    "# plain_imgs[2, :, :16, :16] = 1\n",
    "# plain_imgs[2, :, 16:, 16:] = -1\n",
    "# # Color progression\n",
    "# xx, yy = torch.meshgrid(torch.linspace(-1, 1, 32), torch.linspace(-1, 1, 32))\n",
    "# plain_imgs[3, 0, :, :] = xx\n",
    "# plain_imgs[3, 1, :, :] = yy\n",
    "# visualize_reconstructions(model_di#ct[256][\"model\"], plain_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82474d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scvelo\n",
    "import scvelo as scv\n",
    "adata = scv.datasets.pancreas()\n",
    "\n",
    "scv.pp.filter_genes(adata, min_shared_counts=20)\n",
    "scv.pp.normalize_per_cell(adata)\n",
    "scv.pp.filter_genes_dispersion(adata, n_top_genes=2000)\n",
    "scv.pp.log1p(adata)\n",
    "scv.pp.moments(adata, n_pcs=30, n_neighbors=30)\n",
    "scv.tl.velocity(adata)\n",
    "#scv.pl.velocity_embedding_stream(adata, basis='umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d5681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev = 'cuda:0'\n",
    "def test_prediction(test_predictions):\n",
    "    ctrl = test_predictions.sum(1).tolist()[0]\n",
    "    isFalse = len([sum(row) for idx, row in enumerate(test_predictions.tolist()) if sum(row) <= ctrl and t_dep[idx] == 0])\n",
    "    isTrue = len([sum(row) for idx, row in enumerate(test_predictions.tolist()) if sum(row) > ctrl and t_dep[idx] == 1])\n",
    "    allFalse = len([sum(row) for idx, row in enumerate(test_predictions.tolist()) if t_dep[idx] == 0])\n",
    "    allTrue = len([sum(row) for idx, row in enumerate(test_predictions.tolist()) if t_dep[idx] == 1])\n",
    "    return (isFalse / allFalse, isTrue / allTrue, isFalse, isTrue)\n",
    "def plot_loss(l):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    legends = []\n",
    "    plt.plot(l) \n",
    "    plt.plot([0, len([i for k,i in enumerate(rowGeneExpression.values()) if dependent_variables[k]])], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "    plt.legend(legends);\n",
    "\n",
    "numerical_values = df.select_dtypes(include=[int, float]).values.tolist()\n",
    "t_indep = torch.Tensor(numerical_values).to(dev)\n",
    "# t_indep = t_indep / vals\n",
    "# .requires_grad_(True)\n",
    "#3 variations, test, t_indep and t_indep+embedding\n",
    "resultant_tensor = t_indep\n",
    "encodedOutput.requires_grad_(True)\n",
    "#resultant_tensor = torch.cat((t_indep.to(dev),tensor.to(dev)), 1)\n",
    "#resultant_tensor = \n",
    "#resultant_tensor = tensor\n",
    "vals, indices = resultant_tensor.max(dim=0)\n",
    "resultant_tensor = resultant_tensor / vals\n",
    "resultant_tensor = resultant_tensor.to(dev)\n",
    "test_indep = torch.tensor([[t_dep[k].item() for i in enumerate(range(resultant_tensor.shape[1]))] for k, i in enumerate(range(resultant_tensor.shape[0]))])\n",
    "dim = resultant_tensor.shape[1]\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(dim,dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(dim,dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(dim, dim),\n",
    "    nn.Sigmoid()\n",
    ").to(dev)\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=.1, \n",
    "    weight_decay=0.01\n",
    ")\n",
    "# model = model.to(device)\n",
    "# input_tensor = input_tensor.to(device)\n",
    "\n",
    "n_iterations = 1000\n",
    "loss_track = []\n",
    "accuracy_track = []\n",
    "no_entropy = []\n",
    "loss_function = torch.nn.BCELoss()\n",
    "def plot_loss_update(epoch, epochs, mb, train_loss, valid_loss):\n",
    "    \"\"\" dynamically print the loss plot during the training/validation loop.\n",
    "        expects epoch to start from 1.\n",
    "    \"\"\"\n",
    "    x = range(1, epoch+1)\n",
    "    y = np.concatenate((train_loss, valid_loss))\n",
    "    graphs = [[x,train_loss], [x,valid_loss]]\n",
    "    x_margin = 0.2\n",
    "    y_margin = 0.05\n",
    "    x_bounds = [1-x_margin, epochs+x_margin]\n",
    "    y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]\n",
    "\n",
    "    mb.update_graph(graphs, x_bounds, y_bounds)\n",
    "mb = master_bar(range(1))\n",
    "def plot_loss_update(epoch, epochs, mb, train_loss, valid_loss):\n",
    "    \"\"\" dynamically print the loss plot during the training/validation loop.\n",
    "        expects epoch to start from 1.\n",
    "    \"\"\"\n",
    "    x = range(1, epoch+1)\n",
    "    y = np.concatenate((train_loss, valid_loss))\n",
    "    print(x,y)\n",
    "    graphs = [[x,train_loss], [x,valid_loss]]\n",
    "    x_margin = 0.2\n",
    "    y_margin = 0.05\n",
    "    x_bounds = [1-x_margin, epochs+x_margin]\n",
    "    y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]\n",
    "    print(x_bounds, y_bounds)\n",
    "    mb.update_graph(graphs, x_bounds, y_bounds)\n",
    "\n",
    "for i in mb:    \n",
    "#for j in progress_bar(range(2000), parent=mb):\n",
    "    for j in progress_bar(range(2000)):\n",
    "        loss = loss_function(model(resultant_tensor).sum(1).sigmoid(), t_dep.to(dev))\n",
    "        optimizer.zero_grad()  # 3\n",
    "        loss.backward(retain_graph=True)  # 4\n",
    "        optimizer.step()  # 5\n",
    "        if j == 1 or j % 50 == 0:\n",
    "            test_predictions = model(resultant_tensor)\n",
    "            #print(loss.item(), test_predictions.sum().item() / 8)\n",
    "            print(test_prediction(test_predictions))\n",
    "        loss_track.append(loss.item())\n",
    "        accuracy_track.append(test_predictions.sum().item() / 8)\n",
    "        #no_entropy += [test_predictions.sum().item() / 8]\n",
    "        #         k = 100 * i + j\n",
    "        #         x = np.arange(0, 2*k*np.pi/1000, 0.01)\n",
    "        #         y1, y2 = np.cos(x), np.sin(x)\n",
    "        #         graphs = [[x,y1], [x,y2]]\n",
    "        #         x_bounds = [0, 2*np.pi]\n",
    "        #         y_bounds = [-1,1]\n",
    "        #         mb.update_graph(graphs, x_bounds, y_bounds)\n",
    "        #         print(loss_track, accuracy_track)\n",
    "        #print(loss_track, accuracy_track)\n",
    "        #plot_loss_update(j, n_iterations, mb, loss_track, accuracy_track)\n",
    "        #for batch in progress_bar(range(2), parent=mb): sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b21b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "cont_keys = {}\n",
    "for key in filteredGeneCellLists:\n",
    "    cont_keys[key] = count\n",
    "    count += 1\n",
    "continuousFilteredGeneCellLists = {}\n",
    "for k in list(filteredGeneCellLists.keys()):\n",
    "    continuousFilteredGeneCellLists[cont_keys[k]] = filteredGeneCellLists[k]\n",
    "#continuousFilteredGeneCellLists\n",
    "#cont_keys\n",
    "#len(list(continuousFilteredGeneCellLists.keys()))\n",
    "#cellCountWithinGroup\n",
    "#zscore\n",
    "#continuousFilteredGeneCellLists check\n",
    "# x = cells in group(s) , cellCountWithinGroup\n",
    "# y = genes affected \n",
    "# z = cluster number\n",
    "#for each cell\n",
    "#make a graph -> \n",
    "#negative * negative = positive, \n",
    "#x  cluster \"name\" or index (clusters should change)\n",
    "#y = genes above/below threshold \n",
    "#z = total dist above threshold\n",
    "#convert 200 dimensions to 3\n",
    "cellGroups = [0 for i in list(range(5905))]\n",
    "cellGroupLengths = [0 for i in list(range(5905))]\n",
    "cellDistCounts = [0 for i in list(range(5905))]\n",
    "for column in continuousFilteredGeneCellLists:\n",
    "    for cell in continuousFilteredGeneCellLists[column]:\n",
    "        cellGroups[cell] = column\n",
    "        cellGroupLengths[cell] = len(continuousFilteredGeneCellLists[column])\n",
    "for idx, row in enumerate(mat_for_embed):\n",
    "    for val in row: \n",
    "        cellDistCounts[idx] += val\n",
    "        \n",
    "#cellDistCounts\n",
    "#https://en.wikipedia.org/wiki/Foundation%27s_Edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e50c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find gradient of row\n",
    "#given two rows that belong to same perturbation -> return identical or similar values\n",
    "#given a matrix -> return mx5 vals that can be transformed into a p-val\n",
    "#capture the 'features' that can be used to reconstruct -> molecular response\n",
    "# find the molecular response of each phenotype interaction or simply the gene by itself\n",
    "#gradient ascent -> descent -> find distributions -> sparsify them\n",
    "#ring that captures relevant known info about you and stores it cryptographically \n",
    "#given n rows and a matrix -> return a tuple that can be used to identify rows which belong to a perturbation response\n",
    "#given an expression matrix -> group cells by perturbation profiles\n",
    "#transcriptomics, genomics, proteinomics, metabolomics\n",
    "#recorded actions -> comic generator\n",
    "#script -> comic generator\n",
    "#comic -> animation generator\n",
    "#$https://www.youtube.com/watch?v=DzNmUNvnB04\n",
    "#plot the matrix before + after - 200x6k to 3x6k -> bright colors for rows with perturbations \n",
    "#perturbations defined as belonging to a group of rows that have multiple columns that are covarying from mean-zscore\n",
    "#makeCoolStuff = [[float(k) for k in range(5905)] for i in range(200)]\n",
    "#https://explained.ai/regularization/index.html\n",
    "#oft constraint with non-regularized loss function (blue-red) term and penalty term (orange).\n",
    "#invent a new architecture \n",
    "#that captures probability of perturbation across a matrix\n",
    "#https://www.10xgenomics.com/resources/datasets/5-k-a-549-lung-carcinoma-cells-no-treatment-transduced-with-a-crispr-pool-3-1-standard-6-0-0\n",
    "# all_url = [\n",
    "# #     \"https://zenodo.org/record/7416068/files/AdamsonWeissman2016_GSM2406675_10X001.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/AdamsonWeissman2016_GSM2406677_10X005.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/AdamsonWeissman2016_GSM2406681_10X010.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/AissaBenevolenskaya2021.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/ChangYe2021.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/DatlingerBock2017.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/DatlingerBock2021.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/DixitRegev2016.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/FrangiehIzar2021_protein.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/FrangiehIzar2021_RNA.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/GasperiniShendure2019_atscale.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/GasperiniShendure2019_highMOI.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/GasperiniShendure2019_lowMOI.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/GehringPachter2019.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/McFarlandTsherniak2020.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/NormanWeissman2019_filtered.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/PapalexiSatija2021_eccite_arrayed_protein.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/PapalexiSatija2021_eccite_arrayed_RNA.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/PapalexiSatija2021_eccite_protein.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/PapalexiSatija2021_eccite_RNA.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/ReplogleWeissman2022_K562_essential.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/ReplogleWeissman2022_K562_gwps.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/ReplogleWeissman2022_rpe1.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SchiebingerLander2019_GSE106340.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SchiebingerLander2019_GSE115943.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SchraivogelSteinmetz2020_TAP_SCREEN__chromosome_11_screen.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SchraivogelSteinmetz2020_TAP_SCREEN__chromosome_8_screen.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/ShifrutMarson2018.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SrivatsanTrapnell2020_sciplex2.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SrivatsanTrapnell2020_sciplex3.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SrivatsanTrapnell2020_sciplex4.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/TianKampmann2019_day7neuron.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/TianKampmann2019_iPSC.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/TianKampmann2021_CRISPRa.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/TianKampmann2021_CRISPRi.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/WeinrebKlein2020.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/XieHon2017.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/ZhaoSims2021.h5ad?download=1\"\n",
    "# ]\n",
    "#scarches.dataset.remove_sparsity(adata)\n",
    "#https://docs.scarches.org/en/latest/api/models.html\n",
    "# mdata = muon.read_10x_h5(\"pbmc_10k_protein_v3_filtered_feature_bc_matrix.h5\")\n",
    "# scvi.model.TOTALVI.setup_mudata(mdata, modalities={\"rna_layer\": \"rna\": \"protein_layer\": \"prot\"})\n",
    "# vae = scvi.model.TOTALVI(mdata)\n",
    "#https://docs.scvi-tools.org/en/stable/api/reference/scvi.module.LDVAE.html\n",
    "#[i for i in test_predictions.tolist() if i < 1]\n",
    "# Regularization in Logistic Regression\n",
    "# Regularization is extremely important in logistic regression modeling. Without regularization, the asymptotic nature of logistic regression would keep driving loss towards 0 in high dimensions. Consequently, most logistic regression models use one of the following two strategies to dampen model complexity:\n",
    "# L2 regularization.\n",
    "# Early stopping, that is, limiting the number of training steps or the learning rate.\n",
    "# (We'll discuss a third strategyL1 regularizationin a later module.)\n",
    "# Imagine that you assign a unique id to each example, and map each id to its own feature. If you don't specify a regularization function, the model will become completely overfit. That's because the model would try to drive loss to zero on all examples and never get there, driving the weights for each indicator feature to +infinity or -infinity. This can happen in high dimensional data with feature crosses, when theres a huge mass of rare crosses that happen only on one example each.\n",
    "# Fortunately, using L2 or early stopping will prevent this problem.\n",
    "#[ x for x in [iden(sum(item), 10)  for item in test_predictions.tolist()] if x > .1]\n",
    "#plot(loss_track)\n",
    "#make demo = good\n",
    "\n",
    "def plot_loss(l):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    legends = []\n",
    "#     blue = [i for k,i in enumerate(rowGeneExpression.values()) if dependent_variables[k]]\n",
    "#     oj =[i for k,i in enumerate(rowGeneExpression.values()) if not dependent_variables[k]]\n",
    "#     blue.sort()\n",
    "#     oj.sort()\n",
    "#     plt.plot((blue)) #blue true peturbation \n",
    "    plt.plot(l) #orange false ctrl\n",
    "    #legends.append('param %d' % i)\n",
    "    plt.plot([0, len([i for k,i in enumerate(rowGeneExpression.values()) if dependent_variables[k]])], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "    plt.legend(legends);\n",
    "# #https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02021-3\n",
    "# # Medicine Finding anomalies in radiology images, including CT, MRI, and X-ray images; counting features in pathology slides; measuring features in ultrasounds; diagnosing diabetic retinopathy\n",
    "# #Biology Folding proteins; classifying proteins; many genomics tasks, such as tumor-normal sequencing and classifying clinically actionable genetic mutations; cell classification; analyzing protein/protein interactions\n",
    "# #Other applications Financial and logistical forecasting, text to speech, and much more\n",
    "# # humor analysis - larry david vs seinfeld ? \n",
    "#https://www.kaggle.com/code/jhoward/why-you-should-use-a-framework\n",
    "#handle \"values outside of domain\" by \"SVM\"\n",
    "#random forest classifier\n",
    "#logisitc regression - hard to get right\n",
    "#correct transformations, outlier handling, correct interactions\n",
    "#os.listdir('./data_sets')\n",
    "#wget -m http://www.example.com 2>&1 | grep '^--' | awk '{ print $3 }' | grep -v '\\.\\(css\\|js\\|png\\|gif\\|jpg\\|JPG\\)$' > urls.txt\n",
    "#https://academic.oup.com/bib/article/22/4/bbaa268/5943793\n",
    "#plot(loss_track)\n",
    "#https://terrytao.files.wordpress.com/2011/02/matrix-book.pdf\n",
    "#https://academic.oup.com/bioinformatics/article/36/Supplement_2/i610/6055927?login=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b693a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #sc.pl.StackedViolin(adata, , groupby='', use_raw=None, log=False, num_categories=7, categories_order=None, title=None, figsize=None, gene_symbols=None, var_group_positions=None, var_group_labels=None, var_group_rotation=None, layer=None, standard_scale=None, ax=None, vmin=None, vmax=None, vcenter=None, norm=None)\n",
    "# sc.pl.StackedViolin(adata, list(hv_genes), groupby='perturbation', dendrogram=True).show()\n",
    "# hg = list(hv_genes)[100:]\n",
    "# sc.pl.DotPlot(adata, hg,  groupby='perturbation').show()\n",
    "# sc.pl.MatrixPlot(adata, hg, groupby='perturbation').show()\n",
    "# first = adata.X.A[:100]\n",
    "# second = adata.X.T.A[:100]\n",
    "# perturbations = []\n",
    "# for key, row in enumerate(first):\n",
    "#     trackPerts = []\n",
    "#     for column in row:\n",
    "#         if column > 0: trackPerts.append(column)\n",
    "#     print(t_dep[key].item(), len(trackPerts))\n",
    "#https://datahacker.rs/003-gans-autoencoder-implemented-with-pytorch/\n",
    "#https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/\n",
    "#https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf\n",
    "#https://www.cs.utoronto.ca/~hinton/absps/cogscibm.pdf\n",
    "#Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\n",
    "# Pierre-Antoine Manzagol. 2008. Extracting and\n",
    "# composing robust features with denoising autoencoders. In Proceedings of the 25th international\n",
    "# conference on Machine learning, pages 10961103.\n",
    "# ACM.\n",
    "#https://github.com/fastai/course22p2/blob/master/nbs/08_autoencoder.ipynb\n",
    "#file:///Users/adnanwahab/Downloads/Molecular%20Systems%20Biology%20-%202016%20-%20Angermueller.pdf\n",
    "#https://www.cell.com/patterns/pdf/S2666-3899(21)00001-5.pdf\n",
    "#https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC3.ipynb\n",
    "##https://www.genome.gov/research-funding/Funded-Programs-Projects/Multi-Omics-for-Health-and-Disease\n",
    "#IFrame('https://www.shadertoy.com/embed/dlScDy?gui=true&t=10&paused=true&muted=false', width=700, height=350)\n",
    "#https://github.com/AntixK/PyTorch-VAE/blob/master/models/lvae.py\n",
    "# from torchvision.datasets import CIFAR10\n",
    "# import os\n",
    "# import urllib.request\n",
    "# from urllib.error import HTTPError\n",
    "\n",
    "# import lightning as L\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib_inline.backend_inline\n",
    "# import seaborn as sns\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# import torch.utils.data as data\n",
    "# import torchvision\n",
    "# from lightning.pytorch.callbacks import Callback, LearningRateMonitor, ModelCheckpoint\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# from torchvision import transforms\n",
    "# from torchvision.datasets import CIFAR10\n",
    "# from tqdm.notebook import tqdm\n",
    "# model_dict = {}\n",
    "\n",
    "# class GenerateCallback(Callback):\n",
    "#     def __init__(self, input_imgs, every_n_epochs=1):\n",
    "#         super().__init__()\n",
    "#         self.input_imgs = input_imgs  # Images to reconstruct during training\n",
    "#         # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "#         self.every_n_epochs = every_n_epochs\n",
    "\n",
    "#     def on_train_epoch_end(self, trainer, pl_module):\n",
    "#         if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "#             # Reconstruct images\n",
    "#             input_imgs = self.input_imgs.to(pl_module.device)\n",
    "#             with torch.no_grad():\n",
    "#                 pl_module.eval()\n",
    "#                 reconst_imgs = pl_module(input_imgs)\n",
    "#                 pl_module.train()\n",
    "#             # Plot and add to tensorboard\n",
    "#             imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0, 1)\n",
    "#             grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, range=(-1, 1))\n",
    "#             trainer.logger.experiment.add_image(\"Reconstructions\", grid, global_step=trainer.global_step)    \n",
    "    \n",
    "# def train_cifar(latent_dim):\n",
    "#     # Create a PyTorch Lightning trainer with the generation callback\n",
    "#     trainer = L.Trainer(\n",
    "#         default_root_dir=os.path.join(CHECKPOINT_PATH, \"cifar10_%i\" % latent_dim),\n",
    "#         accelerator=\"auto\",\n",
    "#         devices=1,\n",
    "#         max_epochs=500,\n",
    "#         callbacks=[\n",
    "#             ModelCheckpoint(save_weights_only=True),\n",
    "#             GenerateCallback(get_train_images(8), every_n_epochs=10),\n",
    "#             LearningRateMonitor(\"epoch\"),\n",
    "#         ],\n",
    "#     )\n",
    "#     trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "#     trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "#     # Check whether pretrained model exists. If yes, load it and skip training\n",
    "#     pretrained_filename = os.path.join(CHECKPOINT_PATH, \"cifar10_%i.ckpt\" % latent_dim)\n",
    "#     if os.path.isfile(pretrained_filename):\n",
    "#         print(\"Found pretrained model, loading...\")\n",
    "#         model = Autoencoder.load_from_checkpoint(pretrained_filename)\n",
    "#     else:\n",
    "#         model = Autoencoder(base_channel_size=32, latent_dim=latent_dim)\n",
    "#         trainer.fit(model, train_loader, val_loader)\n",
    "#     # Test best model on validation and test set\n",
    "#     val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
    "#     test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "#     result = {\"test\": test_result, \"val\": val_result}\n",
    "#     return model, result\n",
    "\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#            num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "#            base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "#            latent_dim : Dimensionality of latent representation z\n",
    "#            act_fn : Activation function used throughout the decoder network\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         c_hid = base_channel_size\n",
    "#         self.linear = nn.Sequential(nn.Linear(latent_dim, 2 * 16 * c_hid), act_fn())\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(\n",
    "#                 2 * c_hid, 2 * c_hid, kernel_size=3, output_padding=1, padding=1, stride=2\n",
    "#             ),  # 4x4 => 8x8\n",
    "#             act_fn(),\n",
    "#             nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "#             act_fn(),\n",
    "#             nn.ConvTranspose2d(2 * c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),  # 8x8 => 16x16\n",
    "#             act_fn(),\n",
    "#             nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "#             act_fn(),\n",
    "#             nn.ConvTranspose2d(\n",
    "#                 c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2\n",
    "#             ),  # 16x16 => 32x32\n",
    "#             nn.Tanh(),  # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear(x)\n",
    "#         x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "#         x = self.net(x)\n",
    "#         return x    \n",
    "    \n",
    "# class Autoencoder(L.LightningModule):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         base_channel_size: int,\n",
    "#         latent_dim: int,\n",
    "#         encoder_class: object = Encoder,\n",
    "#         decoder_class: object = Decoder,\n",
    "#         num_input_channels: int = 3,\n",
    "#         width: int = 32,\n",
    "#         height: int = 32,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         # Saving hyperparameters of autoencoder\n",
    "#         self.save_hyperparameters()\n",
    "#         # Creating encoder and decoder\n",
    "#         self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "#         self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "#         # Example input array needed for visualizing the graph of the network\n",
    "#         self.example_input_array = torch.zeros(2, num_input_channels, width, height)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"The forward function takes in an image and returns the reconstructed image.\"\"\"\n",
    "#         z = self.encoder(x)\n",
    "#         x_hat = self.decoder(z)\n",
    "#         return x_hat\n",
    "\n",
    "#     def _get_reconstruction_loss(self, batch):\n",
    "#         \"\"\"Given a batch of images, this function returns the reconstruction loss (MSE in our case)\"\"\"\n",
    "#         x, _ = batch  # We do not need the labels\n",
    "#         x_hat = self.forward(x)\n",
    "#         loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "#         loss = loss.sum(dim=[1, 2, 3]).mean(dim=[0])\n",
    "#         return loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "#         # Using a scheduler is optional but can be helpful.\n",
    "#         # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "#         scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.2, patience=20, min_lr=5e-5)\n",
    "#         return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         loss = self._get_reconstruction_loss(batch)\n",
    "#         self.log(\"train_loss\", loss)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         loss = self._get_reconstruction_loss(batch)\n",
    "#         self.log(\"val_loss\", loss)\n",
    "\n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         loss = self._get_reconstruction_loss(batch)\n",
    "#         self.log(\"test_loss\", loss)\n",
    "\n",
    "\n",
    "# from torchvision import transforms\n",
    "# #%matplotlib inline\n",
    "# #matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "# #matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "# sns.reset_orig()\n",
    "# sns.set()\n",
    "\n",
    "# # Tensorboard extension (for visualization purposes later)\n",
    "\n",
    "# # Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "# DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data\")\n",
    "# # Path to the folder where the pretrained models are saved\n",
    "# CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/tutorial9\")\n",
    "\n",
    "# # Setting the seed\n",
    "# L.seed_everything(42)\n",
    "\n",
    "# # Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# print(\"Device:\", device)\n",
    "# #model = Autoencoder(base_channel_size=32, latent_dim=latent_dim)\n",
    "\n",
    "# m = Encoder(200, 50, 10)\n",
    "# #m(data)\n",
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# # Loading the training dataset. We need to split it into a training and validation part\n",
    "# # train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "# # L.seed_everything(42)\n",
    "# # train_set, val_set = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "\n",
    "# # # Loading the test set\n",
    "# # test_set = CIFAR10(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# # # We define a set of data loaders that we can use for various purposes later.\n",
    "# # train_loader = data.DataLoader(train_set, batch_size=256, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "# # val_loader = data.DataLoader(val_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "# # test_loader = data.DataLoader(test_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "\n",
    "# def get_train_images(num):\n",
    "#     return torch.stack([train_dataset[i][0] for i in range(num)], dim=0)\n",
    "# base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial9/\"\n",
    "# # Files to download\n",
    "# pretrained_files = [\"cifar10_64.ckpt\", \"cifar10_128.ckpt\", \"cifar10_256.ckpt\", \"cifar10_384.ckpt\"]\n",
    "# # Create checkpoint path if it doesn't exist yet\n",
    "# os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# # For each file, check whether it already exists. If not, try downloading it.\n",
    "# # for file_name in pretrained_files:\n",
    "# #     file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "# #     if not os.path.isfile(file_path):\n",
    "# #         file_url = base_url + file_name\n",
    "# #         print(\"Downloading %s...\" % file_url)\n",
    "# #         try:\n",
    "# #             urllib.request.urlretrieve(file_url, file_path)\n",
    "# #         except HTTPError as e:\n",
    "# #             print(\n",
    "# #                 \"Something went wrong. Please try to download the files manually,\"\n",
    "# #                 \" or contact the author with the full output including the following error:\\n\",\n",
    "# #                 e,\n",
    "# #             )\n",
    "# def visualize_reconstructions(model, input_imgs):\n",
    "#     # Reconstruct images\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         reconst_imgs = model(input_imgs.to(model.device))\n",
    "#     reconst_imgs = reconst_imgs.cpu()\n",
    "\n",
    "#     # Plotting\n",
    "#     imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0, 1)\n",
    "#     grid = torchvision.utils.make_grid(imgs, nrow=4, normalize=True, range=(-1, 1))\n",
    "#     grid = grid.permute(1, 2, 0)\n",
    "#     plt.figure(figsize=(7, 4.5))\n",
    "#     plt.title(\"Reconstructed from %i latents\" % (model.hparams.latent_dim))\n",
    "#     plt.imshow(grid)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "# input_imgs = get_train_images(4)\n",
    "# def find_similar_images(query_img, query_z, key_embeds, K=8):\n",
    "#     # Find closest K images. We use the euclidean distance here but other like cosine distance can also be used.\n",
    "#     dist = torch.cdist(query_z[None, :], key_embeds[1], p=2)\n",
    "#     dist = dist.squeeze(dim=0)\n",
    "#     dist, indices = torch.sort(dist)\n",
    "#     # Plot K closest images\n",
    "#     imgs_to_display = torch.cat([query_img[None], key_embeds[0][indices[:K]]], dim=0)\n",
    "#     grid = torchvision.utils.make_grid(imgs_to_display, nrow=K + 1, normalize=True, range=(-1, 1))\n",
    "#     grid = grid.permute(1, 2, 0)\n",
    "#     plt.figure(figsize=(12, 3))\n",
    "#     plt.imshow(grid)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "# # Plot the closest images for the first N test images as example\n",
    "\n",
    "# def embed_imgs(model, data_loader):\n",
    "#     # Encode all images in the data_laoder using model, and return both images and encodings\n",
    "#     img_list, embed_list = [], []\n",
    "#     model.eval()\n",
    "#     for imgs, _ in tqdm(data_loader, desc=\"Encoding images\", leave=False):\n",
    "#         with torch.no_grad():\n",
    "#             z = model.encoder(imgs.to(model.device))\n",
    "#         img_list.append(imgs)\n",
    "#         embed_list.append(z)\n",
    "#     return (torch.cat(img_list, dim=0), torch.cat(embed_list, dim=0))\n",
    "\n",
    "\n",
    "# train_img_embeds = embed_imgs(model, train_loader)\n",
    "# test_img_embeds = embed_imgs(model, test_loader)\n",
    "# for i in range(8):\n",
    "#     find_similar_images(test_img_embeds[0][i], test_img_embeds[1][i], key_embeds=train_img_embeds)     \n",
    "    \n",
    "\n",
    "# latent_vectors = torch.randn(8, model.hparams.latent_dim, device=model.device)\n",
    "# # with torch.no_grad():\n",
    "# #     imgs = model.decoder(latent_vectors)\n",
    "# #     imgs = imgs.cpu()\n",
    "\n",
    "# # grid = torchvision.utils.make_grid(imgs, nrow=4, normalize=True, range=(-1, 1), pad_value=0.5)\n",
    "# # grid = grid.permute(1, 2, 0)\n",
    "# # plt.figure(figsize=(8, 5))\n",
    "# # plt.imshow(grid)\n",
    "# # plt.axis(\"off\")\n",
    "# # plt.show()\n",
    "\n",
    "#! ls ./data_sets/* -lh\n",
    "#https://github.com/chriswi93/Neural-Networks-and-Logistic-Regression-Backpropagation-in-depth\n",
    "# ![Alt text](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-021-22197-x/MediaObjects/41467_2021_22197_Fig3_HTML.png?as=webp)\n",
    "# https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bib/22/4/10.1093_bib_bbaa268/1/m_bbaa268f1.jpeg?Expires=1695201196&Signature=1KEY92u4ZstK959i3C6haCKHZ7-6ghmNkBQwGELax4hVBn6N0o7lasyTNgnHk6sQ6eP2yiV~E51~X8JdkQkF9D5PfM7pk0N-z1rOF1HJpYaNBZ7IrUSqzdj-lQHw-TTBMjlW8rFKnSWg8~Y0y2y7q7a1hGweo3LHFNk7pSxu0kgYUaN54HwRrCWvpuMe0Eq~PL4oIh857EOSI9YaYyZ4U3ilKNy9bzbEHrLUiGOdfBBvJV09gq5g1Xp3rl49KqxwnpaFVs1qEj0z94TBYtJMDnUXEoV8ZXGJ2ESWxaXQRGziXBHA-b5l2Ac40c2eSVvTgqGFK2ClL0yGFZM5J458dg__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA\n",
    "#https://muon-tutorials.readthedocs.io/en/latest/trimodal/tea-seq/1-TEA-seq-PBMC.html\n",
    "#solve known perturbations 100%\n",
    "#solve unknown perturbations -> when exactly \n",
    "#predict perturbations before they occur -> multimodal\n",
    "#solve adjacent problems in preventative medicine#\n",
    "#put flask in notebook / torch script\n",
    "#https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed\n",
    "#-> make notebook where you can select all the files and learn everything there is to know in single cell omics (transcript, protein, metabolimcs )\n",
    "\n",
    "\n",
    "\n",
    "# The cancer sample matrix was normalized by the Z-score method, \n",
    "#which scaled the mean of each row (corresponding to feature edge) to zero and variance to one. \n",
    "#First, the rows of the matrix were clustered using hierarchical clustering based on the complete linkage method with the cluster number set to 100, \n",
    "#and clusters containing more than 30 edges were retained.\n",
    "#We then computed the mean values of perturbation for each edge in each subtype through Z-scores.\n",
    "#For each subtype, we counted the percentage of edges whose absolute value of the average perturbation was greater than 0.5 in each retained cluster. \n",
    "#A cluster with a percentage greater than 70% was regarded as a perturbed cluster for this subtype. \n",
    "#All edges in all of the perturbed clusters for each subtype constituted the subtype-specific networks.\n",
    "#All genes involved in each subtype-specific network were used for pathway enrichment analysis by Metascape (http://metascape.org). \n",
    "#The KEGG and Reactome pathways with a P-value less than 0.01 were retained. \n",
    "#Finally, the subtype-specific pathways were identified.\n",
    "#grouping based on shared genes\n",
    "#network = nodes = cell\n",
    "#edges = shared gene expression above mean -> only retain those above 30 \n",
    "#graeter than > .5 of the zscore\n",
    "#a cluster with a percentage greater than ??? (look at ribosomes)\n",
    "# https://metascape.org/blog/\n",
    "# ##   *\n",
    "# #   /_\\\n",
    "# #  (@@)\n",
    "# #---T----\n",
    "# #  /\\\n",
    "# #_|  \\_\n",
    "#https://www.genecards.org/cgi-bin/carddisp.pl?gene=A1BG\n",
    "#https://cancer.sanger.ac.uk/cosmic#:~:text=COSMIC%2C%20the%20Catalogue%20Of%20Somatic,%2C%20mutation%2C%20etc.%20below.\n",
    "#https://observablehq.com/d/124e11318fe98788\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0af383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### https://www.kaggle.com/code/llttyy/open-problem-biological-ideas/notebook\n",
    "#https://www.kaggle.com/competitions/open-problems-multimodal/discussion/366961\n",
    "#extrapolate sca onto mm datasets\n",
    "# 1. train algo on all data sets available\n",
    "# 2. apply to all multi-omics datasets aviable\n",
    "# 3. identify features which have statistical correlation\n",
    "# 4. improve multi-view algo until it predicts perturbations at earliest possible onset -> \n",
    "# 5. find adjacent problems worth solving\n",
    "\n",
    "#invent algorithm for detecting pertrubations \n",
    "#loop through every high_variable_column\n",
    "#loop through every row\n",
    "#fidn rows where 1 gene is above zscore\n",
    "# find orthogonal features of 7000 element matrix\n",
    "# cant do that without iteration to find nonlinear statistical relationship occurances\n",
    "# remove noise\n",
    "# find where columns overlap \n",
    "# attempt to cluster in batches of 100\n",
    "# happiest bear in the world - (infinite thank you + apologetic) 4ever\n",
    "# dont affect stream negatively - always think of effect on others \n",
    "# dont take any breaks\n",
    "# finsih this problem by noon - continue everyday forever for 100 years\n",
    "# https://zenodo.org/record/6546964\n",
    "# classify how different perturbations contain different profiles of information distance\n",
    "\n",
    "#many people just want a cool matrix transform that encodes probabilitiy from gene expression matrix\n",
    "# we dont want dimensionality reduction\n",
    "#most people just want it to work asap and dont care \n",
    "#eggnog wants to be silent typist and wants everyone in stream to be happy - also finish by noon \n",
    "#timebox - what can be solved in 2-12?\n",
    "#go to austin tomorrow - chill w/ computer\n",
    "\n",
    "#5000 x 2000 = too slow for python\n",
    "#other datasets 100x more data\n",
    "\n",
    "#determine what causes the variation in the gene expression profile\n",
    "#which gene contribute most magnitude in amplitude\n",
    "#filter data that isnt relevant \n",
    "#whats left is is just the perturbations (depending on experiment intent and data)\n",
    "#https://proceedings.mlr.press/v108/zhao20c/zhao20c.pdf\n",
    "\n",
    "\n",
    "# how would 1 billion people message each other simultaneously - telepathy may in fact solve this -> qualia->save to to disk\n",
    "# one community\n",
    "# 3-5 topics a day\n",
    "# topics stay relevant as long ast they get votes\n",
    "# pass\n",
    "# add comment threads - no firebase\n",
    "# how would you get traction -> have to send it to right people -> offer tons of free shit \n",
    "# good security - traceless \n",
    "# inner loop - how discussion changes -> what is the consensus in the community about how to solve a problem?? \n",
    "# 3-5 topics -> each has 3-5 top threads\n",
    "# make it fun somehow https://freefrontend.com/css-carousels/ -> filter lots of stuff  \n",
    "\n",
    "# use LLM to merge comments -> already been said - might be fun part -> train on whether dicussion makes someone happy\n",
    "# hover to upvote\n",
    "# what do people want to discuss? \n",
    "\n",
    "\n",
    "#be ready to make 2-3 demos -> 2-3 days see which will stick\n",
    "#media server search by text\n",
    "#cooperation.party -> one conversation -> rolling window \n",
    "#https://threejs.org/examples/#webgl_modifier_curve_instanced\n",
    "#use LLM very well -> have __very__ __good__ discussions - music\n",
    "#table this \n",
    "\n",
    "# create adventures to do together -> add plot or agenda\n",
    "# #0 rule is something actually cool\n",
    "# Expedience is #2 rule\n",
    "\n",
    "# figure out something we want to figure out\n",
    "\n",
    "# think of problems that need solving\n",
    "# think of solutions that can be improved\n",
    "# build a community - like ours - people have to know who you are in a community "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5538ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://foresight.org/summary/simon-durr-designing-stable-metalloproteins-using-deep-learning/\n",
    "#https://www.youtube.com/watch?v=_gXiVOmaVSo&t=1028s&ab_channel=ForesightInstitute\n",
    "#ghost in the shell episode 13 season 2\n",
    "#https://www.youtube.com/watch?v=Nbmnx0hTPjA&ab_channel=DartmouthEngineering\n",
    "#http://book.bionumbers.org/wp-content/uploads/2020/04/SARS-CoV-2_BTN_0401.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f516918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install cellrank\n",
    "#! cellblast\n",
    "\n",
    "#https://scverse.org/packages/#core-packages\n",
    "#https://github.com/aristoteleo/dynamo-release\n",
    "\n",
    "#genome sequence corn and algae - select\n",
    "#design crispr guide RNA for them\n",
    "#???\n",
    "#predict perturbations -> \"this edit may cause this yy\"\n",
    "#visualize molecular response\n",
    "#\n",
    "#send specs to lab and get kernels back\n",
    "#use new plant \n",
    "#https://github.com/KANG-BIOINFO/CellDrift\n",
    "#https://academic.oup.com/bib/article/23/5/bbac324/6673850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8114ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.ouseful.info/2021/09/30/a-simple-pattern-for-embedding-third-party-javascript-generated-graphics-in-jupyter-notebools/\n",
    "#https://www.biorxiv.org/content/10.1101/2022.07.20.500854v1\n",
    "#https://pubmed.ncbi.nlm.nih.gov/35625556/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
