{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "77be3776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symA3_37.fasta\n",
      "symA3_37.fasta.gz.1\n",
      "gzip: symA3_37.fasta.gz: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#https://genome.jgi.doe.gov/portal/pages/dynamicOrganismDownload.jsf?organism=Chlremi1\n",
    "#design fusion of two proteins\n",
    "#https://observablehq.com/d/124e11318fe98788\n",
    "#canu -d SLA-04_q100_Canu_assembly -p SLA-04 genomeSize=55m -nanopore-raw /home/calvinc/SLA-04_fast5_original/fastq_skipped+passed/*.fastq\n",
    "#https://www.google.com/search?q=algae+gneome+.fasta&sca_esv=558593241&sxsrf=AB5stBilP6j3p89gvHZ0GXMjNzQtcR3Q-A%3A1692557692181&ei=fGHiZMHPCoKrqtsPo7GTkAM&ved=0ahUKEwjB_YPg9OuAAxWClWoFHaPYBDIQ4dUDCBE&uact=5&oq=algae+gneome+.fasta&gs_lp=Egxnd3Mtd2l6LXNlcnAiE2FsZ2FlIGduZW9tZSAuZmFzdGEyBxAhGKABGApI_gpQJ1jECHAAeAKQAQCYAZUBoAHHBaoBAzQuM7gBA8gBAPgBAcICBBAAGEfCAgcQABgNGIAEwgIGEAAYFhgewgIIEAAYFhgeGA_CAgUQIRigAeIDBBgAIEGIBgGQBgU&sclient=gws-wiz-serp#ip=1\n",
    "#https://chempert.uni.lu/transcriptionalresponse/RID16890\n",
    "\n",
    "#!wget https://marinegenomics.oist.jp/symb/download/symA3_37.fasta.gz\n",
    "# !ls | grep symA3\n",
    "# !gunzip symA3_37.fasta.gz\n",
    "#run experiments\n",
    "\n",
    "#convolutions on nucoleotide basepairs -> lookign for patterns \n",
    "# make algae do more cool stuff -> increase yield, disease resitant, less water, faster thing, longer lifespan  \n",
    "#https://www.kaggle.com/competitions/open-problems-multimodal/data\n",
    "\n",
    "\n",
    "\n",
    "#4 step pipeline -> each step is programmable -> different workflows -> this one is for a different lab and a different problem\n",
    "#do a different problem -> crispr for plants, or make it generalizable across lots of workflows \n",
    "#\n",
    "#1 day\n",
    "# assemble routing structure for RNA to connect to\n",
    "#s guide rna\n",
    "#ribosome viewer\n",
    "#transcriptome design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f04e8db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ File filtered_feature_bc_matrix.h5 from brain3k_multiome has been found at /home/awahab/mudatasets/brain3k_multiome/filtered_feature_bc_matrix.h5\n",
      "■ Checksum is validated (md5) for filtered_feature_bc_matrix.h5\n",
      "■ File atac_fragments.tsv.gz from brain3k_multiome has been found at /home/awahab/mudatasets/brain3k_multiome/atac_fragments.tsv.gz\n",
      "■ Checksum is validated (md5) for atac_fragments.tsv.gz\n",
      "■ File atac_fragments.tsv.gz.tbi from brain3k_multiome has been found at /home/awahab/mudatasets/brain3k_multiome/atac_fragments.tsv.gz.tbi\n",
      "■ Checksum is validated (md5) for atac_fragments.tsv.gz.tbi\n",
      "■ File atac_peaks.bed from brain3k_multiome has been found at /home/awahab/mudatasets/brain3k_multiome/atac_peaks.bed\n",
      "■ Checksum is validated (md5) for atac_peaks.bed\n",
      "■ File atac_peak_annotation.tsv from brain3k_multiome has been found at /home/awahab/mudatasets/brain3k_multiome/atac_peak_annotation.tsv\n",
      "■ Checksum is validated (md5) for atac_peak_annotation.tsv\n",
      "■ Loading filtered_feature_bc_matrix.h5...\n",
      "Added `interval` annotation for features from /home/awahab/mudatasets/brain3k_multiome/filtered_feature_bc_matrix.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awahab/anaconda3/envs/shit/lib/python3.10/site-packages/mudata/_core/mudata.py:710: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data_mod.loc[:, colname] = col\n",
      "/home/awahab/anaconda3/envs/shit/lib/python3.10/site-packages/mudata/_core/mudata.py:710: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  data_mod.loc[:, colname] = col\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added peak annotation from /home/awahab/mudatasets/brain3k_multiome/atac_peak_annotation.tsv to .uns['atac']['peak_annotation']\n",
      "Added gene names to peak annotation in .uns['atac']['peak_annotation']\n",
      "Located fragments file: /home/awahab/mudatasets/brain3k_multiome/atac_fragments.tsv.gz\n",
      "pysam is not available. It is required to work with the fragments file.                 Install pysam from PyPI (`pip install pysam`)                 or from GitHub (`pip install git+https://github.com/pysam-developers/pysam`)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>MuData object with n_obs ￗ n_vars = 3233 ￗ 170631\n",
       "  var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;genome&#x27;, &#x27;interval&#x27;\n",
       "  2 modalities\n",
       "    rna:\t3233 x 36601\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;genome&#x27;, &#x27;interval&#x27;\n",
       "    atac:\t3233 x 134030\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;genome&#x27;, &#x27;interval&#x27;\n",
       "      uns:\t&#x27;atac&#x27;</pre>"
      ],
      "text/plain": [
       "MuData object with n_obs × n_vars = 3233 × 170631\n",
       "  var:\t'gene_ids', 'feature_types', 'genome', 'interval'\n",
       "  2 modalities\n",
       "    rna:\t3233 x 36601\n",
       "      var:\t'gene_ids', 'feature_types', 'genome', 'interval'\n",
       "    atac:\t3233 x 134030\n",
       "      var:\t'gene_ids', 'feature_types', 'genome', 'interval'\n",
       "      uns:\t'atac'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import mudatasets\n",
    "# mdata = mudatasets.load(\"brain3k_multiome\", full=True)\n",
    "# mdata.var_names_make_unique()\n",
    "import mudatasets\n",
    "mdata = mudatasets.load(\"brain3k_multiome\", full=True)\n",
    "#mdata.var_names_make_unique()\n",
    "mdata\n",
    "#https://cpa-tools.readthedocs.io/en/latest/tutorials/GSM.html\n",
    "#https://thedataquarry.com/posts/vector-db-4/\n",
    "#invent something in blue biotech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "81c950cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://support.10xgenomics.com/single-cell-multiome-atac-gex/datasets/1.0.0/pbmc_granulocyte_sorted_10k\n",
    "# #https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.2/5k_pbmc_protein_v3\n",
    "# import os\n",
    "\n",
    "# import muon as mu\n",
    "# import muon.atac as ac\n",
    "# import muon.prot as pt\n",
    "\n",
    "# import matplotlib\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# from glob import glob\n",
    "\n",
    "# data_dir = \"./\"\n",
    "\n",
    "# for file in _(f\"{data_dir}/GSM5123951*\"):\n",
    "#     print(file)\n",
    "    \n",
    "# get_h5_file = lambda root, s, w: f\"{root}/{s}_X066-MP0C1{w}_leukopak_perm-cells_tea_200M_cellranger-arc_filtered_feature_bc_matrix.h5\"\n",
    "\n",
    "\n",
    "# meta = {}\n",
    "# for file in glob(f\"{data_dir}/GSM5123951*\"):\n",
    "#     tokens = os.path.basename(file).split(\"_\")\n",
    "#     meta[\n",
    "#         tokens[0]       # GSM5123951\n",
    "#     ] = tokens[1][-2:]  # W3\n",
    "\n",
    "# meta\n",
    "# s, w = list(meta.items())[0]\n",
    "\n",
    "\n",
    "# def reArrangingOrigamiBlocks():\n",
    "#     return 'aligning em microscopy images'\n",
    "#     return 'particle images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3f95e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdata.obs[\"sample\"] = s\n",
    "# mdata.obs[\"well\"] = w\n",
    "\n",
    "# mdata = mu.read_10x_h5(\n",
    "#     get_h5_file(data_dir, s, w)\n",
    "# )\n",
    "\n",
    "# mdata.update()\n",
    "# mdata.var_names_make_unique()\n",
    "\n",
    "# plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# mu.pl.embedding(mdata, basis=\"X_wnn_umap\", color=list(map(\n",
    "#     lambda x: \"prot:\" + x,\n",
    "#     [\n",
    "#         \"CD4\", \"CD45RA\", \"CD45RO\",  # CD4 T cells\n",
    "#         \"CD8a\",                     # CD8 T cells\n",
    "#         \"KLRG1\",                    # NK cells\n",
    "#         \"CD14\", \"CD16\",             # monocytes\n",
    "#         \"CD19\", \"CD21\", \"IgD\",      # B cells\n",
    "#     ]\n",
    "# )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d538764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "results_file = 'write/pbmc3k.h5ad'  # the file that will store the analysis results\n",
    "adata = sc.read_10x_mtx(\n",
    "    'data/filtered_gene_bc_matrices/hg19/',  # the directory with the `.mtx` file\n",
    "    var_names='gene_symbols',                # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)      \n",
    "sc.pp.highly_variable_genes(adata, \n",
    "                                layer=None, \n",
    "                                n_top_genes=200, \n",
    "                                min_disp=0.5, \n",
    "                                max_disp=1, \n",
    "                                min_mean=0.0125, \n",
    "                                max_mean=3, \n",
    "                                span=0.3, \n",
    "                                n_bins=20, \n",
    "                                flavor='seurat_v3', \n",
    "                                subset=False, \n",
    "                                inplace=True, \n",
    "                                batch_key=None, \n",
    "                                check_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3609c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncounts       ctrl =    7085.9976    pert =  7251.1616\n",
      "ngenes       ctrl =    1460.5028512358208    pert =  1472.5600646518021\n",
      "percent_mito       ctrl =    2.642741    pert =  2.799472\n",
      "percent_ribo       ctrl =    3.813907    pert =  3.8422022\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix, find\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas \n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "import torch, numpy as np, pandas as pd\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "rowGeneExpression2 = defaultdict(dict)\n",
    "import math\n",
    "import torch\n",
    "pandas.set_option('mode.use_inf_as_na', True)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "import scanpy as sc\n",
    "from torch import tensor\n",
    "import torch, numpy as np, pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch import tensor\n",
    "import torch, numpy as np, pandas as pd\n",
    "import torch.optim as optim\n",
    "from fastai.data.transforms import RandomSplitter\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix, tril\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "from fastprogress.fastprogress import master_bar \n",
    "np.set_printoptions(linewidth=140)\n",
    "torch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\n",
    "pd.set_option('display.width', 140)\n",
    "#one ='DatlingerBock2021.h5ad'\n",
    "#one = 'AissaBenevolenskaya2021.h5ad'\n",
    "#one = 'AissaBenevolenskaya2021.h5ad'\n",
    "folders = '/home/awahab/llm-testing/data_sets/'\n",
    "#one = 'AdamsonWeissman2016_GSM2406675_10X001.h5ad' #sigmoid returns nan in 0th frame\n",
    "one ='DatlingerBock2017.h5ad'\n",
    "one = 'AissaBenevolenskaya2021.h5ad'\n",
    "one = 'SrivatsanTrapnell2020_sciplex2.h5ad'\n",
    "one ='DatlingerBock2017.h5ad'\n",
    "#one = 'AdamsonWeissman2016_GSM2406675_10X001.h5ad'\n",
    "#one = 'AissaBenevolenskaya2021.h5ad'\n",
    "#one = 'XieHon2017.h5ad'\n",
    "#one = 'SrivatsanTrapnell2020_sciplex2.h5ad'\n",
    "def readFiles():\n",
    "    adata = sc.read_h5ad(folders + one)\n",
    "    one ='DatlingerBock2017.h5ad'\n",
    "    return adata\n",
    "adata = sc.read_h5ad(folders + one)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata, \n",
    "                                layer=None, \n",
    "                                n_top_genes=200, \n",
    "                                min_disp=0.5, \n",
    "                                max_disp=1, \n",
    "                                min_mean=0.0125, \n",
    "                                max_mean=3, \n",
    "                                span=0.3, \n",
    "                                n_bins=20, \n",
    "                                flavor='seurat_v3', \n",
    "                                subset=False, \n",
    "                                inplace=True, \n",
    "                                batch_key=None, \n",
    "                                check_values=True)\n",
    "\n",
    "sc.pp.pca(adata)\n",
    "found = find(adata.X)\n",
    "torch.manual_seed(440)\n",
    "#adata.obs.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n",
    "#adata.obs = adata.iloc[:5000]\n",
    "#adata.obs= adata.obs[adata.obs.iloc[:5000]]\n",
    "#adata.obs.iloc[:5000]\n",
    "#adata.var_names\n",
    "var_df = adata.var\n",
    "df = adata.obs#.iloc[:5000]\n",
    "df = df.drop(columns=['nperts'])\n",
    "df['percent_mito'] = 1\n",
    "def getMode(l): \n",
    "    return max(set(l), key=l.count)\n",
    "#sc.pp.filter_cells(adata, min_counts=None, min_genes=None, max_counts=None, max_genes=10, inplace=True, copy=False)\n",
    "#sc.pp.filter_genes(adata, min_counts=None, min_cells=None, max_counts=None, max_cells=None, inplace=True, copy=False)\n",
    "#sc.pp.highly_variable_genes(adata, layer=None, n_top_genes=None, min_disp=0.5, max_disp=inf, min_mean=0.0125, max_mean=3, span=0.3, n_bins=20, flavor='seurat', subset=False, inplace=True, batch_key=None, check_values=True)\n",
    "#sc.pp.regress_out(adata, keys, n_jobs=None, copy=False)\n",
    "#cell perturbation is defined as molecular response or gene expression that is different to what is \"normal\"\n",
    "from IPython.display import IFrame\n",
    "#check for expression values that are equal from crispr\n",
    "#join with gene ontology\n",
    "#this is a program\n",
    "#input an adata file\n",
    "#outputs a list of cell-IDs and the genes perturbed \n",
    "#and then what that gene does \n",
    "#and what interactions may occur with those perturbations \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder1 = LabelEncoder()\n",
    "label_encoder2 = LabelEncoder()\n",
    "label_encoder3 = LabelEncoder()\n",
    "#df['chembl-ID'] = label_encoder1.fit_transform(df['chembl-ID'])\n",
    "df['perturbation_2'] = label_encoder1.fit_transform(df['perturbation_2'])\n",
    "df['target_2'] = label_encoder2.fit_transform(df['target'])\n",
    "cool_columns = 'ncounts ngenes percent_mito percent_ribo'.split(' ')\n",
    "for key in cool_columns:\n",
    "    ct = adata.obs[adata.obs['perturbation'] == 'control'][key].std()\n",
    "    pt = adata.obs[adata.obs['perturbation'] != 'control'][key].std()\n",
    "    print(key, '      ctrl =   ', ct, '   pert = ', pt)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#https://github.com/scipy/scipy/blob/v1.11.2/scipy/sparse/_compressed.py#L1158-L1165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2cc7f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('../')\n",
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "\n",
    "# import scanpy as sc\n",
    "# import torch\n",
    "# import scarches as sca\n",
    "# from scarches.dataset.trvae.data_handling import remove_sparsity\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import gdown\n",
    "\n",
    "# from absl import flags\n",
    "# flags.DEFINE_string(\"flags\", None, \"flag for host IP address\")\n",
    "\n",
    "# sc.settings.set_figure_params(dpi=200, frameon=False)\n",
    "# sc.set_figure_params(dpi=200)\n",
    "# sc.set_figure_params(figsize=(4, 4))\n",
    "# torch.set_printoptions(precision=3, sci_mode=False, edgeitems=7)\n",
    "\n",
    "# target_conditions = [\"10X\"]\n",
    "# source_adata = adata[~adata.obs.study.isin(target_conditions)].copy()\n",
    "# target_adata = adata[adata.obs.study.isin(target_conditions)].copy()\n",
    "# print(source_adata)\n",
    "# print(target_adata)\n",
    "\n",
    "# sca.models.SCVI.setup_anndata(source_adata, batch_key=\"batch\")\n",
    "\n",
    "# vae = sca.models.SCVI(\n",
    "#     source_adata,\n",
    "#     n_layers=2,\n",
    "#     encode_covariates=True,\n",
    "#     deeply_inject_covariates=False,\n",
    "#     use_layer_norm=\"both\",\n",
    "#     use_batch_norm=\"none\",\n",
    "# )\n",
    "\n",
    "# early_stopping_kwargs = {\n",
    "#     \"early_stopping_metric\": \"elbo\",\n",
    "#     \"save_best_state_metric\": \"elbo\",\n",
    "#     \"patience\": 10,\n",
    "#     \"threshold\": 0,\n",
    "#     \"reduce_lr_on_plateau\": True,\n",
    "#     \"lr_patience\": 8,\n",
    "#     \"lr_factor\": 0.1,\n",
    "# }\n",
    "# vae.train(n_epochs=500, frequency=1, early_stopping_kwargs=early_stopping_kwargs)\n",
    "\n",
    "\n",
    "# reference_latent = sc.AnnData(vae.get_latent_representation())\n",
    "# reference_latent.obs[\"cell_type\"] = source_adata.obs[\"final_annotation\"].tolist()\n",
    "# reference_latent.obs[\"batch\"] = source_adata.obs[\"batch\"].tolist()\n",
    "# sc.pp.neighbors(reference_latent, n_neighbors=8)\n",
    "# sc.tl.leiden(reference_latent)\n",
    "# sc.tl.umap(reference_latent)\n",
    "# sc.pl.umap(reference_latent,\n",
    "#            color=['batch', 'cell_type'],\n",
    "#            frameon=False,\n",
    "#            wspace=0.6,\n",
    "#            )\n",
    "\n",
    "# ref_path = 'ref_model/'\n",
    "# vae.save(ref_path, overwrite=True)\n",
    "\n",
    "# model = sca.models.SCVI.load_query_data(\n",
    "#     target_adata,\n",
    "#     ref_path,\n",
    "#     freeze_dropout = True,\n",
    "# )\n",
    "\n",
    "# query_latent = sc.AnnData(model.get_latent_representation())\n",
    "# query_latent.obs['cell_type'] = target_adata.obs[\"final_annotation\"].tolist()\n",
    "# query_latent.obs['batch'] = target_adata.obs[\"batch\"].tolist()\n",
    "\n",
    "# model.train(n_epochs=500, frequency=1, early_stopping_kwargs=early_stopping_kwargs, weight_decay=0)\n",
    "# sc.pp.neighbors(query_latent)\n",
    "# sc.tl.leiden(query_latent)\n",
    "# sc.tl.umap(query_latent)\n",
    "# plt.figure()\n",
    "# sc.pl.umap(\n",
    "#     query_latent,\n",
    "#     color=[\"batch\", \"cell_type\"],\n",
    "#     frameon=False,\n",
    "#     wspace=0.6,\n",
    "# )\n",
    "\n",
    "# surgery_path = 'surgery_model'\n",
    "# model.save(surgery_path, overwrite=True)\n",
    "\n",
    "#pathing errors -> conda installed with sudo\n",
    "\n",
    "# from absl import flags\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "# from absl import flags\n",
    "# from absl.flags import FLAGS\n",
    "\n",
    "# flags.DEFINE_string('dataroot',\"D:\\College\",'path to root folder of dataset')\n",
    "\n",
    "\n",
    "# import scarches as sca\n",
    "#reinstall. abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "45167ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4585"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import tensor\n",
    "import torch, numpy as np, pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "from fastai.data.transforms import RandomSplitter\n",
    "from collections import defaultdict\n",
    "\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_columns] = df[numerical_columns].fillna(0)\n",
    "numerical_values = df.select_dtypes(include=[int, float]).values.tolist()\n",
    "numerical_values\n",
    "rowGeneExpression = defaultdict(int)\n",
    "hv_genes = set(list(var_df[var_df['highly_variable'] == True].index))\n",
    "normal_genes = (list(adata.var_names))\n",
    "high_variance_columns = set([ i for i,val in enumerate(normal_genes) if val in hv_genes ])\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_columns] = df[numerical_columns].fillna(0)\n",
    "sums = []\n",
    "column_averages = defaultdict(list)\n",
    "rowGeneExpression = defaultdict(int)\n",
    "rows, columns, vals = found\n",
    "high_variance = set(high_variance_columns)\n",
    "row_id = 0\n",
    "control_variables = set(['ctrl', 'control', '*'])\n",
    "dependent_variables = list(df['perturbation'].map(lambda val: 0 if val in control_variables else 1).values)\n",
    "geneValues = defaultdict(int)\n",
    "columnMode = defaultdict(list)\n",
    "geneAverages = defaultdict(int)\n",
    "geneOccurences = defaultdict(int)\n",
    "geneVariance = defaultdict(list)\n",
    "cell_variance_score = defaultdict(int)\n",
    "\n",
    "row_variance = [] \n",
    "c,g,v = found\n",
    "\n",
    "cell_variance_score = {}\n",
    "for i in range(df.shape[0]): cell_variance_score[i]= 0\n",
    "\n",
    "for cell,gene,val in zip(c,g,v):\n",
    "    if gene not in high_variance_columns: continue\n",
    "    geneValues[gene] += val\n",
    "    geneOccurences[gene] += 1\n",
    "    columnMode[gene].append(val)\n",
    "    \n",
    "for k in dict(geneValues):\n",
    "    geneAverages[k] =  geneValues[k] / geneOccurences[k]\n",
    "    \n",
    "for k in dict(geneValues): columnMode[k] = getMode(columnMode[k])\n",
    "    \n",
    "for cell, gene, val in zip(c,g,v):\n",
    "    if gene not in high_variance_columns: continue\n",
    "    geneVariance[gene].append(abs(val - geneAverages[gene]))# ** 2\n",
    "    \n",
    "    \n",
    "for k in dict(geneAverages):  \n",
    "    geneVariance[k] = max(set(geneVariance[k]), key=geneVariance[k].count)\n",
    "\n",
    "geneModes = defaultdict(list)\n",
    "\n",
    "for cell, gene, val in zip(c,g,v):\n",
    "    if gene not in high_variance_columns: continue\n",
    "    geneModes[gene].append(abs(val))# ** 2\n",
    "\n",
    "for val in geneModes: geneModes[val] = max(set(geneModes[val]), key=geneModes[val].count)\n",
    "\n",
    "num_cells = len(df.select_dtypes(include=[int, float]).values.tolist())\n",
    "    \n",
    "mini_cell_var = defaultdict(list)\n",
    "for cell, gene, val in zip(c,g,v):\n",
    "    if gene not in high_variance_columns: continue\n",
    "    columnColor = geneAverages[gene]\n",
    "    cellColorForGene = val\n",
    "    threshold = columnColor\n",
    "    if (cellColorForGene - columnColor) < 0:\n",
    "        mini_cell_var[cell].append(cellColorForGene - columnColor)\n",
    "        cell_variance_score[cell] += abs(cellColorForGene - columnColor)\n",
    "      \n",
    "for key in mini_cell_var: mini_cell_var[key] = max(mini_cell_var[key])\n",
    "        \n",
    "df['geneVarianceScore'] = cell_variance_score.values()\n",
    "numerical_values = df.select_dtypes(include=[int, float]).values.tolist()\n",
    "independent_variables = pd.DataFrame(numerical_values)\n",
    "\n",
    "vals += .01\n",
    "t_dep = tensor([float(i) for i in dependent_variables]) # pertrubations\n",
    "t_indep = tensor(numerical_values, dtype=torch.float)\n",
    "\n",
    "n_coeff = t_indep.shape[1]\n",
    "\n",
    "vals,indices = t_indep.max(dim=0)\n",
    "t_indep = t_indep / vals\n",
    "trn_split,val_split=RandomSplitter(seed=42)(independent_variables)\n",
    "\n",
    "trn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\n",
    "trn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\n",
    "\n",
    "indep_cols =  df.select_dtypes(include=[int, float]).columns.tolist()\n",
    "indep_cols\n",
    "\n",
    "len([item for item in list(t_dep) if item.item() == 0])\n",
    "len([item for item in list(t_dep) if item.item() > .5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8dd3dedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct722, total_guess943, perb_total 4585, accuracy 0.7656415694591728\n",
      "precision 0.20567066521264996\n"
     ]
    }
   ],
   "source": [
    "cell_variance_score= defaultdict(int)\n",
    "for cell, gene, val in zip(c,g,v):\n",
    "    if gene not in high_variance_columns: continue\n",
    "    columnColor = geneAverages[gene]\n",
    "    cellColorForGene = val\n",
    "    threshold = columnColor\n",
    "    if abs(cellColorForGene) > columnColor and columnColor < 1:\n",
    "        #mini_cell_var[cell].append(cellColorForGene - columnColor)\n",
    "        cell_variance_score[cell] += abs(cellColorForGene - columnColor)\n",
    "l = cell_variance_score.values()   \n",
    "avg = sum(l) / len(l)\n",
    "avg = 0\n",
    "import random\n",
    "cvs = cell_variance_score.values()\n",
    "mini_cell_var.values()\n",
    "total_guess = len([item for key, item in enumerate(cvs) if item > avg])\n",
    "correct_guess = len([item for key, item in enumerate(cvs) if item > avg and dependent_variables[key] == 1])\n",
    "perb_total =  len([item for key, item in enumerate(dependent_variables) if dependent_variables[key] == 1])\n",
    "print(f'correct{correct_guess}, total_guess{total_guess}, perb_total {perb_total}, accuracy {correct_guess / total_guess}')\n",
    "print(f'precision {total_guess / perb_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1c75d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = []\n",
    "# test = defaultdict(int)\n",
    "# for i in high_variance_columns:\n",
    "#     m=adata.X.getcol(i).todense()\n",
    "#     mode = getMode(m.tolist()[0]) \n",
    "#     avg = sum(m.tolist()[0]) / len(m.tolist()[0])\n",
    "#     pert_and_above_zero = len([i for k, i in enumerate(m.tolist()) if i[0] > 0 and dependent_variables[k] > 0])\n",
    "#     not_pert_and_above_zero = len([i for k, i in enumerate(m.tolist()) if i[0] > 0 and dependent_variables[k] < 1])\n",
    "#     above_zero = len([i for k, i in enumerate(m.tolist()) if i[0] > 0])\n",
    "#     eq_zero = len([i for k, i in enumerate(m.tolist()) if i[0] == 0])\n",
    "#     test[i] = above_zero\n",
    "#     cellCounts = 5904\n",
    "#     if (above_zero > 30): continue # 90%\n",
    "#     for key,element in enumerate(m.tolist()):\n",
    "#         if element[0] > 0: count.append(key)\n",
    "# print(len(set(count)))\n",
    "# count = set(count)\n",
    "# print(len([x for row, x in enumerate(count) if dependent_variables[x] > 0]),len([x for row, x in enumerate(count) if dependent_variables[x] < 1]))\n",
    "# print(len([x for row, x in enumerate(count) if dependent_variables[x] > 0]) / len([x for row, x in enumerate(count)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4eb0b6c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "12156\n"
     ]
    }
   ],
   "source": [
    "category_indices = df.groupby('perturbation').apply(lambda x: x.index.tolist() )\n",
    "most_cells = category_indices[2]\n",
    "\n",
    "most_cell_indices = []\n",
    "for i in most_cells:\n",
    "    most_cell_indices.append(adata.obs.index.get_loc(i))\n",
    "\n",
    "a=most_cell_indices[0]\n",
    "b=most_cell_indices[10]\n",
    "\n",
    "b_matrix = adata.X.getrow(b).todense().tolist()[0]\n",
    "a_matrix = adata.X.getrow(a).todense().tolist()[0]\n",
    "\n",
    "a_matrix\n",
    "print(len(most_cells))\n",
    "sum(a_matrix), sum(b_matrix)\n",
    "count = {}\n",
    "        \n",
    "distance = defaultdict(int)\n",
    "indicesAbove = defaultdict(list)\n",
    "\n",
    "for row in range(5904):\n",
    "    m = adata.X.getrow(row).todense().tolist()[0]\n",
    "    for k in high_variance_columns:\n",
    "        if (geneAverages[k]) < m[k] and m[k] < 100:\n",
    "            distance[k] += m[k]\n",
    "            indicesAbove[row].append(k)\n",
    "            \n",
    "distance_max = max(list(distance.values()))\n",
    "\n",
    "for k in distance:\n",
    "    if distance[k] == distance_max: print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d93e0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### scipy.stats.zscore(adata.X.getcol(0).todense().tolist())\n",
    "indicesAbove = dict(indicesAbove)\n",
    "# for cell,gene,val in zip(c,g,v):\n",
    "#     if gene not in high_variance_columns: continue\n",
    "#     geneValues[gene] += val\n",
    "#     geneOccurences[gene] += 1\n",
    "#     columnMode[gene].append(val)\n",
    "\n",
    "timesAbove = defaultdict(int)\n",
    "geneAboveMeanOccurances = defaultdict(list)\n",
    "\n",
    "for row in dict(indicesAbove): \n",
    "    for column in indicesAbove[row]: \n",
    "        geneAboveMeanOccurances[column].append(row)\n",
    "        \n",
    "prob_perts = {} \n",
    "\n",
    "filteredGeneCellLists = defaultdict(list)\n",
    "\n",
    "threshold = 30\n",
    "\n",
    "for geneList in geneAboveMeanOccurances:\n",
    "    cellsWithGene = geneAboveMeanOccurances[geneList]\n",
    "    if  threshold < len(cellsWithGene) and len(cellsWithGene) < 100:\n",
    "        filteredGeneCellLists[geneList] = cellsWithGene\n",
    "\n",
    "cellToGeneEmbedding = [[] for i in range(5904)]\n",
    "\n",
    "for column in filteredGeneCellLists:\n",
    "    cellList = filteredGeneCellLists[column]\n",
    "    for cellRow in cellList:\n",
    "        cellToGeneEmbedding[cellRow].append(column)\n",
    "    \n",
    "cellToGeneEmbedding\n",
    "\n",
    "cellCount = 0\n",
    "for cellList in list(filteredGeneCellLists.values()):\n",
    "    cellCount += len(cellList)\n",
    "    \n",
    "totalCells = []\n",
    "for key in (filteredGeneCellLists.keys()):\n",
    "    cellList = filteredGeneCellLists[key]\n",
    "    totalCells += cellList\n",
    "    for cell in cellList:\n",
    "        gene = adata.var.iloc[cell].name\n",
    "        row = df.iloc[cell]\n",
    "        \n",
    "len(set(totalCells))\n",
    "\n",
    "len([item for key, item in enumerate(t_dep) if item.item() > .5 and key in totalCells])\n",
    "total = defaultdict(int)\n",
    "for row in range(500):\n",
    "    total[row] += sum(adata.X.getrow(row).data)\n",
    "avg = sum(list(total.values())) / 500\n",
    "\n",
    "counter = 0\n",
    "for key, item in enumerate(list(total.values())):\n",
    "    if item > avg:\n",
    "        counter += 1\n",
    "        \n",
    "counter\n",
    "count_per_category = df.groupby('perturbation').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a34c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairwise dense output:\n",
      " [[0.9999786]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(67, 67, 1.000028),\n",
       " (67, 353, 0.7105069),\n",
       " (353, 67, 0.7105069),\n",
       " (353, 353, 0.9999786)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort genes by perturbation so diagram makes a clear line from top left to bottom right\n",
    "# solve the adata roblem -> cosine -> predict molecular response\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "category_indices = df.groupby('perturbation').apply(lambda x: x.index.tolist() )\n",
    "most_cells = [category_indices[2][0] , category_indices[1][0]]\n",
    "\n",
    "most_cell_indices = []\n",
    "for i in most_cells:\n",
    "    most_cell_indices.append(adata.obs.index.get_loc(i))\n",
    "    \n",
    "cell_group_indices = [df.index.get_loc(i) for i in most_cells]\n",
    "similarities_list = []\n",
    "\n",
    "for i in cell_group_indices:\n",
    "    for j in cell_group_indices:\n",
    "        A =  adata.X.getrow(i)\n",
    "        A_sparse = adata.X.getrow(j)\n",
    "        similarities = cosine_similarity(A_sparse, A)\n",
    "        if similarities[0][0] > .5:\n",
    "            similarities_list.append((i,j, similarities[0][0]))\n",
    "\n",
    "    \n",
    "    \n",
    "print('pairwise dense output:\\n {}\\n'.format(similarities))\n",
    "\n",
    "#also can output sparse matrices\n",
    "similarities_sparse = cosine_similarity(A_sparse,dense_output=False)\n",
    "similarities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fdc8eca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Replace NaN values with 0 only in numerical columns\n",
    "df[numerical_columns] = df[numerical_columns].fillna(0)\n",
    "\n",
    "numerical_values = df.select_dtypes(include=[int, float]).values.tolist()\n",
    "numerical_values\n",
    "rowGeneExpression = defaultdict(int)\n",
    "\n",
    "hv_genes = set(list(var_df[var_df['highly_variable'] == True].index))\n",
    "normal_genes = (list(adata.var_names))\n",
    "\n",
    "high_variance_columns = set([ i for i,val in enumerate(normal_genes) if val in hv_genes ])\n",
    "\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Replace NaN values with 0 only in numerical columns\n",
    "df[numerical_columns] = df[numerical_columns].fillna(0)\n",
    "\n",
    "sums = []\n",
    "\n",
    "column_averages = defaultdict(list)\n",
    "rowGeneExpression = defaultdict(int)\n",
    "rows, columns, vals = found\n",
    "high_variance = set(high_variance_columns)\n",
    "row_id = 0\n",
    "\n",
    "embedLayer = []\n",
    "for i in high_variance_columns:\n",
    "        intermediate = []\n",
    "        for i in adata.X.getcol(i).toarray():\n",
    "            intermediate.append(i[0])\n",
    "        embedLayer.append(intermediate)\n",
    "\n",
    "mat_for_embed = np.random.rand(t_dep.shape[0], 200)\n",
    "for key,col in enumerate(list(high_variance_columns)[:200]):\n",
    "    m= adata.X.getcol(col)\n",
    "    m = m.todense().tolist()\n",
    "    for row,val in enumerate(m):\n",
    "        mat_for_embed[row, key] = val[0]\n",
    "\n",
    "a = mat_for_embed\n",
    "plt.imshow(a[:80], cmap='nipy_spectral_r', interpolation='nearest')\n",
    "plt.show()\n",
    "#adata.X.A[0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a86216ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://douglaslab.org/tutorials/files/Install-Cadnano-macOS-v4.pdf\n",
    "# # # http# ! pip install biomart\n",
    "# # mito_gene_names = sc.queries.mitochondrial_genes(\"hsapiens\")\n",
    "# # mito_ensembl_ids = sc.queries.mitochondrial_genes(\"hsapiens\", attrname=\"ensembl_gene_id\")\n",
    "# # mito_gene_names_fly = sc.queries.mitochondrial_genes(\"dmelanogaster\", chromosome=\"mitochondrion_genome\")\n",
    "# # import scanpy as sc\n",
    "# # sc.queries.enrich(['KLF4', 'PAX5', 'SOX2', 'NANOG'], org=\"hsapiens\")\n",
    "# # sc.queries.enrich({'set1':['KLF4', 'PAX5'], 'set2':['SOX2', 'NANOG']}, org=\"hsapiens\")\n",
    "# # pbmcs = sc.datasets.pbmc68k_reduced()\n",
    "# # sc.tl.rank_genes_groups(pbmcs, \"bulk_labels\")\n",
    "# # sc.queries.enrich(pbmcs, \"CD34+\")\n",
    "# # # pbmcs\n",
    "# # category_indices = df.groupby('perturbation').apply(lambda x: x.index.tolist() )\n",
    "# # df.index.get_loc('TACTTGACCCCN')\n",
    "# # allRows = defaultdict(int)\n",
    "# # categories = df['perturbation'].unique()\n",
    "# # for i, group in enumerate(category_indices):\n",
    "# #     for row in group:\n",
    "# #         allRows[categories[i]] += 1\n",
    "        \n",
    "# # df.groupby('perturbation')\n",
    "\n",
    "# # categories\n",
    "# # len(category_indices)\n",
    "\n",
    "# # groupCellCounts = list(allRows.values())\n",
    "\n",
    "# # nonZerosInColumn = list(test.values())\n",
    "\n",
    "# # for k,v in enumerate(groupCellCounts):\n",
    "# #     cellCount = nonZerosInColumn[k]\n",
    "# # # from ipywidgets import interact\n",
    "# # # trn_xs = [1,2,3,4,5]\n",
    "# # # conts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]\n",
    "\n",
    "# #just get it working - improve it now\n",
    "# from torch import nn\n",
    "# import torch\n",
    "# def conv(ni, nf, ks=3, stride=1, act=True):\n",
    "#     res = nn.Conv1d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n",
    "#     if act: res = nn.Sequential(res, nn.ReLU())\n",
    "#     return res\n",
    "\n",
    "# def deconv(ni, nf, ks=3, act=True):\n",
    "#     layers = [\n",
    "#     #    nn.UpsamplingNearest2d(scale_factor=2),\n",
    "#               nn.Conv2d(ni, nf, stride=1, kernel_size=ks, padding=ks//2)\n",
    "#     ]\n",
    "#     if act: layers.append(nn.ReLU())\n",
    "#     return nn.Sequential(*layers)\n",
    "\n",
    "# #data /= torch.max(data , 1)\n",
    "# #sort them by cluster and back\n",
    "# finishDemoBy6 = nn.Sequential(\n",
    "#     #nn.RNN(200, 200),\n",
    "#     torch.nn.Linear(200, 200),\n",
    "#     nn.Tanhshrink(),\n",
    "#     #nn.PairwiseDistance(p=2),\n",
    "#     conv(5905,5905, 3),       \n",
    "#     nn.AvgPool1d(101, stride=1),\n",
    "#     conv(5905,5905),\n",
    "#     nn.BatchNorm1d(100),\n",
    "#     nn.AvgPool1d(51, stride=1),\n",
    "#     #conv(5905,5905), \n",
    "#     nn.AvgPool1d(48, stride=1),\n",
    "#     nn.Sigmoid()\n",
    "# ).to('cuda:0')\n",
    "\n",
    "# num_input_channels = 3\n",
    "# c_hid=16\n",
    "# latent_dim = 64\n",
    "# finishDemoBy6= nn.Sequential(\n",
    "#         nn.Conv2d(1, c_hid, kernel_size=2, padding=1, stride=2),  # 32x32 => 16x16\n",
    "#            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "#            nn.Conv2d(c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 16x16 => 8x8\n",
    "#            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "#            nn.Conv2d(2 * c_hid, 3, kernel_size=3, padding=0, stride=1),  # 8x8 => 4x4\n",
    "#            #nn.Flatten(),  # Image grid to single feature vector\n",
    "# #             nn.Linear(2 * 16 * c_hid, 3),\n",
    "# )\n",
    "\n",
    "# opt = optim.SGD(finishDemoBy6.parameters(), lr=0.01)\n",
    "# loss_function2 = torch.nn.MSELoss()\n",
    "# opt = optim.SGD(finishDemoBy6.parameters(), lr=0.01)\n",
    "# loss_function2 = torch.nn.MSELoss()\n",
    "# data = torch.Tensor(mat_for_embed).cuda()\n",
    "# for i in range(50):\n",
    "#     encodedOutput = (finishDemoBy6(data))\n",
    "# #     loss = loss_function2(encodedOutput.sum(1), t_dep.cuda())\n",
    "# #     opt.zero_grad()  # 3\n",
    "# #     loss.backward()\n",
    "#     opt.step()\n",
    "# encodedOutput.to('cuda:0')\n",
    "# encodedOutput\n",
    "# Z = adata.X.A\n",
    "# encodedOutput\n",
    "# https://www.frontiersin.org/articles/10.3389/fpls.2023.1091588/full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7519afa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['start_position', 'end_position', 'chromosome_name'], dtype='object')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assert 4 == 24\n",
    "#! pip install pybiomart\n",
    "#! pip install gprofiler-official\n",
    "import biomart\n",
    "import pybiomart\n",
    "\n",
    "mito_gene_names = sc.queries.mitochondrial_genes(\"hsapiens\")\n",
    "mito_ensembl_ids = sc.queries.mitochondrial_genes(\"hsapiens\", attrname=\"ensembl_gene_id\")\n",
    "mito_gene_names_fly = sc.queries.mitochondrial_genes(\"dmelanogaster\", chromosome=\"mitochondrion_genome\")\n",
    "import scanpy as sc\n",
    "sc.queries.enrich(['KLF4', 'PAX5', 'SOX2', 'NANOG'], org=\"hsapiens\")\n",
    "sc.queries.enrich({'set1':['KLF4', 'PAX5'], 'set2':['SOX2', 'NANOG']}, org=\"hsapiens\")\n",
    "pbmcs = sc.datasets.pbmc68k_reduced()\n",
    "sc.tl.rank_genes_groups(pbmcs, \"bulk_labels\")\n",
    "sc.queries.enrich(pbmcs, \"CD34+\")\n",
    "# pbmcs\n",
    "category_indices = df.groupby('perturbation').apply(lambda x: x.index.tolist() )\n",
    "df.index.get_loc('TACTTGACCCCN')\n",
    "allRows = defaultdict(int)\n",
    "categories = df['perturbation'].unique()\n",
    "for i, group in enumerate(category_indices):\n",
    "    for row in group:\n",
    "        allRows[categories[i]] += 1\n",
    "annot = sc.queries.biomart_annotations(\n",
    "        \"hsapiens\",\n",
    "        [\"ensembl_gene_id\", \"start_position\", \"end_position\", \"chromosome_name\"],\n",
    "    ).set_index(\"ensembl_gene_id\")\n",
    "adata.var[annot.columns] = annot\n",
    "\n",
    "#adata.var[annot.columns]\n",
    "annot.columns\n",
    "#https://www.nature.com/articles/s41467-022-29268-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1a9d7b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2057, 0.0822, 0.3074],\n",
       "        [0.2027, 0.1078, 0.2251],\n",
       "        [0.3322, 0.1701, 0.3380],\n",
       "        [0.3198, 0.1093, 0.2710],\n",
       "        [0.1881, 0.1146, 0.2404],\n",
       "        [0.2955, 0.0425, 0.1499],\n",
       "        [0.2218, 0.0703, 0.2085],\n",
       "        ...,\n",
       "        [0.1900, 0.1898, 0.3577],\n",
       "        [0.2813, 0.1207, 0.2329],\n",
       "        [0.3608, 0.1398, 0.2904],\n",
       "        [0.2524, 0.1475, 0.2592],\n",
       "        [0.3325, 0.2440, 0.3542],\n",
       "        [0.2048, 0.0958, 0.1252],\n",
       "        [0.3278, 0.2052, 0.4736]], device='cuda:0', grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "       \n",
    "df.groupby('perturbation')\n",
    "\n",
    "categories\n",
    "len(category_indices)\n",
    "\n",
    "groupCellCounts = list(allRows.values())\n",
    "\n",
    "nonZerosInColumn = list(allRows.values())\n",
    "\n",
    "# for k,v in enumerate(groupCellCounts):\n",
    "#     cellCount = nonZerosInColumn[k]\n",
    "# # from ipywidgets import interact\n",
    "# # trn_xs = [1,2,3,4,5]\n",
    "# # conts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]\n",
    "\n",
    "\n",
    "# # interact(nm=conts, split=15.5)(iscore);\n",
    "#just get it working - improve it now\n",
    "# num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2),  # 32x32 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(),  # Image grid to single feature vector\n",
    "            nn.Linear(2 * 16 * c_hid, latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "from torch import nn\n",
    "import torch\n",
    "def conv(ni, nf, ks=3, stride=1, act=True):\n",
    "    res = nn.Conv1d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n",
    "    if act: res = nn.Sequential(res, nn.ReLU())\n",
    "    return res\n",
    "\n",
    "def deconv(ni, nf, ks=3, act=True):\n",
    "    layers = [\n",
    "    #    nn.UpsamplingNearest2d(scale_factor=2),\n",
    "              nn.Conv2d(ni, nf, stride=1, kernel_size=ks, padding=ks//2)\n",
    "    ]\n",
    "    if act: layers.append(nn.ReLU())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "c_hid=16\n",
    "latent_dim = 3\n",
    "kernel_size = (1, 5)\n",
    "finishDemo= nn.Sequential(\n",
    "            nn.Conv2d(1, c_hid, kernel_size=(1, 5)),  # 32x32 => 16x16\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=kernel_size),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(c_hid, 2 * c_hid, kernel_size=kernel_size),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=kernel_size),\n",
    "            nn.LeakyReLU(), #L\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(2 * c_hid, 1, kernel_size=kernel_size),  # 8x8 => 4x4\n",
    "            nn.Linear(180, 3)\n",
    ").to('cuda:0')\n",
    "##add some matrix math on latent representation\n",
    "## decode back ???\n",
    "opt = optim.SGD(finishDemo.parameters(), lr=0.01)\n",
    "loss_function2 = torch.nn.MSELoss()\n",
    "data = torch.Tensor([[mat_for_embed]]).to('cuda')\n",
    "\n",
    "for i in range(5):\n",
    "    encodedOutput = (finishDemo(data.to('cuda:0')))\n",
    "    loss = loss_function2(encodedOutput.squeeze().sum(1), t_dep.cuda())\n",
    "    opt.zero_grad()  # 3\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "encodedOutput.to('cuda:0')\n",
    "encodedOutput = (finishDemo(data))\n",
    "tensor = encodedOutput\n",
    "tensor = tensor.squeeze()\n",
    "tensor\n",
    "#print(tensor.shape)\n",
    "#spongebob\n",
    "#tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0f5e5802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 3., 1., 2., 3., 2., 3.,  ..., 0., 2., 1., 0., 2., 0., 2.]]], device='cuda:0')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.sum(1)\n",
    "#simplify model, use a decoder, use another simple function on ('latent representation') to figure out stochastic relationships between columns and \n",
    "#mean, variance, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aa16f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for latent_dim in [64, 128, 256, 384]:\n",
    "#     model_ld, result_ld = train_cifar(latent_dim)\n",
    "#     model_dict[latent_dim] = {\"model\": model_ld, \"result\": result_ld}\n",
    "#     print(model_dict)\n",
    "# model = model_dict[128][\"model\"]\n",
    "# model = model_dict[256][\"model\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1863480f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f40c5adf220>]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAGwCAYAAAC0HlECAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKX0lEQVR4nO3de1xUdeL/8fegMKAJiAZIklGWt7xbhpmXRPBSm+a2a1q5K5tbi6XSWtEmeSlN85q6mdvF+q5uaq1uqSGTplTiDSWvmZpJN6DyQngZkDm/P/ox24SXMy44g+f1fDx81JzzmXM+7zlg7845HGyGYRgCAADAeQX4egIAAADVAaUJAADABEoTAACACZQmAAAAEyhNAAAAJlCaAAAATKA0AQAAmFDT1xO4XLhcLn377beqU6eObDabr6cDAABMMAxDP/30k2JiYhQQcP5zSZSmSvLtt98qNjbW19MAAAAX4auvvlLDhg3PO4bSVEnq1Kkj6ecPPTQ0tFK3XVpaqszMTCUmJiowMLBSt+0vrJBRskZOK2SUrJHTChkla+S0Qkbp4nIWFRUpNjbW/d/x86E0VZLyS3KhoaFVUppq1aql0NDQy/aL3QoZJWvktEJGyRo5rZBRskZOK2SU/recZm6t4UZwAAAAEyhNAAAAJlCaAAAATKA0AQAAmEBpAgAAMIHSBAAAYAKlCQAAwARKEwAAgAmUJgAAABMoTQAAACZQmgAAAEygNAEAAJhAaQIAADCB0gQAAGACpQkAAMCEmr7ceVZWll544QXl5OTou+++07Jly9SvXz+PMXv37tUTTzyh9evX68yZM2revLneeecdXX311ZKk06dP67HHHtNbb70lp9OppKQk/f3vf1dUVJR7G3l5eXr44Yf14Ycf6oorrtCQIUM0adIk1az53/jr1q1Tamqqdu/erdjYWD399NP6wx/+cCk+BtNuHLtazjKbr6dRJew1DE252fcZv3y+r8/2DQDwbz4903TixAm1bt1ac+fOPev6gwcPqnPnzmratKnWrVunHTt2aMyYMQoODnaPGTVqlN577z0tXbpU69ev17fffqu7777bvb6srEx9+/ZVSUmJNmzYoDfeeEMLFixQenq6e8yhQ4fUt29fde/eXbm5uRo5cqT+9Kc/afXq1VUXHgAAVCs+PdPUu3dv9e7d+5zr//a3v6lPnz6aMmWKe9l1113n/vfjx4/r1Vdf1aJFi3T77bdLkl5//XU1a9ZMGzdu1C233KLMzEzt2bNHH3zwgaKiotSmTRtNmDBBTzzxhMaOHaugoCDNmzdPcXFxmjZtmiSpWbNm+vjjjzVjxgwlJSVVUXoAAFCd+LQ0nY/L5dLKlSv1+OOPKykpSdu3b1dcXJzS0tLcl/BycnJUWlqqhIQE9/uaNm2qq6++WtnZ2brllluUnZ2tli1belyuS0pK0sMPP6zdu3erbdu2ys7O9thG+ZiRI0eec35Op1NOp9P9uqioSJJUWlqq0tLSSvgE/qt8e/YAo1K360/Ks/k6Y2Ufu3Ntv6r340tWyChZI6cVMkrWyGmFjNLF5fRmrN+WpsLCQhUXF+v555/Xs88+q8mTJysjI0N33323PvzwQ3Xt2lX5+fkKCgpSeHi4x3ujoqKUn58vScrPz/coTOXry9edb0xRUZFOnTqlkJCQCvObNGmSxo0bV2F5ZmamatWqddG5z2dCB1eVbNef+DrjqlWrLsl+HA7HJdmPL1kho2SNnFbIKFkjpxUySt7lPHnypOmxfluaXK6f/+N51113adSoUZKkNm3aaMOGDZo3b566du3qy+kpLS1Nqamp7tdFRUWKjY1VYmKiQkNDK3VfpaWlcjgcGrM1QE7XZXojeIChCR1cPs+4a2zVXo4tP5Y9e/ZUYGBgle7LV6yQUbJGTitklKyR0woZpYvLWX6lyAy/LU3169dXzZo11bx5c4/l5fcbSVJ0dLRKSkp07Ngxj7NNBQUFio6Odo/ZvHmzxzYKCgrc68r/Wb7sl2NCQ0PPepZJkux2u+x2e4XlgYGBVfYF6XTZLtufnivn64yX6i+Tqvw68RdWyChZI6cVMkrWyGmFjJJ3Ob35PPz2OU1BQUG66aabtG/fPo/ln3/+uRo1aiRJat++vQIDA7VmzRr3+n379ikvL0/x8fGSpPj4eO3cuVOFhYXuMQ6HQ6Ghoe5CFh8f77GN8jHl2wAAAPDpmabi4mIdOHDA/frQoUPKzc1VRESErr76ao0ePVq///3v1aVLF3Xv3l0ZGRl67733tG7dOklSWFiYkpOTlZqaqoiICIWGhuqRRx5RfHy8brnlFklSYmKimjdvrvvvv19TpkxRfn6+nn76aaWkpLjPFD300EOaM2eOHn/8cQ0dOlRr167VkiVLtHLlykv+mQAAAP/k09K0detWde/e3f26/B6hIUOGaMGCBerfv7/mzZunSZMm6dFHH1WTJk30zjvvqHPnzu73zJgxQwEBARowYIDHwy3L1ahRQytWrNDDDz+s+Ph41a5dW0OGDNH48ePdY+Li4rRy5UqNGjVKs2bNUsOGDfXKK6/wuAEAAODm09LUrVs3Gcb5f8R86NChGjp06DnXBwcHa+7cued8QKYkNWrU6II/FdWtWzdt3779/BMGAACW5bf3NAEAAPgTShMAAIAJlCYAAAATKE0AAAAmUJoAAABMoDQBAACYQGkCAAAwgdIEAABgAqUJAADABEoTAACACZQmAAAAEyhNAAAAJlCaAAAATKA0AQAAmEBpAgAAMIHSBAAAYAKlCQAAwARKEwAAgAmUJgAAABMoTQAAACZQmgAAAEygNAEAAJhAaQIAADCB0gQAAGACpQkAAMAEShMAAIAJlCYAAAATKE0AAAAmUJoAAABMoDQBAACYQGkCAAAwgdIEAABggk9LU1ZWlu68807FxMTIZrNp+fLl5xz70EMPyWazaebMmR7Ljxw5osGDBys0NFTh4eFKTk5WcXGxx5gdO3botttuU3BwsGJjYzVlypQK21+6dKmaNm2q4OBgtWzZUqtWraqMiAAA4DLh09J04sQJtW7dWnPnzj3vuGXLlmnjxo2KiYmpsG7w4MHavXu3HA6HVqxYoaysLA0bNsy9vqioSImJiWrUqJFycnL0wgsvaOzYsZo/f757zIYNG3TvvfcqOTlZ27dvV79+/dSvXz/t2rWr8sICAIBqraYvd967d2/17t37vGO++eYbPfLII1q9erX69u3rsW7v3r3KyMjQli1b1KFDB0nS7Nmz1adPH02dOlUxMTFauHChSkpK9NprrykoKEgtWrRQbm6upk+f7i5Xs2bNUq9evTR69GhJ0oQJE+RwODRnzhzNmzevCpIDAIDqxqel6UJcLpfuv/9+jR49Wi1atKiwPjs7W+Hh4e7CJEkJCQkKCAjQpk2b1L9/f2VnZ6tLly4KCgpyj0lKStLkyZN19OhR1a1bV9nZ2UpNTfXYdlJS0nkvFzqdTjmdTvfroqIiSVJpaalKS0svNvJZlW/PHmBU6nb9SXk2X2es7GN3ru1X9X58yQoZJWvktEJGyRo5rZBRuric3oz169I0efJk1axZU48++uhZ1+fn5ysyMtJjWc2aNRUREaH8/Hz3mLi4OI8xUVFR7nV169ZVfn6+e9kvx5Rv42wmTZqkcePGVViemZmpWrVqXTjcRZjQwVUl2/Unvs54qe5lczgcl2Q/vmSFjJI1cloho2SNnFbIKHmX8+TJk6bH+m1pysnJ0axZs7Rt2zbZbDZfT6eCtLQ0j7NTRUVFio2NVWJiokJDQyt1X6WlpXI4HBqzNUBOl/99FpXBHmBoQgeXzzPuGptUpdsvP5Y9e/ZUYGBgle7LV6yQUbJGTitklKyR0woZpYvLWX6lyAy/LU0fffSRCgsLdfXVV7uXlZWV6bHHHtPMmTP15ZdfKjo6WoWFhR7vO3PmjI4cOaLo6GhJUnR0tAoKCjzGlL++0Jjy9Wdjt9tlt9srLA8MDKyyL0inyyZn2eVZmsr5OuOl+sukKr9O/IUVMkrWyGmFjJI1cloho+RdTm8+D799TtP999+vHTt2KDc31/0nJiZGo0eP1urVqyVJ8fHxOnbsmHJyctzvW7t2rVwulzp27Ogek5WV5XHN0uFwqEmTJqpbt657zJo1azz273A4FB8fX9UxAQBANeHTM03FxcU6cOCA+/WhQ4eUm5uriIgIXX311apXr57H+MDAQEVHR6tJkyaSpGbNmqlXr1568MEHNW/ePJWWlmr48OEaOHCg+/EEgwYN0rhx45ScnKwnnnhCu3bt0qxZszRjxgz3dkeMGKGuXbtq2rRp6tu3r9566y1t3brV47EEAADA2nx6pmnr1q1q27at2rZtK0lKTU1V27ZtlZ6ebnobCxcuVNOmTdWjRw/16dNHnTt39ig7YWFhyszM1KFDh9S+fXs99thjSk9P93iWU6dOnbRo0SLNnz9frVu31ttvv63ly5frxhtvrLywAACgWvPpmaZu3brJMMz/iPmXX35ZYVlERIQWLVp03ve1atVKH3300XnH3HPPPbrnnntMzwUAAFiL397TBAAA4E8oTQAAACZQmgAAAEygNAEAAJhAaQIAADCB0gQAAGACpQkAAMAEShMAAIAJlCYAAAATKE0AAAAmUJoAAABMoDQBAACYQGkCAAAwgdIEAABgAqUJAADABEoTAACACZQmAAAAEyhNAAAAJlCaAAAATKA0AQAAmEBpAgAAMIHSBAAAYAKlCQAAwARKEwAAgAmUJgAAABMoTQAAACZQmgAAAEygNAEAAJhAaQIAADCB0gQAAGACpQkAAMAEShMAAIAJlCYAAAATfFqasrKydOeddyomJkY2m03Lly93rystLdUTTzyhli1bqnbt2oqJidEDDzygb7/91mMbR44c0eDBgxUaGqrw8HAlJyeruLjYY8yOHTt02223KTg4WLGxsZoyZUqFuSxdulRNmzZVcHCwWrZsqVWrVlVJZgAAUD35tDSdOHFCrVu31ty5cyusO3nypLZt26YxY8Zo27Zt+ve//619+/bpN7/5jce4wYMHa/fu3XI4HFqxYoWysrI0bNgw9/qioiIlJiaqUaNGysnJ0QsvvKCxY8dq/vz57jEbNmzQvffeq+TkZG3fvl39+vVTv379tGvXrqoLDwAAqpWavtx579691bt377OuCwsLk8Ph8Fg2Z84c3XzzzcrLy9PVV1+tvXv3KiMjQ1u2bFGHDh0kSbNnz1afPn00depUxcTEaOHChSopKdFrr72moKAgtWjRQrm5uZo+fbq7XM2aNUu9evXS6NGjJUkTJkyQw+HQnDlzNG/evCr8BAAAQHXh09LkrePHj8tmsyk8PFySlJ2drfDwcHdhkqSEhAQFBARo06ZN6t+/v7Kzs9WlSxcFBQW5xyQlJWny5Mk6evSo6tatq+zsbKWmpnrsKykpyeNy4a85nU45nU7366KiIkk/X1YsLS2thLT/Vb49e4BRqdv1J+XZfJ2xso/dubZf1fvxJStklKyR0woZJWvktEJG6eJyejO22pSm06dP64knntC9996r0NBQSVJ+fr4iIyM9xtWsWVMRERHKz893j4mLi/MYExUV5V5Xt25d5efnu5f9ckz5Ns5m0qRJGjduXIXlmZmZqlWrlvcBTZjQwVUl2/Unvs54qe5l+/VZ1MuRFTJK1shphYySNXJaIaPkXc6TJ0+aHlstSlNpaal+97vfyTAMvfTSS76ejiQpLS3N4+xUUVGRYmNjlZiY6C51laW0tFQOh0NjtgbI6bJV6rb9hT3A0IQOLp9n3DU2qUq3X34se/bsqcDAwCrdl69YIaNkjZxWyChZI6cVMkoXl7P8SpEZfl+aygvT4cOHtXbtWo9CEh0drcLCQo/xZ86c0ZEjRxQdHe0eU1BQ4DGm/PWFxpSvPxu73S673V5heWBgYJV9QTpdNjnLLs/SVM7XGS/VXyZV+XXiL6yQUbJGTitklKyR0woZJe9yevN5+PVzmsoL0/79+/XBBx+oXr16Huvj4+N17Ngx5eTkuJetXbtWLpdLHTt2dI/JysryuGbpcDjUpEkT1a1b1z1mzZo1Htt2OByKj4+vqmgAAKCa8WlpKi4uVm5urnJzcyVJhw4dUm5urvLy8lRaWqrf/va32rp1qxYuXKiysjLl5+crPz9fJSUlkqRmzZqpV69eevDBB7V582Z98sknGj58uAYOHKiYmBhJ0qBBgxQUFKTk5GTt3r1bixcv1qxZszwurY0YMUIZGRmaNm2aPvvsM40dO1Zbt27V8OHDL/lnAgAA/JNPS9PWrVvVtm1btW3bVpKUmpqqtm3bKj09Xd98843effddff3112rTpo0aNGjg/rNhwwb3NhYuXKimTZuqR48e6tOnjzp37uzxDKawsDBlZmbq0KFDat++vR577DGlp6d7PMupU6dOWrRokebPn6/WrVvr7bff1vLly3XjjTdeug8DAAD4NZ/e09StWzcZxrl/xPx868pFRERo0aJF5x3TqlUrffTRR+cdc8899+iee+654P4AAIA1+fU9TQAAAP6C0gQAAGACpQkAAMAEShMAAIAJlCYAAAATKE0AAAAmUJoAAABMoDQBAACYQGkCAAAwgdIEAABgAqUJAADABEoTAACACZQmAAAAEyhNAAAAJlCaAAAATKA0AQAAmEBpAgAAMIHSBAAAYAKlCQAAwISLKk1nzpzRBx98oJdfflk//fSTJOnbb79VcXFxpU4OAADAX9T09g2HDx9Wr169lJeXJ6fTqZ49e6pOnTqaPHmynE6n5s2bVxXzBAAA8CmvzzSNGDFCHTp00NGjRxUSEuJe3r9/f61Zs6ZSJwcAAOAvvD7T9NFHH2nDhg0KCgryWH7NNdfom2++qbSJAQAA+BOvzzS5XC6VlZVVWP7111+rTp06lTIpAAAAf+N1aUpMTNTMmTPdr202m4qLi/XMM8+oT58+lTk3AAAAv+H15blp06YpKSlJzZs31+nTpzVo0CDt379f9evX17/+9a+qmCMAAIDPeV2aGjZsqE8//VRvvfWWduzYoeLiYiUnJ2vw4MEeN4YDAABcTrwuTZJUs2ZN3XfffZU9FwAAAL/ldWl69913z7rcZrMpODhYjRs3Vlxc3P88MQAAAH/idWnq16+fbDabDMPwWF6+zGazqXPnzlq+fLnq1q1baRMFAADwJa9/es7hcOimm26Sw+HQ8ePHdfz4cTkcDnXs2FErVqxQVlaWfvzxR/31r3+tivkCAAD4hNdnmkaMGKH58+erU6dO7mU9evRQcHCwhg0bpt27d2vmzJkaOnRopU4UAADAl7w+03Tw4EGFhoZWWB4aGqovvvhCknT99dfrhx9+uOC2srKydOeddyomJkY2m03Lly/3WG8YhtLT09WgQQOFhIQoISFB+/fv9xhz5MgRDR48WKGhoQoPD1dycnKFXxy8Y8cO3XbbbQoODlZsbKymTJlSYS5Lly5V06ZNFRwcrJYtW2rVqlUXnD8AALAOr0tT+/btNXr0aH3//ffuZd9//70ef/xx3XTTTZKk/fv3KzY29oLbOnHihFq3bq25c+eedf2UKVP04osvat68edq0aZNq166tpKQknT592j1m8ODB2r17txwOh/vy4LBhw9zri4qKlJiYqEaNGiknJ0cvvPCCxo4dq/nz57vHbNiwQffee6+Sk5O1fft29evXT/369dOuXbu8/XgAAMBlyuvLc6+++qruuusuNWzY0F2MvvrqK1177bX6z3/+I0kqLi7W008/fcFt9e7dW7179z7rOsMwNHPmTD399NO66667JElvvvmmoqKitHz5cg0cOFB79+5VRkaGtmzZog4dOkiSZs+erT59+mjq1KmKiYnRwoULVVJSotdee01BQUFq0aKFcnNzNX36dHe5mjVrlnr16qXRo0dLkiZMmCCHw6E5c+Zo3rx53n5EAADgMuR1aWrSpIn27NmjzMxMff755+5lPXv2VEDAzyeu+vXr9z9P7NChQ8rPz1dCQoJ7WVhYmDp27Kjs7GwNHDhQ2dnZCg8PdxcmSUpISFBAQIA2bdqk/v37Kzs7W126dPH4BcNJSUmaPHmyjh49qrp16yo7O1upqake+09KSqpwufCXnE6nnE6n+3VRUZEkqbS0VKWlpf9rfA/l27MHGBcYWX2VZ/N1xso+dufaflXvx5eskFGyRk4rZJSskdMKGaWLy+nN2It6uGVAQIB69eqlXr16XczbTcnPz5ckRUVFeSyPiopyr8vPz1dkZKTH+po1ayoiIsJjzK+fG1W+zfz8fNWtW1f5+fnn3c/ZTJo0SePGjauwPDMzU7Vq1TIT0WsTOriqZLv+xNcZL9W9bA6H45Lsx5eskFGyRk4rZJSskdMKGSXvcp48edL02IsqTSdOnND69euVl5enkpISj3WPPvroxWyy2klLS/M4O1VUVKTY2FglJiae9Ub5/0VpaakcDofGbA2Q02Wr1G37C3uAoQkdXD7PuGtsUpVuv/xY9uzZU4GBgVW6L1+xQkbJGjmtkFGyRk4rZJQuLmf5lSIzvC5N27dvV58+fXTy5EmdOHFCERER+uGHH1SrVi1FRkZWWmmKjo6WJBUUFKhBgwbu5QUFBWrTpo17TGFhocf7zpw5oyNHjrjfHx0drYKCAo8x5a8vNKZ8/dnY7XbZ7fYKywMDA6vsC9LpsslZdnmWpnK+znip/jKpyq8Tf2GFjJI1cloho2SNnFbIKHmX05vPw+ufnhs1apTuvPNOHT16VCEhIdq4caMOHz6s9u3ba+rUqd5u7pzi4uIUHR2tNWvWuJcVFRVp06ZNio+PlyTFx8fr2LFjysnJcY9Zu3atXC6XOnbs6B6TlZXlcc3S4XCoSZMm7ieWx8fHe+ynfEz5fgAAALwuTbm5uXrssccUEBCgGjVqyOl0up999NRTT3m1reLiYuXm5io3N1fSzzd/5+bmKi8vTzabTSNHjtSzzz6rd999Vzt37tQDDzygmJgY943mzZo1U69evfTggw9q8+bN+uSTTzR8+HANHDhQMTExkqRBgwYpKChIycnJ2r17txYvXqxZs2Z5XFobMWKEMjIyNG3aNH322WcaO3astm7dquHDh3v78QAAgMuU15fnAgMD3T8lFxkZqby8PDVr1kxhYWH66quvvNrW1q1b1b17d/fr8iIzZMgQLViwQI8//rhOnDihYcOG6dixY+rcubMyMjIUHBzsfs/ChQs1fPhw9ejRQwEBARowYIBefPFF9/qwsDBlZmYqJSVF7du3V/369ZWenu7xLKdOnTpp0aJFevrpp/XUU0/p+uuv1/Lly3XjjTd6+/EAAIDLlNelqW3bttqyZYuuv/56de3aVenp6frhhx/0f//3f16XjG7dulX4xb+/ZLPZNH78eI0fP/6cYyIiIrRo0aLz7qdVq1b66KOPzjvmnnvu0T333HP+CQMAAMvy+vLcxIkT3TdmP/fcc6pbt64efvhhff/993r55ZcrfYIAAAD+wOszTb98kGRkZKQyMjIqdUIAAAD+yOszTbfffruOHTtWYXlRUZFuv/32ypgTAACA3/G6NK1bt67CAy0l6fTp0xe8bwgAAKC6Mn15bseOHe5/37Nnj8evGCkrK1NGRoauuuqqyp0dAACAnzBdmtq0aSObzSabzXbWy3AhISGaPXt2pU4OAADAX5guTYcOHZJhGLr22mu1efNmXXnlle51QUFBioyMVI0aNapkkgAAAL5mujQ1atRIkuRy+fa30AMAAPiC148ckKT9+/frww8/VGFhYYUSlZ6eXikTAwAA8Cdel6Z//OMfevjhh1W/fn1FR0fLZvvvb6S32WyUJgAAcFnyujQ9++yzeu655/TEE09UxXwAAAD8ktfPaTp69Ci/ow0AAFiO16XpnnvuUWZmZlXMBQAAwG95fXmucePGGjNmjDZu3KiWLVsqMDDQY/2jjz5aaZMDAADwF16Xpvnz5+uKK67Q+vXrtX79eo91NpuN0gQAAC5LXpemQ4cOVcU8AAAA/JrX9zSVKykp0b59+3TmzJnKnA8AAIBf8ro0nTx5UsnJyapVq5ZatGihvLw8SdIjjzyi559/vtInCAAA4A+8Lk1paWn69NNPtW7dOgUHB7uXJyQkaPHixZU6OQAAAH/h9T1Ny5cv1+LFi3XLLbd4PA28RYsWOnjwYKVODgAAwF94fabp+++/V2RkZIXlJ06c8ChRAAAAlxOvS1OHDh20cuVK9+vyovTKK68oPj6+8mYGAADgR7y+PDdx4kT17t1be/bs0ZkzZzRr1izt2bNHGzZsqPDcJgAAgMuF12eaOnfurNzcXJ05c0YtW7ZUZmamIiMjlZ2drfbt21fFHAEAAHzO6zNNknTdddfpH//4R2XPBQAAwG95faZp1apVWr16dYXlq1ev1vvvv18pkwIAAPA3XpemJ598UmVlZRWWG4ahJ598slImBQAA4G+8Lk379+9X8+bNKyxv2rSpDhw4UCmTAgAA8Ddel6awsDB98cUXFZYfOHBAtWvXrpRJAQAA+BuvS9Ndd92lkSNHejz9+8CBA3rsscf0m9/8plInBwAA4C+8Lk1TpkxR7dq11bRpU8XFxSkuLk7NmjVTvXr1NHXq1KqYIwAAgM95/ciBsLAwbdiwQQ6HQ59++qlCQkLUqlUrdenSpSrmBwAA4Be8Kk2lpaUKCQlRbm6uEhMTlZiYWFXzAgAA8CteXZ4LDAzU1VdffdZHDlSFsrIyjRkzRnFxcQoJCdF1112nCRMmyDAM9xjDMJSenq4GDRooJCRECQkJ2r9/v8d2jhw5osGDBys0NFTh4eFKTk5WcXGxx5gdO3botttuU3BwsGJjYzVlypRLkhEAAFQPXt/T9Le//U1PPfWUjhw5UhXz8TB58mS99NJLmjNnjvbu3avJkydrypQpmj17tnvMlClT9OKLL2revHnatGmTateuraSkJJ0+fdo9ZvDgwdq9e7ccDodWrFihrKwsDRs2zL2+qKhIiYmJatSokXJycvTCCy9o7Nixmj9/fpVnBAAA1YPX9zTNmTNHBw4cUExMjBo1alThMQPbtm2rtMlt2LBBd911l/r27StJuuaaa/Svf/1LmzdvlvTzWaaZM2fq6aef1l133SVJevPNNxUVFaXly5dr4MCB2rt3rzIyMrRlyxZ16NBBkjR79mz16dNHU6dOVUxMjBYuXKiSkhK99tprCgoKUosWLZSbm6vp06d7lCsAAGBdXpemfv36VcE0zq5Tp06aP3++Pv/8c91www369NNP9fHHH2v69OmSpEOHDik/P18JCQnu94SFhaljx47Kzs7WwIEDlZ2drfDwcHdhkqSEhAQFBARo06ZN6t+/v7Kzs9WlSxcFBQW5xyQlJWny5Mk6evSo6tatW2FuTqdTTqfT/bqoqEjSz/d9lZaWVurnUL49e4BxgZHVV3k2X2es7GN3ru1X9X58yQoZJWvktEJGyRo5rZBRuric3oz1ujQ988wz3r7loj355JMqKipS06ZNVaNGDZWVlem5557T4MGDJUn5+fmSpKioKI/3RUVFudfl5+crMjLSY33NmjUVERHhMSYuLq7CNsrXna00TZo0SePGjauwPDMzU7Vq1bqYuBc0oYOrSrbrT3ydcdWqVZdkPw6H45Lsx5eskFGyRk4rZJSskdMKGSXvcp48edL0WK9LkyQdO3ZMb7/9tg4ePKjRo0crIiJC27ZtU1RUlK666qqL2eRZLVmyRAsXLtSiRYvcl8xGjhypmJgYDRkypNL2czHS0tKUmprqfl1UVKTY2FglJiYqNDS0UvdVWloqh8OhMVsD5HTZKnXb/sIeYGhCB5fPM+4am1Sl2y8/lj179lRgYGCV7stXrJBRskZOK2SUrJHTChmli8tZfqXIDK9L044dO5SQkKCwsDB9+eWXevDBBxUREaF///vfysvL05tvvuntJs9p9OjRevLJJzVw4EBJUsuWLXX48GFNmjRJQ4YMUXR0tCSpoKBADRo0cL+voKBAbdq0kSRFR0ersLDQY7tnzpzRkSNH3O+Pjo5WQUGBx5jy1+Vjfs1ut8tut1dYHhgYWGVfkE6XTc6yy7M0lfN1xkv1l0lVfp34CytklKyR0woZJWvktEJGybuc3nweXv/0XGpqqv7whz9o//79Cg4Odi/v06ePsrKyvN3ceZ08eVIBAZ5TrFGjhlyuny/hxMXFKTo6WmvWrHGvLyoq0qZNmxQfHy9Jio+P17Fjx5STk+Mes3btWrlcLnXs2NE9Jisry+O6psPhUJMmTc56aQ4AAFiP16Vpy5Yt+vOf/1xh+VVXXeW+R6iy3HnnnXruuee0cuVKffnll1q2bJmmT5+u/v37S5JsNptGjhypZ599Vu+++6527typBx54QDExMe4b1ps1a6ZevXrpwQcf1ObNm/XJJ59o+PDhGjhwoGJiYiRJgwYNUlBQkJKTk7V7924tXrxYs2bN8rj8BgAArM3ry3N2u/2s1/8+//xzXXnllZUyqXKzZ8/WmDFj9Je//EWFhYWKiYnRn//8Z6Wnp7vHPP744zpx4oSGDRumY8eOqXPnzsrIyPA4C7Zw4UINHz5cPXr0UEBAgAYMGKAXX3zRvT4sLEyZmZlKSUlR+/btVb9+faWnp/O4AQAA4OZ1afrNb36j8ePHa8mSJZJ+PtuTl5enJ554QgMGDKjUydWpU0czZ87UzJkzzznGZrNp/PjxGj9+/DnHREREaNGiRefdV6tWrfTRRx9d7FQBAMBlzuvLc9OmTVNxcbEiIyN16tQpde3aVY0bN1adOnX03HPPVcUcAQAAfM7rM01hYWFyOBz65JNP9Omnn6q4uFjt2rXzeMAkAADA5car0rR48WK9++67KikpUY8ePfSXv/ylquYFAADgV0yXppdeekkpKSm6/vrrFRISon//+986ePCgXnjhhaqcHwAAgF8wfU/TnDlz9Mwzz2jfvn3Kzc3VG2+8ob///e9VOTcAAAC/Ybo0ffHFFx6/umTQoEE6c+aMvvvuuyqZGAAAgD8xXZqcTqdq16793zcGBCgoKEinTp2qkokBAAD4E69uBB8zZoxq1arlfl1SUqLnnntOYWFh7mXTp0+vvNkBAAD4CdOlqUuXLtq3b5/Hsk6dOumLL75wv7bZLu9fJgsAAKzLdGlat25dFU4DAADAv3n9RHAAAAArojQBAACYQGkCAAAwgdIEAABgAqUJAADABNOlaf/+/br33ntVVFRUYd3x48c1aNAgj8cPAAAAXE5Ml6YXXnhBsbGxCg0NrbAuLCxMsbGx/PJeAABw2TJdmtavX6977rnnnOt/97vfae3atZUyKQAAAH9jujTl5eUpMjLynOvr16+vr776qlImBQAA4G9Ml6awsDAdPHjwnOsPHDhw1kt3AAAAlwPTpalLly6aPXv2Ode/+OKLuu222yplUgAAAP7GdGlKS0vT+++/r9/+9rfavHmzjh8/ruPHj2vTpk0aMGCAVq9erbS0tKqcKwAAgM+Y/oW9bdu21dtvv62hQ4dq2bJlHuvq1aunJUuWqF27dpU+QQAAAH9gujRJ0h133KHDhw8rIyNDBw4ckGEYuuGGG5SYmKhatWpV1RwBAAB8zqvSJEkhISHq379/VcwFAADAb5kuTadOndKaNWt0xx13SPr5Hien0+leX6NGDU2YMEHBwcGVP0sAAAAfM12a3njjDa1cudJdmubMmaMWLVooJCREkvTZZ58pJiZGo0aNqpqZAgAA+JDpn55buHChhg0b5rFs0aJF+vDDD/Xhhx/qhRde0JIlSyp9ggAAAP7AdGk6cOCAWrZs6X4dHBysgID/vv3mm2/Wnj17Knd2AAAAfsL05bljx4553MP0/fffe6x3uVwe6wEAAC4nps80NWzYULt27Trn+h07dqhhw4aVMikAAAB/Y7o09enTR+np6Tp9+nSFdadOndK4cePUt2/fSp0cAACAvzB9ee6pp57SkiVL1KRJEw0fPlw33HCDJGnfvn2aM2eOzpw5o6eeeqrKJgoAAOBLps80RUVFacOGDWrWrJmefPJJ9e/fX/3791daWpqaN2+ujz/+WFFRUZU+wW+++Ub33Xef6tWrp5CQELVs2VJbt251rzcMQ+np6WrQoIFCQkKUkJCg/fv3e2zjyJEjGjx4sEJDQxUeHq7k5GQVFxd7jNmxY4duu+02BQcHKzY2VlOmTKn0LAAAoPry6ongcXFxysjI0JEjR3TgwAFJUuPGjRUREVElkzt69KhuvfVWde/eXe+//76uvPJK7d+/X3Xr1nWPmTJlil588UW98cYbiouL05gxY5SUlKQ9e/a4H7Q5ePBgfffdd3I4HCotLdUf//hHDRs2TIsWLZIkFRUVKTExUQkJCZo3b5527typoUOHKjw8vMJjFgAAgDV5/WtUJCkiIkI333xzZc+lgsmTJys2Nlavv/66e1lcXJz73w3D0MyZM/X000/rrrvukiS9+eabioqK0vLlyzVw4EDt3btXGRkZ2rJlizp06CBJmj17tvr06aOpU6cqJiZGCxcuVElJiV577TUFBQWpRYsWys3N1fTp089ZmpxOp8dPCxYVFUmSSktLVVpaWqmfQ/n27AFGpW7Xn5Rn83XGyj5259p+Ve/Hl6yQUbJGTitklKyR0woZpYvL6c1Ym2EYfvtf4ubNmyspKUlff/211q9fr6uuukp/+ctf9OCDD0qSvvjiC1133XXavn272rRp435f165d1aZNG82aNUuvvfaaHnvsMR09etS9/syZMwoODtbSpUvVv39/PfDAAyoqKtLy5cvdYz788EPdfvvtOnLkiMeZrXJjx47VuHHjKixftGgRv7wYAIBq4uTJkxo0aJCOHz+u0NDQ8469qDNNl8oXX3yhl156SampqXrqqae0ZcsWPfroowoKCtKQIUOUn58vSRXupYqKinKvy8/PV2RkpMf6mjVrKiIiwmPML89g/XKb+fn5Zy1NaWlpSk1Ndb8uKipSbGysEhMTL/ihe6u0tFQOh0NjtgbI6bJV6rb9hT3A0IQOLp9n3DU2qUq3X34se/bsqcDAwCrdl69YIaNkjZxWyChZI6cVMkoXl7P8SpEZfl2aXC6XOnTooIkTJ0qS2rZtq127dmnevHkaMmSIT+dmt9tlt9srLA8MDKyyL0inyyZn2eVZmsr5OuOl+sukKr9O/IUVMkrWyGmFjJI1cloho+RdTm8+D9M/PecLDRo0UPPmzT2WNWvWTHl5eZKk6OhoSVJBQYHHmIKCAve66OhoFRYWeqw/c+aMjhw54jHmbNv45T4AAIC1+XVpuvXWW7Vv3z6PZZ9//rkaNWok6eebwqOjo7VmzRr3+qKiIm3atEnx8fGSpPj4eB07dkw5OTnuMWvXrpXL5VLHjh3dY7KysjxuBnM4HGrSpMlZL80BAADr8evSNGrUKG3cuFETJ07UgQMHtGjRIs2fP18pKSmSJJvNppEjR+rZZ5/Vu+++q507d+qBBx5QTEyM+vXrJ+nnM1O9evXSgw8+qM2bN+uTTz7R8OHDNXDgQMXExEiSBg0apKCgICUnJ2v37t1avHixZs2a5XHPEgAAsDa/vqfppptu0rJly5SWlqbx48crLi5OM2fO1ODBg91jHn/8cZ04cULDhg3TsWPH1LlzZ2VkZLif0SRJCxcu1PDhw9WjRw8FBARowIABevHFF93rw8LClJmZqZSUFLVv317169dXeno6z2gCAABufl2aJOmOO+7QHXfccc71NptN48eP1/jx4885JiIiwv0gy3Np1aqVPvroo4ueJwAAuLz59eU5AAAAf0FpAgAAMIHSBAAAYAKlCQAAwARKEwAAgAmUJgAAABMoTQAAACZQmgAAAEygNAEAAJhAaQIAADCB0gQAAGACpQkAAMAEShMAAIAJlCYAAAATKE0AAAAmUJoAAABMoDQBAACYQGkCAAAwgdIEAABgAqUJAADABEoTAACACZQmAAAAEyhNAAAAJlCaAAAATKA0AQAAmEBpAgAAMIHSBAAAYAKlCQAAwARKEwAAgAmUJgAAABMoTQAAACZQmgAAAEyoVqXp+eefl81m08iRI93LTp8+rZSUFNWrV09XXHGFBgwYoIKCAo/35eXlqW/fvqpVq5YiIyM1evRonTlzxmPMunXr1K5dO9ntdjVu3FgLFiy4BIkAAEB1UW1K05YtW/Tyyy+rVatWHstHjRql9957T0uXLtX69ev17bff6u6773avLysrU9++fVVSUqINGzbojTfe0IIFC5Senu4ec+jQIfXt21fdu3dXbm6uRo4cqT/96U9avXr1JcsHAAD8W7UoTcXFxRo8eLD+8Y9/qG7duu7lx48f16uvvqrp06fr9ttvV/v27fX6669rw4YN2rhxoyQpMzNTe/bs0T//+U+1adNGvXv31oQJEzR37lyVlJRIkubNm6e4uDhNmzZNzZo10/Dhw/Xb3/5WM2bM8EleAADgf2r6egJmpKSkqG/fvkpISNCzzz7rXp6Tk6PS0lIlJCS4lzVt2lRXX321srOzdcsttyg7O1stW7ZUVFSUe0xSUpIefvhh7d69W23btlV2drbHNsrH/PIy4K85nU45nU7366KiIklSaWmpSktL/9fIHsq3Zw8wKnW7/qQ8m68zVvaxO9f2q3o/vmSFjJI1cloho2SNnFbIKF1cTm/G+n1peuutt7Rt2zZt2bKlwrr8/HwFBQUpPDzcY3lUVJTy8/PdY35ZmMrXl68735iioiKdOnVKISEhFfY9adIkjRs3rsLyzMxM1apVy3xAL0zo4KqS7foTX2dctWrVJdmPw+G4JPvxJStklKyR0woZJWvktEJGybucJ0+eND3Wr0vTV199pREjRsjhcCg4ONjX0/GQlpam1NRU9+uioiLFxsYqMTFRoaGhlbqv0tJSORwOjdkaIKfLVqnb9hf2AEMTOrh8nnHX2KQq3X75sezZs6cCAwOrdF++YoWMkjVyWiGjZI2cVsgoXVzO8itFZvh1acrJyVFhYaHatWvnXlZWVqasrCzNmTNHq1evVklJiY4dO+ZxtqmgoEDR0dGSpOjoaG3evNlju+U/XffLMb/+ibuCggKFhoae9SyTJNntdtnt9grLAwMDq+wL0umyyVl2eZamcr7OeKn+MqnKrxN/YYWMkjVyWiGjZI2cVsgoeZfTm8/Dr28E79Gjh3bu3Knc3Fz3nw4dOmjw4MHufw8MDNSaNWvc79m3b5/y8vIUHx8vSYqPj9fOnTtVWFjoHuNwOBQaGqrmzZu7x/xyG+VjyrcBAADg12ea6tSpoxtvvNFjWe3atVWvXj338uTkZKWmpioiIkKhoaF65JFHFB8fr1tuuUWSlJiYqObNm+v+++/XlClTlJ+fr6efflopKSnuM0UPPfSQ5syZo8cff1xDhw7V2rVrtWTJEq1cufLSBgYAAH7Lr0uTGTNmzFBAQIAGDBggp9OppKQk/f3vf3evr1GjhlasWKGHH35Y8fHxql27toYMGaLx48e7x8TFxWnlypUaNWqUZs2apYYNG+qVV15RUlLV3t8CAACqj2pXmtatW+fxOjg4WHPnztXcuXPP+Z5GjRpd8KeiunXrpu3bt1fGFAEAwGXIr+9pAgAA8BeUJgAAABMoTQAAACZQmgAAAEygNAEAAJhAaQIAADCB0gQAAGACpQkAAMAEShMAAIAJlCYAAAATKE0AAAAmUJoAAABMoDQBAACYQGkCAAAwgdIEAABgAqUJAADABEoTAACACZQmAAAAEyhNAAAAJlCaAAAATKA0AQAAmEBpAgAAMIHSBAAAYAKlCQAAwARKEwAAgAmUJgAAABMoTQAAACZQmgAAAEygNAEAAJhAaQIAADCB0gQAAGACpQkAAMAEShMAAIAJfl+aJk2apJtuukl16tRRZGSk+vXrp3379nmMOX36tFJSUlSvXj1dccUVGjBggAoKCjzG5OXlqW/fvqpVq5YiIyM1evRonTlzxmPMunXr1K5dO9ntdjVu3FgLFiyo6ngAAKCa8PvStH79eqWkpGjjxo1yOBwqLS1VYmKiTpw44R4zatQovffee1q6dKnWr1+vb7/9Vnfffbd7fVlZmfr27auSkhJt2LBBb7zxhhYsWKD09HT3mEOHDqlv377q3r27cnNzNXLkSP3pT3/S6tWrL2leAADgn2r6egIXkpGR4fF6wYIFioyMVE5Ojrp06aLjx4/r1Vdf1aJFi3T77bdLkl5//XU1a9ZMGzdu1C233KLMzEzt2bNHH3zwgaKiotSmTRtNmDBBTzzxhMaOHaugoCDNmzdPcXFxmjZtmiSpWbNm+vjjjzVjxgwlJSVVmJfT6ZTT6XS/LioqkiSVlpaqtLS0Uj+D8u3ZA4xK3a4/Kc/m64yVfezOtf2q3o8vWSGjZI2cVsgoWSOnFTJKF5fTm7E2wzCq1X+JDxw4oOuvv147d+7UjTfeqLVr16pHjx46evSowsPD3eMaNWqkkSNHatSoUUpPT9e7776r3Nxc9/pDhw7p2muv1bZt29S2bVt16dJF7dq108yZM91jXn/9dY0cOVLHjx+vMI+xY8dq3LhxFZYvWrRItWrVqszIAACgipw8eVKDBg3S8ePHFRoaet6xfn+m6ZdcLpdGjhypW2+9VTfeeKMkKT8/X0FBQR6FSZKioqKUn5/vHhMVFVVhffm6840pKirSqVOnFBIS4rEuLS1Nqamp7tdFRUWKjY1VYmLiBT90b5WWlsrhcGjM1gA5XbZK3ba/sAcYmtDB5fOMu8ZWPKtYmcqPZc+ePRUYGFil+/IVK2SUrJHTChkla+S0Qkbp4nKWXykyo1qVppSUFO3atUsff/yxr6ciu90uu91eYXlgYGCVfUE6XTY5yy7P0lTO1xkv1V8mVfl14i+skFGyRk4rZJSskdMKGSXvcnrzefj9jeDlhg8frhUrVujDDz9Uw4YN3cujo6NVUlKiY8eOeYwvKChQdHS0e8yvf5qu/PWFxoSGhlY4ywQAAKzH70uTYRgaPny4li1bprVr1youLs5jffv27RUYGKg1a9a4l+3bt095eXmKj4+XJMXHx2vnzp0qLCx0j3E4HAoNDVXz5s3dY365jfIx5dsAAADW5veX51JSUrRo0SL95z//UZ06ddz3IIWFhSkkJERhYWFKTk5WamqqIiIiFBoaqkceeUTx8fG65ZZbJEmJiYlq3ry57r//fk2ZMkX5+fl6+umnlZKS4r7E9tBDD2nOnDl6/PHHNXToUK1du1ZLlizRypUrfZYdAAD4D78/0/TSSy/p+PHj6tatmxo0aOD+s3jxYveYGTNm6I477tCAAQPUpUsXRUdH69///rd7fY0aNbRixQrVqFFD8fHxuu+++/TAAw9o/Pjx7jFxcXFauXKlHA6HWrdurWnTpumVV1456+MGAACA9fj9mSYzT0QIDg7W3LlzNXfu3HOOadSokVatWnXe7XTr1k3bt2/3eo4AAODy5/dnmgAAAPwBpQkAAMAEShMAAIAJlCYAAAATKE0AAAAmUJoAAABMoDQBAACYQGkCAAAwgdIEAABgAqUJAADABEoTAACACZQmAAAAEyhNAAAAJlCaAAAATKA0AQAAmEBpAgAAMIHSBAAAYAKlCQAAwARKEwAAgAmUJgAAABMoTQAAACZQmgAAAEygNAEAAJhAaQIAADCB0gQAAGACpQkAAMAEShMAAIAJlCYAAAATKE0AAAAmUJoAAABMoDQBAACYQGkCAAAwgdL0K3PnztU111yj4OBgdezYUZs3b/b1lAAAgB+gNP3C4sWLlZqaqmeeeUbbtm1T69atlZSUpMLCQt9O7MQPan9orj6qmaJnar7h27kAAGBRlKZfmD59uh588EH98Y9/VPPmzTVv3jzVqlVLr732mu8mVXpaNV9PVMNjmxRpO6ZI21HfzQUAAAur6esJ+IuSkhLl5OQoLS3NvSwgIEAJCQnKzs6uMN7pdMrpdLpfHz9+XJJ05MgRlZaWVtq8bN9uV83Cw5Kk1119tLism2rqRKVt31/UdBk6edKlmqUBKnPZfDaPH3/8sUq3X1paqpMnT+rHH39UYGBgle7LV6yQUbJGTitklKyR0woZpYvL+dNPP0mSDMO44FhK0//3ww8/qKysTFFRUR7Lo6Ki9Nlnn1UYP2nSJI0bN67C8ri4uCqbo7T4//+5PA3y9QQk1Z/m6xkAAHzhp59+UlhY2HnHUJouUlpamlJTU92vXS6Xjhw5onr16slmq9wzJUVFRYqNjdVXX32l0NDQSt22v7BCRskaOa2QUbJGTitklKyR0woZpYvLaRiGfvrpJ8XExFxwLKXp/6tfv75q1KihgoICj+UFBQWKjo6uMN5ut8tut3ssCw8Pr8opKjQ09LL+YpeskVGyRk4rZJSskdMKGSVr5LRCRsn7nBc6w1SOG8H/v6CgILVv315r1qxxL3O5XFqzZo3i4+N9ODMAAOAPONP0C6mpqRoyZIg6dOigm2++WTNnztSJEyf0xz/+0ddTAwAAPkZp+oXf//73+v7775Wenq78/Hy1adNGGRkZFW4Ov9TsdrueeeaZCpcDLydWyChZI6cVMkrWyGmFjJI1cloho1T1OW2GmZ+xAwAAsDjuaQIAADCB0gQAAGACpQkAAMAEShMAAIAJlCY/MXfuXF1zzTUKDg5Wx44dtXnz5vOOX7p0qZo2barg4GC1bNlSq1atukQzvXjeZFywYIFsNpvHn+Dg4Es4W+9lZWXpzjvvVExMjGw2m5YvX37B96xbt07t2rWT3W5X48aNtWDBgiqf5//K25zr1q2rcCxtNpvy8/MvzYQvwqRJk3TTTTepTp06ioyMVL9+/bRv374Lvq+6fV9eTM7q9r350ksvqVWrVu6HHcbHx+v9998/73uq23GUvM9Z3Y7j2Tz//POy2WwaOXLkecdV5vGkNPmBxYsXKzU1Vc8884y2bdum1q1bKykpSYWFhWcdv2HDBt17771KTk7W9u3b1a9fP/Xr10+7du26xDM3z9uM0s9PdP3uu+/cfw4fPnwJZ+y9EydOqHXr1po7d66p8YcOHVLfvn3VvXt35ebmauTIkfrTn/6k1atXV/FM/zfe5iy3b98+j+MZGRlZRTP8361fv14pKSnauHGjHA6HSktLlZiYqBMnzv3Lsqvj9+XF5JSq1/dmw4YN9fzzzysnJ0dbt27V7bffrrvuuku7d+8+6/jqeBwl73NK1es4/tqWLVv08ssvq1WrVucdV+nH04DP3XzzzUZKSor7dVlZmRETE2NMmjTprON/97vfGX379vVY1rFjR+PPf/5zlc7zf+Ftxtdff90ICwu7RLOrfJKMZcuWnXfM448/brRo0cJj2e9//3sjKSmpCmdWuczk/PDDDw1JxtGjRy/JnKpCYWGhIclYv379OcdUx+/LXzOTs7p/bxqGYdStW9d45ZVXzrrucjiO5c6Xszofx59++sm4/vrrDYfDYXTt2tUYMWLEOcdW9vHkTJOPlZSUKCcnRwkJCe5lAQEBSkhIUHZ29lnfk52d7TFekpKSks453tcuJqMkFRcXq1GjRoqNjb3g/zFVR9XtOP6v2rRpowYNGqhnz5765JNPfD0drxw/flySFBERcc4xl8PxNJNTqr7fm2VlZXrrrbd04sSJc/56rMvhOJrJKVXf45iSkqK+fftWOE5nU9nHk9LkYz/88IPKysoqPHU8KirqnPd85OfnezXe1y4mY5MmTfTaa6/pP//5j/75z3/K5XKpU6dO+vrrry/FlC+Jcx3HoqIinTp1ykezqnwNGjTQvHnz9M477+idd95RbGysunXrpm3btvl6aqa4XC6NHDlSt956q2688cZzjqtu35e/ZjZndfze3Llzp6644grZ7XY99NBDWrZsmZo3b37WsdX5OHqTszoeR0l66623tG3bNk2aNMnU+Mo+nvwaFfil+Ph4j/9D6tSpk5o1a6aXX35ZEyZM8OHM4K0mTZqoSZMm7tedOnXSwYMHNWPGDP3f//2fD2dmTkpKinbt2qWPP/7Y11OpUmZzVsfvzSZNmig3N1fHjx/X22+/rSFDhmj9+vXnLBTVlTc5q+Nx/OqrrzRixAg5HA6f3bROafKx+vXrq0aNGiooKPBYXlBQoOjo6LO+Jzo62qvxvnYxGX8tMDBQbdu21YEDB6piij5xruMYGhqqkJAQH83q0rj55purRQkZPny4VqxYoaysLDVs2PC8Y6vb9+UveZPz16rD92ZQUJAaN24sSWrfvr22bNmiWbNm6eWXX64wtjofR29y/lp1OI45OTkqLCxUu3bt3MvKysqUlZWlOXPmyOl0qkaNGh7vqezjyeU5HwsKClL79u21Zs0a9zKXy6U1a9ac81p0fHy8x3hJcjgc57127UsXk/HXysrKtHPnTjVo0KCqpnnJVbfjWJlyc3P9+lgahqHhw4dr2bJlWrt2reLi4i74nup4PC8m569Vx+9Nl8slp9N51nXV8Tiey/ly/lp1OI49evTQzp07lZub6/7ToUMHDR48WLm5uRUKk1QFx/Oibh9HpXrrrbcMu91uLFiwwNizZ48xbNgwIzw83MjPzzcMwzDuv/9+48knn3SP/+STT4yaNWsaU6dONfbu3Ws888wzRmBgoLFz505fRbggbzOOGzfOWL16tXHw4EEjJyfHGDhwoBEcHGzs3r3bVxEu6KeffjK2b99ubN++3ZBkTJ8+3di+fbtx+PBhwzAM48knnzTuv/9+9/gvvvjCqFWrljF69Ghj7969xty5c40aNWoYGRkZvopgirc5Z8yYYSxfvtzYv3+/sXPnTmPEiBFGQECA8cEHH/gqwgU9/PDDRlhYmLFu3Trju+++c/85efKke8zl8H15MTmr2/fmk08+aaxfv944dOiQsWPHDuPJJ580bDabkZmZaRjG5XEcDcP7nNXtOJ7Lr396rqqPJ6XJT8yePdu4+uqrjaCgIOPmm282Nm7c6F7XtWtXY8iQIR7jlyxZYtxwww1GUFCQ0aJFC2PlypWXeMbe8ybjyJEj3WOjoqKMPn36GNu2bfPBrM0r/9H6X/8pzzVkyBCja9euFd7Tpk0bIygoyLj22muN119//ZLP21ve5pw8ebJx3XXXGcHBwUZERITRrVs3Y+3atb6ZvElnyyfJ4/hcDt+XF5Ozun1vDh061GjUqJERFBRkXHnllUaPHj3cRcIwLo/jaBje56xux/Fcfl2aqvp42gzDMC7uHBUAAIB1cE8TAACACZQmAAAAEyhNAAAAJlCaAAAATKA0AQAAmEBpAgAAMIHSBAAAYAKlCQAA+LWsrCzdeeediomJkc1m0/Lly73ehmEYmjp1qm644QbZ7XZdddVVeu6557zaBr+wFwAA+LUTJ06odevWGjp0qO6+++6L2saIESOUmZmpqVOnqmXLljpy5IiOHDni1TZ4IjiAy1Z+fr4mTZqklStX6uuvv1ZYWJgaN26s++67T0OGDFGtWrV8PUUAXrLZbFq2bJn69evnXuZ0OvW3v/1N//rXv3Ts2DHdeOONmjx5srp16yZJ2rt3r1q1aqVdu3apSZMmF71vzjQBuCx98cUXuvXWWxUeHq6JEyeqZcuWstvt2rlzp+bPn6+rrrpKv/nNb3w9TQCVYPjw4dqzZ4/eeustxcTEaNmyZerVq5d27typ66+/Xu+9956uvfZarVixQr169ZJhGEpISNCUKVMUERFhfkcX/VvrAMCPJSUlGQ0bNjSKi4vPut7lchmGYRhHjx41kpOTjfr16xt16tQxunfvbuTm5rrHPfPMM0br1q2NN99802jUqJERGhpq/P73vzeKiorcY8rKyoyJEyca11xzjREcHGy0atXKWLp0qXv9kSNHjEGDBhn169c3goODjcaNGxuvvfZaFSUHLm+SjGXLlrlfHz582KhRo4bxzTffeIzr0aOHkZaWZhiGYfz5z3827Ha70bFjRyMrK8v9y9K7d+/u1b450wTgsvPjjz8qMzNTEydOVO3atc86xmazSZLuuecehYSE6P3331dYWJhefvll9ejRQ59//rn7/0APHjyo5cuXa8WKFTp69Kh+97vf6fnnn3ffRDpp0iT985//1Lx583T99dcrKytL9913n6688kp17dpVY8aM0Z49e/T++++rfv36OnDggE6dOnVpPgzgMrdz506VlZXphhtu8FjudDpVr149SZLL5ZLT6dSbb77pHvfqq6+qffv22rdvn+lLdpQmAJedAwcOyDCMCn8R1q9fX6dPn5YkpaSk6M4779TmzZtVWFgou90uSZo6daqWL1+ut99+W8OGDZP081+4CxYsUJ06dSRJ999/v9asWaPnnntOTqdTEydO1AcffKD4+HhJ0rXXXquPP/5YL7/8srp27aq8vDy1bdtWHTp0kCRdc801l+JjACyhuLhYNWrUUE5OjmrUqOGx7oorrpAkNWjQQDVr1vQoVs2aNZMk5eXlUZoA4Nc2b94sl8ulwYMHy+l06tNPP1VxcbH7/0bLnTp1SgcPHnS/vuaaa9yFSfr5L+DCwkJJPxe0kydPqmfPnh7bKCkpUdu2bSVJDz/8sAYMGKBt27YpMTFR/fr1U6dOnaoqJmApbdu2VVlZmQoLC3Xbbbeddcytt96qM2fO6ODBg7ruuuskSZ9//rkkqVGjRqb3RWkCcNlp3LixbDab9u3b57H82muvlSSFhIRI+vn/UBs0aKB169ZV2EZ4eLj73wMDAz3W2Ww2uVwu9zYkaeXKlbrqqqs8xpWfverdu7cOHz6sVatWyeFwqEePHkpJSdHUqVMvPiRgIcXFxTpw4ID79aFDh5Sbm6uIiAjdcMMNGjx4sB544AFNmzZNbdu21ffff681a9aoVatW6tu3rxISEtSuXTsNHTpUM2fOlMvlUkpKinr27Fnhst55/a83ZAGAP0pMTDSuuuqqs94I3rVrV2PEiBFGZmamUaNGDePQoUPn3E75jeC/NGPGDKNRo0aGYRhGUVGRYbfbjTfffNP03ObNm2fUqVPH9HjA6j788ENDUoU/Q4YMMQzDMEpKSoz09HTjmmuuMQIDA40GDRoY/fv3N3bs2OHexjfffGPcfffdxhVXXGFERUUZf/jDH4wff/zRq3lwpgnAZenvf/+7br31VnXo0EFjx45Vq1atFBAQoC1btuizzz5T+/btlZCQoPj4ePXr109TpkzRDTfcoG+//VYrV65U//793fcgnU+dOnX017/+VaNGjZLL5VLnzp11/PhxffLJJwoNDdWQIUOUnp6u9u3bq0WLFnI6nVqxYoX7fgoAF9atWzcZ53msZGBgoMaNG6dx48adc0xMTIzeeeed/2kelCYAl6XrrrtO27dv18SJE5WWlqavv/5adrtdzZs311//+lf95S9/kc1m06pVq/S3v/1Nf/zjH/X9998rOjpaXbp0UVRUlOl9TZgwQVdeeaUmTZqkL774QuHh4WrXrp2eeuopSVJQUJDS0tL05ZdfKiQkRLfddpveeuutqooOoIrwRHAAAAAT+IW9AAAAJlCaAAAATKA0AQAAmEBpAgAAMIHSBAAAYAKlCQAAwARKEwAAgAmUJgAAABMoTQAAACZQmgAAAEygNAEAAJjw/wDqYhyNOrXZ9gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab\n",
    "from Bio.SeqUtils import GC \n",
    "records = [len(rec) for rec in SeqIO.parse(filename, \"fasta\")] \n",
    "pylab.xlabel(\"Genes\") \n",
    "pylab.ylabel(\"GC Percentage\") \n",
    "pylab.grid()\n",
    "pylab.hist(records,bins=5) \n",
    "gc = sorted(GC(rec.seq) for rec in SeqIO.parse(filename, \"fasta\"))\n",
    "pylab.plot(gc) \n",
    "#https://marinegenomics.oist.jp/symb/viewer/download?project_id=37\n",
    "#https://compeau.cbd.cmu.edu/teaching/great-ideas-in-computational-biology/\n",
    "#https://pubs.acs.org/doi/10.1021/acssynbio.1c00329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6af8be6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2779, -0.2634, -0.3469, -0.3543, -0.1871, -0.3220, -0.2789,  ..., -0.1592, -0.2356, -0.1920, -0.2858, -0.0987, -0.4949,\n",
       "           -0.2458]]]], device='cuda:0', grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "c_hid=16\n",
    "latent_dim = 3\n",
    "kernel_size = (1, 5)\n",
    "finishDemo= nn.Sequential(\n",
    "            nn.Conv2d(1, c_hid, kernel_size=(1, 5)),  # 32x32 => 16x16\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=kernel_size),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(c_hid, 2 * c_hid, kernel_size=kernel_size),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=kernel_size),\n",
    "            nn.LeakyReLU(), #L\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(2 * c_hid, 1, kernel_size=kernel_size),  # 8x8 => 4x4\n",
    "            #nn.Linear(180, 3)\n",
    ").to('cuda:0')\n",
    "\n",
    "# buf = numpy.fromfile( dataFile, dtype=np.uint8, count=16384, offset=offs)\n",
    "\n",
    "nucleotides = []\n",
    "# with open('./maize_pseudohap.fasta.gz', 'r') as f:\n",
    "#     for line in f:\n",
    "#         nucleotides.append(line)\n",
    "def read_fasta(filename):\n",
    "    \"\"\"\n",
    "    Read a FASTA file and return a dictionary with sequence headers as keys and sequences as values.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        sequences = {}\n",
    "        header = None\n",
    "        sequence = []\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            line = line.strip()  # Remove whitespace\n",
    "            count+= 1\n",
    "            print(line)\n",
    "            if (count> 100):\n",
    "                break\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "            if line.startswith(\">\"):  # Header line\n",
    "                if header:  # If there's already a previous header, save the sequence\n",
    "                    sequences[header] = ''.join(sequence)\n",
    "                    sequence = []\n",
    "                header = line[1:]  # Exclude the \">\" symbol\n",
    "            else:  # Sequence line\n",
    "                sequence.append(line)\n",
    "        \n",
    "        # Save the last sequence\n",
    "        if header:\n",
    "            sequences[header] = ''.join(sequence)\n",
    "        \n",
    "    return sequences\n",
    "\n",
    "\n",
    "    \n",
    "from Bio import SeqIO\n",
    "\n",
    "filename = 'maize_pseudohap.fasta'\n",
    "filename = 'symA3_37.fasta'\n",
    "count = 0\n",
    "\n",
    "seq = []\n",
    "with open(filename, \"r\") as fasta_file:\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        if count > 100: break\n",
    "        #print(record.id)   # Prints the sequence header (identifier)\n",
    "        seq.append(record.seq)  # Prints the sequence\n",
    "        count += 1\n",
    "s = seq[0]\n",
    "s= str(s)\n",
    "img = []\n",
    "charToI = {\n",
    "    'T': 0,\n",
    "    'A': 1,\n",
    "    'G': 2,\n",
    "    'C': 3,\n",
    "    'N': 4\n",
    "}\n",
    "for char in s:\n",
    "    img.append(charToI[char])\n",
    "import torch\n",
    "tensor = torch.Tensor([[[img]]]).to('cuda:0')\n",
    "finishDemo(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c7d6a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain_imgs = torch.zeros(4, 3, 32, 32)\n",
    "# # Single color channel\n",
    "# plain_imgs[1, 0] = 1\n",
    "# # Checkboard pattern\n",
    "# plain_imgs[2, :, :16, :16] = 1\n",
    "# plain_imgs[2, :, 16:, 16:] = -1\n",
    "# # Color progression\n",
    "# xx, yy = torch.meshgrid(torch.linspace(-1, 1, 32), torch.linspace(-1, 1, 32))\n",
    "# plain_imgs[3, 0, :, :] = xx\n",
    "# plain_imgs[3, 1, :, :] = yy\n",
    "# visualize_reconstructions(model_di#ct[256][\"model\"], plain_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "82474d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 20801 genes that are detected 20 counts (shared).\n",
      "Normalized count data: X, spliced, unspliced.\n",
      "Extracted 2000 highly variable genes.\n",
      "computing neighbors\n",
      "    finished (0:00:01) --> added \n",
      "    'distances' and 'connectivities', weighted adjacency matrices (adata.obsp)\n",
      "computing moments based on connectivities\n",
      "    finished (0:00:00) --> added \n",
      "    'Ms' and 'Mu', moments of un/spliced abundances (adata.layers)\n",
      "computing velocities\n",
      "    finished (0:00:01) --> added \n",
      "    'velocity', velocity vectors for each individual cell (adata.layers)\n"
     ]
    }
   ],
   "source": [
    "#!pip install scvelo\n",
    "import scvelo as scv\n",
    "adata = scv.datasets.pancreas()\n",
    "\n",
    "scv.pp.filter_genes(adata, min_shared_counts=20)\n",
    "scv.pp.normalize_per_cell(adata)\n",
    "scv.pp.filter_genes_dispersion(adata, n_top_genes=2000)\n",
    "scv.pp.log1p(adata)\n",
    "scv.pp.moments(adata, n_pcs=30, n_neighbors=30)\n",
    "scv.tl.velocity(adata)\n",
    "#scv.pl.velocity_embedding_stream(adata, basis='umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "767d5681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2000' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2000/2000 00:07&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.27424242424242423, 0.2791712104689204, 362, 1280)\n",
      "(0.2431818181818182, 0.28789531079607417, 321, 1320)\n",
      "(0.15757575757575756, 0.811123227917121, 208, 3719)\n",
      "(0.13106060606060607, 0.9866957470010905, 173, 4524)\n",
      "(0.17045454545454544, 0.9936750272628135, 225, 4556)\n",
      "(0.1856060606060606, 0.995856052344602, 245, 4566)\n",
      "(0.18409090909090908, 0.9967284623773174, 243, 4570)\n",
      "(0.18106060606060606, 0.9978189749182116, 239, 4575)\n",
      "(0.19090909090909092, 0.9982551799345692, 252, 4577)\n",
      "(0.28484848484848485, 0.9967284623773174, 376, 4570)\n",
      "(0.36742424242424243, 0.9967284623773174, 485, 4570)\n",
      "(0.43636363636363634, 0.9962922573609596, 576, 4568)\n",
      "(0.5189393939393939, 0.9945474372955289, 685, 4560)\n",
      "(0.578030303030303, 0.9921483097055616, 763, 4549)\n",
      "(0.6462121212121212, 0.9906215921483097, 853, 4542)\n",
      "(0.6984848484848485, 0.990185387131952, 922, 4540)\n",
      "(0.7568181818181818, 0.9890948745910578, 999, 4535)\n",
      "(0.8083333333333333, 0.9884405670665213, 1067, 4532)\n",
      "(0.8431818181818181, 0.9871319520174482, 1113, 4526)\n",
      "(0.871969696969697, 0.9871319520174482, 1151, 4526)\n",
      "(0.9, 0.9869138495092693, 1188, 4525)\n",
      "(0.918939393939394, 0.9869138495092693, 1213, 4525)\n",
      "(0.9378787878787879, 0.9866957470010905, 1238, 4524)\n",
      "(0.95, 0.9869138495092693, 1254, 4525)\n",
      "(0.9613636363636363, 0.9873500545256271, 1269, 4527)\n",
      "(0.9696969696969697, 0.9875681570338058, 1280, 4528)\n",
      "(0.9742424242424242, 0.9875681570338058, 1286, 4528)\n",
      "(0.978030303030303, 0.9882224645583424, 1291, 4531)\n",
      "(0.9787878787878788, 0.9886586695747, 1292, 4533)\n",
      "(0.9795454545454545, 0.9886586695747, 1293, 4533)\n",
      "(0.9825757575757575, 0.9886586695747, 1297, 4533)\n",
      "(0.9848484848484849, 0.9895310796074155, 1300, 4537)\n",
      "(0.9856060606060606, 0.9895310796074155, 1301, 4537)\n",
      "(0.9863636363636363, 0.9899672846237731, 1302, 4539)\n",
      "(0.9856060606060606, 0.9904034896401308, 1301, 4541)\n",
      "(0.9871212121212121, 0.9904034896401308, 1303, 4541)\n",
      "(0.9871212121212121, 0.9910577971646674, 1303, 4544)\n",
      "(0.9871212121212121, 0.9914940021810251, 1303, 4546)\n",
      "(0.9856060606060606, 0.9914940021810251, 1301, 4546)\n",
      "(0.9863636363636363, 0.9914940021810251, 1302, 4546)\n",
      "(0.9878787878787879, 0.9917121046892039, 1304, 4547)\n"
     ]
    }
   ],
   "source": [
    "dev = 'cuda:0'\n",
    "def test_prediction(test_predictions):\n",
    "    ctrl = test_predictions.sum(1).tolist()[0]\n",
    "    isFalse = len([sum(row) for idx, row in enumerate(test_predictions.tolist()) if sum(row) <= ctrl and t_dep[idx] == 0])\n",
    "    isTrue = len([sum(row) for idx, row in enumerate(test_predictions.tolist()) if sum(row) > ctrl and t_dep[idx] == 1])\n",
    "    allFalse = len([sum(row) for idx, row in enumerate(test_predictions.tolist()) if t_dep[idx] == 0])\n",
    "    allTrue = len([sum(row) for idx, row in enumerate(test_predictions.tolist()) if t_dep[idx] == 1])\n",
    "    return (isFalse / allFalse, isTrue / allTrue, isFalse, isTrue)\n",
    "def plot_loss(l):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    legends = []\n",
    "    plt.plot(l) \n",
    "    plt.plot([0, len([i for k,i in enumerate(rowGeneExpression.values()) if dependent_variables[k]])], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "    plt.legend(legends);\n",
    "\n",
    "numerical_values = df.select_dtypes(include=[int, float]).values.tolist()\n",
    "t_indep = torch.Tensor(numerical_values).to(dev)\n",
    "# t_indep = t_indep / vals\n",
    "# λλλλλ.requires_grad_(True)\n",
    "#3 variations, test, t_indep and t_indep+embedding\n",
    "resultant_tensor = t_indep\n",
    "encodedOutput.requires_grad_(True)\n",
    "#resultant_tensor = torch.cat((t_indep.to(dev),tensor.to(dev)), 1)\n",
    "#resultant_tensor = λλλλλ\n",
    "#resultant_tensor = tensor\n",
    "vals, indices = resultant_tensor.max(dim=0)\n",
    "resultant_tensor = resultant_tensor / vals\n",
    "resultant_tensor = resultant_tensor.to(dev)\n",
    "test_indep = torch.tensor([[t_dep[k].item() for i in enumerate(range(resultant_tensor.shape[1]))] for k, i in enumerate(range(resultant_tensor.shape[0]))])\n",
    "dim = resultant_tensor.shape[1]\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(dim,dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(dim,dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(dim, dim),\n",
    "    nn.Sigmoid()\n",
    ").to(dev)\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=.1, \n",
    "    weight_decay=0.01\n",
    ")\n",
    "# model = model.to(device)\n",
    "# input_tensor = input_tensor.to(device)\n",
    "\n",
    "n_iterations = 1000\n",
    "loss_track = []\n",
    "accuracy_track = []\n",
    "no_entropy = []\n",
    "loss_function = torch.nn.BCELoss()\n",
    "def plot_loss_update(epoch, epochs, mb, train_loss, valid_loss):\n",
    "    \"\"\" dynamically print the loss plot during the training/validation loop.\n",
    "        expects epoch to start from 1.\n",
    "    \"\"\"\n",
    "    x = range(1, epoch+1)\n",
    "    y = np.concatenate((train_loss, valid_loss))\n",
    "    graphs = [[x,train_loss], [x,valid_loss]]\n",
    "    x_margin = 0.2\n",
    "    y_margin = 0.05\n",
    "    x_bounds = [1-x_margin, epochs+x_margin]\n",
    "    y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]\n",
    "\n",
    "    mb.update_graph(graphs, x_bounds, y_bounds)\n",
    "mb = master_bar(range(1))\n",
    "def plot_loss_update(epoch, epochs, mb, train_loss, valid_loss):\n",
    "    \"\"\" dynamically print the loss plot during the training/validation loop.\n",
    "        expects epoch to start from 1.\n",
    "    \"\"\"\n",
    "    x = range(1, epoch+1)\n",
    "    y = np.concatenate((train_loss, valid_loss))\n",
    "    print(x,y)\n",
    "    graphs = [[x,train_loss], [x,valid_loss]]\n",
    "    x_margin = 0.2\n",
    "    y_margin = 0.05\n",
    "    x_bounds = [1-x_margin, epochs+x_margin]\n",
    "    y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]\n",
    "    print(x_bounds, y_bounds)\n",
    "    mb.update_graph(graphs, x_bounds, y_bounds)\n",
    "\n",
    "for i in mb:    \n",
    "#for j in progress_bar(range(2000), parent=mb):\n",
    "    for j in progress_bar(range(2000)):\n",
    "        loss = loss_function(model(resultant_tensor).sum(1).sigmoid(), t_dep.to(dev))\n",
    "        optimizer.zero_grad()  # 3\n",
    "        loss.backward(retain_graph=True)  # 4\n",
    "        optimizer.step()  # 5\n",
    "        if j == 1 or j % 50 == 0:\n",
    "            test_predictions = model(resultant_tensor)\n",
    "            #print(loss.item(), test_predictions.sum().item() / 8)\n",
    "            print(test_prediction(test_predictions))\n",
    "        loss_track.append(loss.item())\n",
    "        accuracy_track.append(test_predictions.sum().item() / 8)\n",
    "        #no_entropy += [test_predictions.sum().item() / 8]\n",
    "        #         k = 100 * i + j\n",
    "        #         x = np.arange(0, 2*k*np.pi/1000, 0.01)\n",
    "        #         y1, y2 = np.cos(x), np.sin(x)\n",
    "        #         graphs = [[x,y1], [x,y2]]\n",
    "        #         x_bounds = [0, 2*np.pi]\n",
    "        #         y_bounds = [-1,1]\n",
    "        #         mb.update_graph(graphs, x_bounds, y_bounds)\n",
    "        #         print(loss_track, accuracy_track)\n",
    "        #print(loss_track, accuracy_track)\n",
    "        #plot_loss_update(j, n_iterations, mb, loss_track, accuracy_track)\n",
    "        #for batch in progress_bar(range(2), parent=mb): sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "53b21b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "cont_keys = {}\n",
    "for key in filteredGeneCellLists:\n",
    "    cont_keys[key] = count\n",
    "    count += 1\n",
    "continuousFilteredGeneCellLists = {}\n",
    "for k in list(filteredGeneCellLists.keys()):\n",
    "    continuousFilteredGeneCellLists[cont_keys[k]] = filteredGeneCellLists[k]\n",
    "#continuousFilteredGeneCellLists\n",
    "#cont_keys\n",
    "#len(list(continuousFilteredGeneCellLists.keys()))\n",
    "#cellCountWithinGroup\n",
    "#zscore\n",
    "#continuousFilteredGeneCellLists check\n",
    "# x = cells in group(s) , cellCountWithinGroup\n",
    "# y = genes affected \n",
    "# z = cluster number\n",
    "#for each cell\n",
    "#make a graph -> \n",
    "#negative * negative = positive, \n",
    "#x  cluster \"name\" or index (clusters should change)\n",
    "#y = genes above/below threshold \n",
    "#z = total dist above threshold\n",
    "#convert 200 dimensions to 3\n",
    "cellGroups = [0 for i in list(range(5905))]\n",
    "cellGroupLengths = [0 for i in list(range(5905))]\n",
    "cellDistCounts = [0 for i in list(range(5905))]\n",
    "for column in continuousFilteredGeneCellLists:\n",
    "    for cell in continuousFilteredGeneCellLists[column]:\n",
    "        cellGroups[cell] = column\n",
    "        cellGroupLengths[cell] = len(continuousFilteredGeneCellLists[column])\n",
    "for idx, row in enumerate(mat_for_embed):\n",
    "    for val in row: \n",
    "        cellDistCounts[idx] += val\n",
    "        \n",
    "#cellDistCounts\n",
    "#https://en.wikipedia.org/wiki/Foundation%27s_Edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "489e50c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find gradient of row\n",
    "#given two rows that belong to same perturbation -> return identical or similar values\n",
    "#given a matrix -> return mx5 vals that can be transformed into a p-val\n",
    "#capture the 'features' that can be used to reconstruct -> molecular response\n",
    "# find the molecular response of each phenotype interaction or simply the gene by itself\n",
    "#gradient ascent -> descent -> find distributions -> sparsify them\n",
    "#ring that captures relevant known info about you and stores it cryptographically \n",
    "#given n rows and a matrix -> return a tuple that can be used to identify rows which belong to a perturbation response\n",
    "#given an expression matrix -> group cells by perturbation profiles\n",
    "#transcriptomics, genomics, proteinomics, metabolomics\n",
    "#recorded actions -> comic generator\n",
    "#script -> comic generator\n",
    "#comic -> animation generator\n",
    "#$https://www.youtube.com/watch?v=DzNmUNvnB04\n",
    "#plot the matrix before + after - 200x6k to 3x6k -> bright colors for rows with perturbations \n",
    "#perturbations defined as belonging to a group of rows that have multiple columns that are covarying from mean-zscore\n",
    "#makeCoolStuff = [[float(k) for k in range(5905)] for i in range(200)]\n",
    "#https://explained.ai/regularization/index.html\n",
    "#oft constraint with non-regularized loss function (blue-red) term and penalty term (orange).\n",
    "#invent a new architecture \n",
    "#that captures probability of perturbation across a matrix\n",
    "#https://www.10xgenomics.com/resources/datasets/5-k-a-549-lung-carcinoma-cells-no-treatment-transduced-with-a-crispr-pool-3-1-standard-6-0-0\n",
    "# all_url = [\n",
    "# #     \"https://zenodo.org/record/7416068/files/AdamsonWeissman2016_GSM2406675_10X001.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/AdamsonWeissman2016_GSM2406677_10X005.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/AdamsonWeissman2016_GSM2406681_10X010.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/AissaBenevolenskaya2021.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/ChangYe2021.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/DatlingerBock2017.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/DatlingerBock2021.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/DixitRegev2016.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/FrangiehIzar2021_protein.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/FrangiehIzar2021_RNA.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/GasperiniShendure2019_atscale.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/GasperiniShendure2019_highMOI.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/GasperiniShendure2019_lowMOI.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/GehringPachter2019.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/McFarlandTsherniak2020.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/NormanWeissman2019_filtered.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/PapalexiSatija2021_eccite_arrayed_protein.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/PapalexiSatija2021_eccite_arrayed_RNA.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/PapalexiSatija2021_eccite_protein.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/PapalexiSatija2021_eccite_RNA.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/ReplogleWeissman2022_K562_essential.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/ReplogleWeissman2022_K562_gwps.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/ReplogleWeissman2022_rpe1.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SchiebingerLander2019_GSE106340.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SchiebingerLander2019_GSE115943.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SchraivogelSteinmetz2020_TAP_SCREEN__chromosome_11_screen.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SchraivogelSteinmetz2020_TAP_SCREEN__chromosome_8_screen.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/ShifrutMarson2018.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SrivatsanTrapnell2020_sciplex2.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SrivatsanTrapnell2020_sciplex3.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SrivatsanTrapnell2020_sciplex4.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/TianKampmann2019_day7neuron.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/TianKampmann2019_iPSC.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/TianKampmann2021_CRISPRa.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/TianKampmann2021_CRISPRi.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/WeinrebKlein2020.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/XieHon2017.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/ZhaoSims2021.h5ad?download=1\"\n",
    "# ]\n",
    "#scarches.dataset.remove_sparsity(adata)\n",
    "#https://docs.scarches.org/en/latest/api/models.html\n",
    "# mdata = muon.read_10x_h5(\"pbmc_10k_protein_v3_filtered_feature_bc_matrix.h5\")\n",
    "# scvi.model.TOTALVI.setup_mudata(mdata, modalities={\"rna_layer\": \"rna\": \"protein_layer\": \"prot\"})\n",
    "# vae = scvi.model.TOTALVI(mdata)\n",
    "#https://docs.scvi-tools.org/en/stable/api/reference/scvi.module.LDVAE.html\n",
    "#[i for i in test_predictions.tolist() if i < 1]\n",
    "# Regularization in Logistic Regression\n",
    "# Regularization is extremely important in logistic regression modeling. Without regularization, the asymptotic nature of logistic regression would keep driving loss towards 0 in high dimensions. Consequently, most logistic regression models use one of the following two strategies to dampen model complexity:\n",
    "# L2 regularization.\n",
    "# Early stopping, that is, limiting the number of training steps or the learning rate.\n",
    "# (We'll discuss a third strategy—L1 regularization—in a later module.)\n",
    "# Imagine that you assign a unique id to each example, and map each id to its own feature. If you don't specify a regularization function, the model will become completely overfit. That's because the model would try to drive loss to zero on all examples and never get there, driving the weights for each indicator feature to +infinity or -infinity. This can happen in high dimensional data with feature crosses, when there’s a huge mass of rare crosses that happen only on one example each.\n",
    "# Fortunately, using L2 or early stopping will prevent this problem.\n",
    "#[ x for x in [iden(sum(item), 10)  for item in test_predictions.tolist()] if x > .1]\n",
    "#plot(loss_track)\n",
    "#make demo = good\n",
    "\n",
    "def plot_loss(l):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    legends = []\n",
    "#     blue = [i for k,i in enumerate(rowGeneExpression.values()) if dependent_variables[k]]\n",
    "#     oj =[i for k,i in enumerate(rowGeneExpression.values()) if not dependent_variables[k]]\n",
    "#     blue.sort()\n",
    "#     oj.sort()\n",
    "#     plt.plot((blue)) #blue true peturbation \n",
    "    plt.plot(l) #orange false ctrl\n",
    "    #legends.append('param %d' % i)\n",
    "    plt.plot([0, len([i for k,i in enumerate(rowGeneExpression.values()) if dependent_variables[k]])], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "    plt.legend(legends);\n",
    "# #https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02021-3\n",
    "# # Medicine Finding anomalies in radiology images, including CT, MRI, and X-ray images; counting features in pathology slides; measuring features in ultrasounds; diagnosing diabetic retinopathy\n",
    "# #Biology Folding proteins; classifying proteins; many genomics tasks, such as tumor-normal sequencing and classifying clinically actionable genetic mutations; cell classification; analyzing protein/protein interactions\n",
    "# #Other applications Financial and logistical forecasting, text to speech, and much more…\n",
    "# # humor analysis - larry david vs seinfeld ? \n",
    "#https://www.kaggle.com/code/jhoward/why-you-should-use-a-framework\n",
    "#handle \"values outside of domain\" by \"SVM\"\n",
    "#random forest classifier\n",
    "#logisitc regression - hard to get right\n",
    "#correct transformations, outlier handling, correct interactions\n",
    "#os.listdir('./data_sets')\n",
    "#wget -m http://www.example.com 2>&1 | grep '^--' | awk '{ print $3 }' | grep -v '\\.\\(css\\|js\\|png\\|gif\\|jpg\\|JPG\\)$' > urls.txt\n",
    "#https://academic.oup.com/bib/article/22/4/bbaa268/5943793\n",
    "#plot(loss_track)\n",
    "#https://terrytao.files.wordpress.com/2011/02/matrix-book.pdf\n",
    "#https://academic.oup.com/bioinformatics/article/36/Supplement_2/i610/6055927?login=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b693a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #sc.pl.StackedViolin(adata, , groupby='', use_raw=None, log=False, num_categories=7, categories_order=None, title=None, figsize=None, gene_symbols=None, var_group_positions=None, var_group_labels=None, var_group_rotation=None, layer=None, standard_scale=None, ax=None, vmin=None, vmax=None, vcenter=None, norm=None)\n",
    "# sc.pl.StackedViolin(adata, list(hv_genes), groupby='perturbation', dendrogram=True).show()\n",
    "# hg = list(hv_genes)[100:]\n",
    "# sc.pl.DotPlot(adata, hg,  groupby='perturbation').show()\n",
    "# sc.pl.MatrixPlot(adata, hg, groupby='perturbation').show()\n",
    "# first = adata.X.A[:100]\n",
    "# second = adata.X.T.A[:100]\n",
    "# perturbations = []\n",
    "# for key, row in enumerate(first):\n",
    "#     trackPerts = []\n",
    "#     for column in row:\n",
    "#         if column > 0: trackPerts.append(column)\n",
    "#     print(t_dep[key].item(), len(trackPerts))\n",
    "#https://datahacker.rs/003-gans-autoencoder-implemented-with-pytorch/\n",
    "#https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/\n",
    "#https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf\n",
    "#https://www.cs.utoronto.ca/~hinton/absps/cogscibm.pdf\n",
    "#Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\n",
    "# Pierre-Antoine Manzagol. 2008. Extracting and\n",
    "# composing robust features with denoising autoencoders. In Proceedings of the 25th international\n",
    "# conference on Machine learning, pages 1096–1103.\n",
    "# ACM.\n",
    "#https://github.com/fastai/course22p2/blob/master/nbs/08_autoencoder.ipynb\n",
    "#file:///Users/adnanwahab/Downloads/Molecular%20Systems%20Biology%20-%202016%20-%20Angermueller.pdf\n",
    "#https://www.cell.com/patterns/pdf/S2666-3899(21)00001-5.pdf\n",
    "#https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC3.ipynb\n",
    "##https://www.genome.gov/research-funding/Funded-Programs-Projects/Multi-Omics-for-Health-and-Disease\n",
    "#IFrame('https://www.shadertoy.com/embed/dlScDy?gui=true&t=10&paused=true&muted=false', width=700, height=350)\n",
    "#https://github.com/AntixK/PyTorch-VAE/blob/master/models/lvae.py\n",
    "# from torchvision.datasets import CIFAR10\n",
    "# import os\n",
    "# import urllib.request\n",
    "# from urllib.error import HTTPError\n",
    "\n",
    "# import lightning as L\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib_inline.backend_inline\n",
    "# import seaborn as sns\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# import torch.utils.data as data\n",
    "# import torchvision\n",
    "# from lightning.pytorch.callbacks import Callback, LearningRateMonitor, ModelCheckpoint\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# from torchvision import transforms\n",
    "# from torchvision.datasets import CIFAR10\n",
    "# from tqdm.notebook import tqdm\n",
    "# model_dict = {}\n",
    "\n",
    "# class GenerateCallback(Callback):\n",
    "#     def __init__(self, input_imgs, every_n_epochs=1):\n",
    "#         super().__init__()\n",
    "#         self.input_imgs = input_imgs  # Images to reconstruct during training\n",
    "#         # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "#         self.every_n_epochs = every_n_epochs\n",
    "\n",
    "#     def on_train_epoch_end(self, trainer, pl_module):\n",
    "#         if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "#             # Reconstruct images\n",
    "#             input_imgs = self.input_imgs.to(pl_module.device)\n",
    "#             with torch.no_grad():\n",
    "#                 pl_module.eval()\n",
    "#                 reconst_imgs = pl_module(input_imgs)\n",
    "#                 pl_module.train()\n",
    "#             # Plot and add to tensorboard\n",
    "#             imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0, 1)\n",
    "#             grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, range=(-1, 1))\n",
    "#             trainer.logger.experiment.add_image(\"Reconstructions\", grid, global_step=trainer.global_step)    \n",
    "    \n",
    "# def train_cifar(latent_dim):\n",
    "#     # Create a PyTorch Lightning trainer with the generation callback\n",
    "#     trainer = L.Trainer(\n",
    "#         default_root_dir=os.path.join(CHECKPOINT_PATH, \"cifar10_%i\" % latent_dim),\n",
    "#         accelerator=\"auto\",\n",
    "#         devices=1,\n",
    "#         max_epochs=500,\n",
    "#         callbacks=[\n",
    "#             ModelCheckpoint(save_weights_only=True),\n",
    "#             GenerateCallback(get_train_images(8), every_n_epochs=10),\n",
    "#             LearningRateMonitor(\"epoch\"),\n",
    "#         ],\n",
    "#     )\n",
    "#     trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "#     trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "#     # Check whether pretrained model exists. If yes, load it and skip training\n",
    "#     pretrained_filename = os.path.join(CHECKPOINT_PATH, \"cifar10_%i.ckpt\" % latent_dim)\n",
    "#     if os.path.isfile(pretrained_filename):\n",
    "#         print(\"Found pretrained model, loading...\")\n",
    "#         model = Autoencoder.load_from_checkpoint(pretrained_filename)\n",
    "#     else:\n",
    "#         model = Autoencoder(base_channel_size=32, latent_dim=latent_dim)\n",
    "#         trainer.fit(model, train_loader, val_loader)\n",
    "#     # Test best model on validation and test set\n",
    "#     val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
    "#     test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "#     result = {\"test\": test_result, \"val\": val_result}\n",
    "#     return model, result\n",
    "\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#            num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "#            base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "#            latent_dim : Dimensionality of latent representation z\n",
    "#            act_fn : Activation function used throughout the decoder network\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         c_hid = base_channel_size\n",
    "#         self.linear = nn.Sequential(nn.Linear(latent_dim, 2 * 16 * c_hid), act_fn())\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(\n",
    "#                 2 * c_hid, 2 * c_hid, kernel_size=3, output_padding=1, padding=1, stride=2\n",
    "#             ),  # 4x4 => 8x8\n",
    "#             act_fn(),\n",
    "#             nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "#             act_fn(),\n",
    "#             nn.ConvTranspose2d(2 * c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),  # 8x8 => 16x16\n",
    "#             act_fn(),\n",
    "#             nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "#             act_fn(),\n",
    "#             nn.ConvTranspose2d(\n",
    "#                 c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2\n",
    "#             ),  # 16x16 => 32x32\n",
    "#             nn.Tanh(),  # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear(x)\n",
    "#         x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "#         x = self.net(x)\n",
    "#         return x    \n",
    "    \n",
    "# class Autoencoder(L.LightningModule):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         base_channel_size: int,\n",
    "#         latent_dim: int,\n",
    "#         encoder_class: object = Encoder,\n",
    "#         decoder_class: object = Decoder,\n",
    "#         num_input_channels: int = 3,\n",
    "#         width: int = 32,\n",
    "#         height: int = 32,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         # Saving hyperparameters of autoencoder\n",
    "#         self.save_hyperparameters()\n",
    "#         # Creating encoder and decoder\n",
    "#         self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "#         self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "#         # Example input array needed for visualizing the graph of the network\n",
    "#         self.example_input_array = torch.zeros(2, num_input_channels, width, height)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"The forward function takes in an image and returns the reconstructed image.\"\"\"\n",
    "#         z = self.encoder(x)\n",
    "#         x_hat = self.decoder(z)\n",
    "#         return x_hat\n",
    "\n",
    "#     def _get_reconstruction_loss(self, batch):\n",
    "#         \"\"\"Given a batch of images, this function returns the reconstruction loss (MSE in our case)\"\"\"\n",
    "#         x, _ = batch  # We do not need the labels\n",
    "#         x_hat = self.forward(x)\n",
    "#         loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "#         loss = loss.sum(dim=[1, 2, 3]).mean(dim=[0])\n",
    "#         return loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "#         # Using a scheduler is optional but can be helpful.\n",
    "#         # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "#         scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.2, patience=20, min_lr=5e-5)\n",
    "#         return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         loss = self._get_reconstruction_loss(batch)\n",
    "#         self.log(\"train_loss\", loss)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         loss = self._get_reconstruction_loss(batch)\n",
    "#         self.log(\"val_loss\", loss)\n",
    "\n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         loss = self._get_reconstruction_loss(batch)\n",
    "#         self.log(\"test_loss\", loss)\n",
    "\n",
    "\n",
    "# from torchvision import transforms\n",
    "# #%matplotlib inline\n",
    "# #matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "# #matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "# sns.reset_orig()\n",
    "# sns.set()\n",
    "\n",
    "# # Tensorboard extension (for visualization purposes later)\n",
    "\n",
    "# # Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "# DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data\")\n",
    "# # Path to the folder where the pretrained models are saved\n",
    "# CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/tutorial9\")\n",
    "\n",
    "# # Setting the seed\n",
    "# L.seed_everything(42)\n",
    "\n",
    "# # Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# print(\"Device:\", device)\n",
    "# #model = Autoencoder(base_channel_size=32, latent_dim=latent_dim)\n",
    "\n",
    "# m = Encoder(200, 50, 10)\n",
    "# #m(data)\n",
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# # Loading the training dataset. We need to split it into a training and validation part\n",
    "# # train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "# # L.seed_everything(42)\n",
    "# # train_set, val_set = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "\n",
    "# # # Loading the test set\n",
    "# # test_set = CIFAR10(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# # # We define a set of data loaders that we can use for various purposes later.\n",
    "# # train_loader = data.DataLoader(train_set, batch_size=256, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "# # val_loader = data.DataLoader(val_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "# # test_loader = data.DataLoader(test_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "\n",
    "# def get_train_images(num):\n",
    "#     return torch.stack([train_dataset[i][0] for i in range(num)], dim=0)\n",
    "# base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial9/\"\n",
    "# # Files to download\n",
    "# pretrained_files = [\"cifar10_64.ckpt\", \"cifar10_128.ckpt\", \"cifar10_256.ckpt\", \"cifar10_384.ckpt\"]\n",
    "# # Create checkpoint path if it doesn't exist yet\n",
    "# os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# # For each file, check whether it already exists. If not, try downloading it.\n",
    "# # for file_name in pretrained_files:\n",
    "# #     file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "# #     if not os.path.isfile(file_path):\n",
    "# #         file_url = base_url + file_name\n",
    "# #         print(\"Downloading %s...\" % file_url)\n",
    "# #         try:\n",
    "# #             urllib.request.urlretrieve(file_url, file_path)\n",
    "# #         except HTTPError as e:\n",
    "# #             print(\n",
    "# #                 \"Something went wrong. Please try to download the files manually,\"\n",
    "# #                 \" or contact the author with the full output including the following error:\\n\",\n",
    "# #                 e,\n",
    "# #             )\n",
    "# def visualize_reconstructions(model, input_imgs):\n",
    "#     # Reconstruct images\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         reconst_imgs = model(input_imgs.to(model.device))\n",
    "#     reconst_imgs = reconst_imgs.cpu()\n",
    "\n",
    "#     # Plotting\n",
    "#     imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0, 1)\n",
    "#     grid = torchvision.utils.make_grid(imgs, nrow=4, normalize=True, range=(-1, 1))\n",
    "#     grid = grid.permute(1, 2, 0)\n",
    "#     plt.figure(figsize=(7, 4.5))\n",
    "#     plt.title(\"Reconstructed from %i latents\" % (model.hparams.latent_dim))\n",
    "#     plt.imshow(grid)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "# input_imgs = get_train_images(4)\n",
    "# def find_similar_images(query_img, query_z, key_embeds, K=8):\n",
    "#     # Find closest K images. We use the euclidean distance here but other like cosine distance can also be used.\n",
    "#     dist = torch.cdist(query_z[None, :], key_embeds[1], p=2)\n",
    "#     dist = dist.squeeze(dim=0)\n",
    "#     dist, indices = torch.sort(dist)\n",
    "#     # Plot K closest images\n",
    "#     imgs_to_display = torch.cat([query_img[None], key_embeds[0][indices[:K]]], dim=0)\n",
    "#     grid = torchvision.utils.make_grid(imgs_to_display, nrow=K + 1, normalize=True, range=(-1, 1))\n",
    "#     grid = grid.permute(1, 2, 0)\n",
    "#     plt.figure(figsize=(12, 3))\n",
    "#     plt.imshow(grid)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "# # Plot the closest images for the first N test images as example\n",
    "\n",
    "# def embed_imgs(model, data_loader):\n",
    "#     # Encode all images in the data_laoder using model, and return both images and encodings\n",
    "#     img_list, embed_list = [], []\n",
    "#     model.eval()\n",
    "#     for imgs, _ in tqdm(data_loader, desc=\"Encoding images\", leave=False):\n",
    "#         with torch.no_grad():\n",
    "#             z = model.encoder(imgs.to(model.device))\n",
    "#         img_list.append(imgs)\n",
    "#         embed_list.append(z)\n",
    "#     return (torch.cat(img_list, dim=0), torch.cat(embed_list, dim=0))\n",
    "\n",
    "\n",
    "# train_img_embeds = embed_imgs(model, train_loader)\n",
    "# test_img_embeds = embed_imgs(model, test_loader)\n",
    "# for i in range(8):\n",
    "#     find_similar_images(test_img_embeds[0][i], test_img_embeds[1][i], key_embeds=train_img_embeds)     \n",
    "    \n",
    "\n",
    "# latent_vectors = torch.randn(8, model.hparams.latent_dim, device=model.device)\n",
    "# # with torch.no_grad():\n",
    "# #     imgs = model.decoder(latent_vectors)\n",
    "# #     imgs = imgs.cpu()\n",
    "\n",
    "# # grid = torchvision.utils.make_grid(imgs, nrow=4, normalize=True, range=(-1, 1), pad_value=0.5)\n",
    "# # grid = grid.permute(1, 2, 0)\n",
    "# # plt.figure(figsize=(8, 5))\n",
    "# # plt.imshow(grid)\n",
    "# # plt.axis(\"off\")\n",
    "# # plt.show()\n",
    "\n",
    "#! ls ./data_sets/* -lh\n",
    "#https://github.com/chriswi93/Neural-Networks-and-Logistic-Regression-Backpropagation-in-depth\n",
    "# ![Alt text](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-021-22197-x/MediaObjects/41467_2021_22197_Fig3_HTML.png?as=webp)\n",
    "# https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bib/22/4/10.1093_bib_bbaa268/1/m_bbaa268f1.jpeg?Expires=1695201196&Signature=1KEY92u4ZstK959i3C6haCKHZ7-6ghmNkBQwGELax4hVBn6N0o7lasyTNgnHk6sQ6eP2yiV~E51~X8JdkQkF9D5PfM7pk0N-z1rOF1HJpYaNBZ7IrUSqzdj-lQHw-TTBMjlW8rFKnSWg8~Y0y2y7q7a1hGweo3LHFNk7pSxu0kgYUaN54HwRrCWvpuMe0Eq~PL4oIh857EOSI9YaYyZ4U3ilKNy9bzbEHrLUiGOdfBBvJV09gq5g1Xp3rl49KqxwnpaFVs1qEj0z94TBYtJMDnUXEoV8ZXGJ2ESWxaXQRGziXBHA-b5l2Ac40c2eSVvTgqGFK2ClL0yGFZM5J458dg__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA\n",
    "#https://muon-tutorials.readthedocs.io/en/latest/trimodal/tea-seq/1-TEA-seq-PBMC.html\n",
    "#solve known perturbations 100%\n",
    "#solve unknown perturbations -> when exactly \n",
    "#predict perturbations before they occur -> multimodal\n",
    "#solve adjacent problems in preventative medicine#\n",
    "#put flask in notebook / torch script\n",
    "#https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed\n",
    "#-> make notebook where you can select all the files and learn everything there is to know in single cell omics (transcript, protein, metabolimcs )\n",
    "\n",
    "\n",
    "\n",
    "# The cancer sample matrix was normalized by the Z-score method, \n",
    "#which scaled the mean of each row (corresponding to feature edge) to zero and variance to one. \n",
    "#First, the rows of the matrix were clustered using hierarchical clustering based on the complete linkage method with the cluster number set to 100, \n",
    "#and clusters containing more than 30 edges were retained.\n",
    "#We then computed the mean values of perturbation for each edge in each subtype through Z-scores.\n",
    "#For each subtype, we counted the percentage of edges whose absolute value of the average perturbation was greater than 0.5 in each retained cluster. \n",
    "#A cluster with a percentage greater than 70% was regarded as a perturbed cluster for this subtype. \n",
    "#All edges in all of the perturbed clusters for each subtype constituted the subtype-specific networks.\n",
    "#All genes involved in each subtype-specific network were used for pathway enrichment analysis by Metascape (http://metascape.org). \n",
    "#The KEGG and Reactome pathways with a P-value less than 0.01 were retained. \n",
    "#Finally, the subtype-specific pathways were identified.\n",
    "#grouping based on shared genes\n",
    "#network = nodes = cell\n",
    "#edges = shared gene expression above mean -> only retain those above 30 \n",
    "#graeter than > .5 of the zscore\n",
    "#a cluster with a percentage greater than ??? (look at ribosomes)\n",
    "# https://metascape.org/blog/\n",
    "# ##   *\n",
    "# #   /_\\\n",
    "# #  (@@)\n",
    "# #---T----\n",
    "# #  /\\\n",
    "# #_|  \\_\n",
    "#https://www.genecards.org/cgi-bin/carddisp.pl?gene=A1BG\n",
    "#https://cancer.sanger.ac.uk/cosmic#:~:text=COSMIC%2C%20the%20Catalogue%20Of%20Somatic,%2C%20mutation%2C%20etc.%20below.\n",
    "#https://observablehq.com/d/124e11318fe98788\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5d0af383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### https://www.kaggle.com/code/llttyy/open-problem-biological-ideas/notebook\n",
    "#https://www.kaggle.com/competitions/open-problems-multimodal/discussion/366961\n",
    "#extrapolate sca onto mm datasets\n",
    "# 1. train algo on all data sets available\n",
    "# 2. apply to all multi-omics datasets aviable\n",
    "# 3. identify features which have statistical correlation\n",
    "# 4. improve multi-view algo until it predicts perturbations at earliest possible onset -> \n",
    "# 5. find adjacent problems worth solving\n",
    "\n",
    "#invent algorithm for detecting pertrubations \n",
    "#loop through every high_variable_column\n",
    "#loop through every row\n",
    "#fidn rows where 1 gene is above zscore\n",
    "# find orthogonal features of 7000 element matrix\n",
    "# cant do that without iteration to find nonlinear statistical relationship occurances\n",
    "# remove noise\n",
    "# find where columns overlap \n",
    "# attempt to cluster in batches of 100\n",
    "# happiest bear in the world - (infinite thank you + apologetic) 4ever\n",
    "# dont affect stream negatively - always think of effect on others \n",
    "# dont take any breaks\n",
    "# finsih this problem by noon - continue everyday forever for 100 years\n",
    "# https://zenodo.org/record/6546964\n",
    "# classify how different perturbations contain different profiles of information distance\n",
    "\n",
    "#many people just want a cool matrix transform that encodes probabilitiy from gene expression matrix\n",
    "# we dont want dimensionality reduction\n",
    "#most people just want it to work asap and dont care \n",
    "#eggnog wants to be silent typist and wants everyone in stream to be happy - also finish by noon \n",
    "#timebox - what can be solved in 2-12?\n",
    "#go to austin tomorrow - chill w/ computer\n",
    "\n",
    "#5000 x 2000 = too slow for python\n",
    "#other datasets 100x more data\n",
    "\n",
    "#determine what causes the variation in the gene expression profile\n",
    "#which gene contribute most magnitude in amplitude\n",
    "#filter data that isnt relevant \n",
    "#whats left is is just the perturbations (depending on experiment intent and data)\n",
    "#https://proceedings.mlr.press/v108/zhao20c/zhao20c.pdf\n",
    "\n",
    "\n",
    "# how would 1 billion people message each other simultaneously - telepathy may in fact solve this -> qualia->save to to disk\n",
    "# one community\n",
    "# 3-5 topics a day\n",
    "# topics stay relevant as long ast they get votes\n",
    "# pass\n",
    "# add comment threads - no firebase\n",
    "# how would you get traction -> have to send it to right people -> offer tons of free shit \n",
    "# good security - traceless \n",
    "# inner loop - how discussion changes -> what is the consensus in the community about how to solve a problem?? \n",
    "# 3-5 topics -> each has 3-5 top threads\n",
    "# make it fun somehow https://freefrontend.com/css-carousels/ -> filter lots of stuff  \n",
    "\n",
    "# use LLM to merge comments -> already been said - might be fun part -> train on whether dicussion makes someone happy\n",
    "# hover to upvote\n",
    "# what do people want to discuss? \n",
    "\n",
    "\n",
    "#be ready to make 2-3 demos -> 2-3 days see which will stick\n",
    "#media server search by text\n",
    "#cooperation.party -> one conversation -> rolling window \n",
    "#https://threejs.org/examples/#webgl_modifier_curve_instanced\n",
    "#use LLM very well -> have __very__ __good__ discussions - music\n",
    "#table this \n",
    "\n",
    "# create adventures to do together -> add plot or agenda\n",
    "# #0 rule is something actually cool\n",
    "# Expedience is #2 rule\n",
    "\n",
    "# figure out something we want to figure out\n",
    "\n",
    "# think of problems that need solving\n",
    "# think of solutions that can be improved\n",
    "# build a community - like ours - people have to know who you are in a community "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5538ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://foresight.org/summary/simon-durr-designing-stable-metalloproteins-using-deep-learning/\n",
    "#https://www.youtube.com/watch?v=_gXiVOmaVSo&t=1028s&ab_channel=ForesightInstitute\n",
    "#ghost in the shell episode 13 season 2\n",
    "#https://www.youtube.com/watch?v=Nbmnx0hTPjA&ab_channel=DartmouthEngineering\n",
    "#http://book.bionumbers.org/wp-content/uploads/2020/04/SARS-CoV-2_BTN_0401.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3f516918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install cellrank\n",
    "#! cellblast\n",
    "\n",
    "#https://scverse.org/packages/#core-packages\n",
    "#https://github.com/aristoteleo/dynamo-release\n",
    "\n",
    "#genome sequence corn and algae - select\n",
    "#design crispr guide RNA for them\n",
    "#???\n",
    "#predict perturbations -> \"this edit may cause this yy\"\n",
    "#visualize molecular response\n",
    "#\n",
    "#send specs to lab and get kernels back\n",
    "#use new plant \n",
    "#https://github.com/KANG-BIOINFO/CellDrift\n",
    "#https://academic.oup.com/bib/article/23/5/bbac324/6673850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cf8114ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.ouseful.info/2021/09/30/a-simple-pattern-for-embedding-third-party-javascript-generated-graphics-in-jupyter-notebools/\n",
    "#https://www.biorxiv.org/content/10.1101/2022.07.20.500854v1\n",
    "#https://pubmed.ncbi.nlm.nih.gov/35625556/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
