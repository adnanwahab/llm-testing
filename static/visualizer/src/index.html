<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>33 - Post-processing</title>
    <link rel="stylesheet" href="./style.css">
    <script src="https://cdn.tailwindcss.com"></script> 

</head>
<body>

 

    <h3 class="lyrics"></h3>


    <div class="lyric-match"></div>
 

    <label>Youtube Downloader</label>
    
    
    <input class="yt">

  
    <br>
   

    <button type="button" class="rounded-md bg-indigo-500 px-3.5 py-2.5 text-sm font-semibold text-white shadow-sm hover:bg-indigo-400 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-indigo-500">
        click here to play music
    </button>


    <auto-complete src="/users/search" for="users-popup">
        <input type="text" name="users">
        <!--
          Optional clear button:
          - id must match the id of the input or the name of the input plus "-clear"
          - recommended to be *before* UL elements to avoid conflicting with their blur logic
      
        -->
        <button id="users-clear">X</button>
        <ul id="users-popup"></ul>
        <!--
          Optional div for screen reader feedback. Note the ID matches the ul, but with -feedback appended.
          Recommended: Use a "Screen Reader Only" class to position the element off the visual boundary of the page.
        -->
        <div id="users-popup-feedback" class="sr-only"></div>
      </auto-complete>

    <audio controls>
      <source src="/dance.mp3" type="audio/mpeg">
      Your browser does not support the audio tag.
  </audio>
<div class="recognizedText"></div>

    <canvas class="webgl" style="display:none;"></canvas>
    <script type="module" src="./script.js"></script>
    <script type="module">


        let lyrics = []
        let speech = []



    import Fuse from 'https://cdn.jsdelivr.net/npm/fuse.js@6.6.2/dist/fuse.esm.js'
    import '@github/auto-complete-element'

    import * as d3 from "https://cdn.jsdelivr.net/npm/d3@7/+esm";
    
    
    import _, { map } from 'https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.13.6/underscore-esm-min.js';
        document.querySelector('button').addEventListener('click', async (e) => {
          document.querySelector('audio').play()
          let req = await fetch('/api/v1', {headers: {'Content-Type': 'application/json'}});
          console.log(req);
          let json = await req.json();
          let lyrics = Object.values(json)
          let timestamps = Object.keys(json).map(str => str.split('-->')[0].trim().slice(1))
          console.log(timestamps)
          let now = Date.now()
          let i = 0
          //00:03.980
      
            requestAnimationFrame(function recur() {
              let elapsed = (Date.now() - now) / 1000
              //console.log(elapsed)
              let ts = timestamps.find(ts => getSeconds(ts) > elapsed)
              let i = timestamps.indexOf(ts) - 1
              changeLyrics(i, lyrics)
              requestAnimationFrame(recur)
              lyrics.push(lyrics[i])
            //   document.querySelector('.lyric-match').textContent = 
            //   _.zip(lyrics, speech)
            })
          })
    
        function changeLyrics(i, lyrics) {
          document.querySelector('.lyrics').textContent = lyrics[i]
        }
    
        function getSeconds (timeStr) {
          let parts = timeStr.split(":");
          let minutes = parseInt(parts[0]);
          let seconds = parseFloat(parts[1]);
          let totalSeconds = minutes * 60 + seconds;
          return totalSeconds
        }
    
        //lyric list 
        //speech List
        //compare the length and if the length of tokens is similar, highlight words differently


        const completer = document.querySelector('auto-complete')
const container = completer.parentElement
completer.addEventListener('loadstart', () => container.classList.add('is-loading'))
completer.addEventListener('loadend', () => container.classList.remove('is-loading'))
completer.addEventListener('load', () => container.classList.add('is-success'))
completer.addEventListener('error', () => container.classList.add('is-error'))
  

completer.addEventListener('auto-complete-change', function(event) {
  console.log('Auto-completed value chosen or cleared', completer.value)
  console.log('Related input element', event.relatedTarget)
})

    
        function getYT2(shit) {
            let req =  fetch('//localhost:3000/youtube-search', {
              headers: {'Content-Type': 'application/json'},
            body: JSON.stringify(shit),
            method: 'POST',
          }).then(req => req.json()).then(json => 
          
          {console.log(json)
            //$("#airportPicker").fuzzyComplete(json.items.map());
    
    
          })
    
        }
    
        let getYT = _.debounce(getYT2, 1000)
    
        document.querySelector('.yt').addEventListener('change', function(event) {
          let shit = event.target.value
    
      
    getYT(shit)
    })
      
    
    const recognizedText = document.querySelector('.recognizedText');

    
        const grammar =
      "#JSGF V1.0; grammar colors; public <color> = aqua | azure | beige | bisque | black | blue | brown | chocolate | coral | crimson | cyan | fuchsia | ghostwhite | gold | goldenrod | gray | green | indigo | ivory | khaki | lavender | lime | linen | magenta | maroon | moccasin | navy | olive | orange | orchid | peru | pink | plum | purple | red | salmon | sienna | silver | snow | tan | teal | thistle | tomato | turquoise | violet | white | yellow ;";
    const recognition = new webkitSpeechRecognition();
    const speechRecognitionList = new webkitSpeechGrammarList();
    speechRecognitionList.addFromString(grammar, 1);
    recognition.grammars = speechRecognitionList;
    recognition.continuous = true;
    recognition.lang = "en-US";
    recognition.interimResults = true;
    recognition.maxAlternatives = 1;
    
    const diagnostic = document.querySelector(".output");
    const bg = document.querySelector("html");
    
    
      recognition.start();
      console.log("Ready to receive a color command.");
    
    
      let count = 0
    recognition.onresult = (event) => {
         
        const transcript = event.results[event.results.length - 1][0].transcript;
      recognizedText.textContent = transcript;
      speech.push(transcript)
      //console.log(event.results)
    };
    
      </script>
</body>
</html>