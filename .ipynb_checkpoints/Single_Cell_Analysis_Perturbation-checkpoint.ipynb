{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cacc042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ls ./data_sets/* -lh\n",
    "#https://github.com/chriswi93/Neural-Networks-and-Logistic-Regression-Backpropagation-in-depth\n",
    "\n",
    "# ![Alt text](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-021-22197-x/MediaObjects/41467_2021_22197_Fig3_HTML.png?as=webp)\n",
    "\n",
    "# https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bib/22/4/10.1093_bib_bbaa268/1/m_bbaa268f1.jpeg?Expires=1695201196&Signature=1KEY92u4ZstK959i3C6haCKHZ7-6ghmNkBQwGELax4hVBn6N0o7lasyTNgnHk6sQ6eP2yiV~E51~X8JdkQkF9D5PfM7pk0N-z1rOF1HJpYaNBZ7IrUSqzdj-lQHw-TTBMjlW8rFKnSWg8~Y0y2y7q7a1hGweo3LHFNk7pSxu0kgYUaN54HwRrCWvpuMe0Eq~PL4oIh857EOSI9YaYyZ4U3ilKNy9bzbEHrLUiGOdfBBvJV09gq5g1Xp3rl49KqxwnpaFVs1qEj0z94TBYtJMDnUXEoV8ZXGJ2ESWxaXQRGziXBHA-b5l2Ac40c2eSVvTgqGFK2ClL0yGFZM5J458dg__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3818fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cancer sample matrix was normalized by the Z-score method, \n",
    "#which scaled the mean of each row (corresponding to feature edge) to zero and variance to one. \n",
    "#First, the rows of the matrix were clustered using hierarchical clustering based on the complete linkage method with the cluster number set to 100, \n",
    "#and clusters containing more than 30 edges were retained.\n",
    "#We then computed the mean values of perturbation for each edge in each subtype through Z-scores.\n",
    "#For each subtype, we counted the percentage of edges whose absolute value of the average perturbation was greater than 0.5 in each retained cluster. \n",
    "#A cluster with a percentage greater than 70% was regarded as a perturbed cluster for this subtype. \n",
    "#All edges in all of the perturbed clusters for each subtype constituted the subtype-specific networks.\n",
    "#All genes involved in each subtype-specific network were used for pathway enrichment analysis by Metascape (http://metascape.org). \n",
    "#The KEGG and Reactome pathways with a P-value less than 0.01 were retained. \n",
    "#Finally, the subtype-specific pathways were identified.\n",
    "#grouping based on shared genes\n",
    "#network = nodes = cell\n",
    "#edges = shared gene expression above mean -> only retain those above 30 \n",
    "#graeter than > .5 of the zscore\n",
    "#a cluster with a percentage greater than ??? (look at ribosomes)\n",
    "# https://metascape.org/blog/\n",
    "# ##   *\n",
    "# #   /_\\\n",
    "# #  (@@)\n",
    "# #---T----\n",
    "# #  /\\\n",
    "# #_|  \\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3609c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncounts       ctrl =    7085.9976    pert =  7251.1616\n",
      "ngenes       ctrl =    1460.5028512358208    pert =  1472.5600646518021\n",
      "percent_mito       ctrl =    2.642741    pert =  2.799472\n",
      "percent_ribo       ctrl =    3.813907    pert =  3.8422022\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix, find\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas \n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "import torch, numpy as np, pandas as pd\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "rowGeneExpression2 = defaultdict(dict)\n",
    "import math\n",
    "import torch\n",
    "pandas.set_option('mode.use_inf_as_na', True)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "# This is required to catch warnings when the multiprocessing module is used\n",
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "import scanpy as sc\n",
    "from torch import tensor\n",
    "import torch, numpy as np, pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch import tensor\n",
    "import torch, numpy as np, pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "from fastai.data.transforms import RandomSplitter\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix, tril\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "from fastprogress.fastprogress import master_bar \n",
    "np.set_printoptions(linewidth=140)\n",
    "torch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\n",
    "pd.set_option('display.width', 140)\n",
    "#one ='DatlingerBock2021.h5ad'\n",
    "#one = 'AissaBenevolenskaya2021.h5ad'\n",
    "#one = 'AissaBenevolenskaya2021.h5ad'\n",
    "folders = '/home/awahab/llm-testing/data_sets/'\n",
    "#one = 'AdamsonWeissman2016_GSM2406675_10X001.h5ad' #sigmoid returns nan in 0th frame\n",
    "one ='DatlingerBock2017.h5ad'\n",
    "one = 'AissaBenevolenskaya2021.h5ad'\n",
    "one = 'SrivatsanTrapnell2020_sciplex2.h5ad'\n",
    "one ='DatlingerBock2017.h5ad'\n",
    "#one = 'AdamsonWeissman2016_GSM2406675_10X001.h5ad'\n",
    "#one = 'AissaBenevolenskaya2021.h5ad'\n",
    "#one = 'XieHon2017.h5ad'\n",
    "#one = 'SrivatsanTrapnell2020_sciplex2.h5ad'\n",
    "def readFiles():\n",
    "    adata = sc.read_h5ad(folders + one)\n",
    "    one ='DatlingerBock2017.h5ad'\n",
    "    return adata\n",
    "adata = sc.read_h5ad(folders + one)\n",
    "#2. Non-negative matrix factorization (NMF)\n",
    "#3. Linear discriminant analysis (LDA)\n",
    "#adata.obs\n",
    "sc.pp.log1p(adata)\n",
    "#sc.pp.highly_variable_genes(adata)\n",
    "sc.pp.highly_variable_genes(adata, \n",
    "                                layer=None, \n",
    "                                n_top_genes=200, \n",
    "                                min_disp=0.5, \n",
    "                                max_disp=1, \n",
    "                                min_mean=0.0125, \n",
    "                                max_mean=3, \n",
    "                                span=0.3, \n",
    "                                n_bins=20, \n",
    "                                flavor='seurat_v3', \n",
    "                                subset=False, \n",
    "                                inplace=True, \n",
    "                                batch_key=None, \n",
    "                                check_values=True)\n",
    "\n",
    "sc.pp.pca(adata)\n",
    "found = find(adata.X)\n",
    "torch.manual_seed(440)\n",
    "#adata.obs.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n",
    "#adata.obs = adata.iloc[:5000]\n",
    "#adata.obs= adata.obs[adata.obs.iloc[:5000]]\n",
    "#adata.obs.iloc[:5000]\n",
    "#adata.var_names\n",
    "var_df = adata.var\n",
    "df = adata.obs#.iloc[:5000]\n",
    "df = df.drop(columns=['nperts'])\n",
    "df['percent_mito'] = 1\n",
    "def getMode(l): \n",
    "    return max(set(l), key=l.count)\n",
    "#sc.pp.filter_cells(adata, min_counts=None, min_genes=None, max_counts=None, max_genes=10, inplace=True, copy=False)\n",
    "#sc.pp.filter_genes(adata, min_counts=None, min_cells=None, max_counts=None, max_cells=None, inplace=True, copy=False)\n",
    "#sc.pp.highly_variable_genes(adata, layer=None, n_top_genes=None, min_disp=0.5, max_disp=inf, min_mean=0.0125, max_mean=3, span=0.3, n_bins=20, flavor='seurat', subset=False, inplace=True, batch_key=None, check_values=True)\n",
    "#sc.pp.regress_out(adata, keys, n_jobs=None, copy=False)\n",
    "#cell perturbation is defined as molecular response or gene expression that is different to what is \"normal\"\n",
    "from IPython.display import IFrame\n",
    "# check for expression values that are equal from crispr\n",
    "#join with gene ontology\n",
    "#this is a program\n",
    "#input an adata file\n",
    "#outputs a list of cell-IDs and the genes perturbed \n",
    "#and then what that gene does \n",
    "#and what interactions may occur with those perturbations \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder1 = LabelEncoder()\n",
    "label_encoder2 = LabelEncoder()\n",
    "label_encoder3 = LabelEncoder()\n",
    "#df['chembl-ID'] = label_encoder1.fit_transform(df['chembl-ID'])\n",
    "df['perturbation_2'] = label_encoder1.fit_transform(df['perturbation_2'])\n",
    "df['target_2'] = label_encoder2.fit_transform(df['target'])\n",
    "cool_columns = 'ncounts ngenes percent_mito percent_ribo'.split(' ')\n",
    "for key in cool_columns:\n",
    "    ct = adata.obs[adata.obs['perturbation'] == 'control'][key].std()\n",
    "    pt = adata.obs[adata.obs['perturbation'] != 'control'][key].std()\n",
    "    print(key, '      ctrl =   ', ct, '   pert = ', pt)\n",
    "#IFrame('https://www.shadertoy.com/embed/dlScDy?gui=true&t=10&paused=true&muted=false', width=700, height=350)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67056872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A1BG'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.var_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45167ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4585"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import tensor\n",
    "import torch, numpy as np, pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "from fastai.data.transforms import RandomSplitter\n",
    "from collections import defaultdict\n",
    "\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_columns] = df[numerical_columns].fillna(0)\n",
    "numerical_values = df.select_dtypes(include=[int, float]).values.tolist()\n",
    "numerical_values\n",
    "rowGeneExpression = defaultdict(int)\n",
    "hv_genes = set(list(var_df[var_df['highly_variable'] == True].index))\n",
    "normal_genes = (list(adata.var_names))\n",
    "high_variance_columns = set([ i for i,val in enumerate(normal_genes) if val in hv_genes ])\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_columns] = df[numerical_columns].fillna(0)\n",
    "sums = []\n",
    "column_averages = defaultdict(list)\n",
    "rowGeneExpression = defaultdict(int)\n",
    "rows, columns, vals = found\n",
    "high_variance = set(high_variance_columns)\n",
    "row_id = 0\n",
    "control_variables = set(['ctrl', 'control', '*'])\n",
    "dependent_variables = list(df['perturbation'].map(lambda val: 0 if val in control_variables else 1).values)\n",
    "geneValues = defaultdict(int)\n",
    "columnMode = defaultdict(list)\n",
    "geneAverages = defaultdict(int)\n",
    "geneOccurences = defaultdict(int)\n",
    "geneVariance = defaultdict(list)\n",
    "cell_variance_score = defaultdict(int)\n",
    "\n",
    "row_variance = [] \n",
    "c,g,v = found\n",
    "\n",
    "cell_variance_score = {}\n",
    "for i in range(df.shape[0]): cell_variance_score[i]= 0\n",
    "\n",
    "for cell,gene,val in zip(c,g,v):\n",
    "    if gene not in high_variance_columns: continue\n",
    "    geneValues[gene] += val\n",
    "    geneOccurences[gene] += 1\n",
    "    columnMode[gene].append(val)\n",
    "    \n",
    "for k in dict(geneValues):\n",
    "    geneAverages[k] =  geneValues[k] / geneOccurences[k]\n",
    "    \n",
    "for k in dict(geneValues): columnMode[k] = getMode(columnMode[k])\n",
    "    \n",
    "for cell, gene, val in zip(c,g,v):\n",
    "    if gene not in high_variance_columns: continue\n",
    "    geneVariance[gene].append(abs(val - geneAverages[gene]))# ** 2\n",
    "    \n",
    "    \n",
    "for k in dict(geneAverages):  \n",
    "    geneVariance[k] = max(set(geneVariance[k]), key=geneVariance[k].count)\n",
    "\n",
    "geneModes = defaultdict(list)\n",
    "\n",
    "for cell, gene, val in zip(c,g,v):\n",
    "    if gene not in high_variance_columns: continue\n",
    "    geneModes[gene].append(abs(val))# ** 2\n",
    "\n",
    "for val in geneModes: geneModes[val] = max(set(geneModes[val]), key=geneModes[val].count)\n",
    "\n",
    "num_cells = len(df.select_dtypes(include=[int, float]).values.tolist())\n",
    "    \n",
    "mini_cell_var = defaultdict(list)\n",
    "for cell, gene, val in zip(c,g,v):\n",
    "    if gene not in high_variance_columns: continue\n",
    "    columnColor = geneAverages[gene]\n",
    "    cellColorForGene = val\n",
    "    threshold = columnColor\n",
    "    if (cellColorForGene - columnColor) < 0:\n",
    "        mini_cell_var[cell].append(cellColorForGene - columnColor)\n",
    "        cell_variance_score[cell] += abs(cellColorForGene - columnColor)\n",
    "      \n",
    "for key in mini_cell_var: mini_cell_var[key] = max(mini_cell_var[key])\n",
    "        \n",
    "df['geneVarianceScore'] = cell_variance_score.values()\n",
    "numerical_values = df.select_dtypes(include=[int, float]).values.tolist()\n",
    "# for k,vi in enumerate(numerical_values):\n",
    "#     x = math.ceil((i / 5904) * 50)\n",
    "#     numerical_values[k] += adata.uns['pca']['variance_ratio'][x -1]\n",
    "    \n",
    "independent_variables = pd.DataFrame(numerical_values)\n",
    "\n",
    "vals += .01\n",
    "t_dep = tensor([float(i) for i in dependent_variables]) # pertrubations\n",
    "t_indep = tensor(numerical_values, dtype=torch.float)\n",
    "\n",
    "n_coeff = t_indep.shape[1]\n",
    "\n",
    "vals,indices = t_indep.max(dim=0)\n",
    "t_indep = t_indep / vals\n",
    "trn_split,val_split=RandomSplitter(seed=42)(independent_variables)\n",
    "\n",
    "trn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\n",
    "trn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\n",
    "\n",
    "indep_cols =  df.select_dtypes(include=[int, float]).columns.tolist()\n",
    "indep_cols\n",
    "\n",
    "len([item for item in list(t_dep) if item.item() == 0])\n",
    "len([item for item in list(t_dep) if item.item() > .5]) \n",
    "\n",
    "#len(numerical_values)\n",
    "#len([item for item in list(t_dep) if item.item() > -1])\n",
    "#len([item for item in list(t_dep) if item.item() == 1]) / len([item for item in list(t_dep) if item.item() > -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8dd3dedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct722, total_guess943, perb_total 4585, accuracy 0.7656415694591728\n",
      "precision 0.20567066521264996\n"
     ]
    }
   ],
   "source": [
    "cell_variance_score= defaultdict(int)\n",
    "for cell, gene, val in zip(c,g,v):\n",
    "    if gene not in high_variance_columns: continue\n",
    "    columnColor = geneAverages[gene]\n",
    "    cellColorForGene = val\n",
    "    threshold = columnColor\n",
    "    if abs(cellColorForGene) > columnColor and columnColor < 1:\n",
    "        #mini_cell_var[cell].append(cellColorForGene - columnColor)\n",
    "        cell_variance_score[cell] += abs(cellColorForGene - columnColor)\n",
    "l = cell_variance_score.values()   \n",
    "avg = sum(l) / len(l)\n",
    "avg = 0\n",
    "import random\n",
    "cvs = cell_variance_score.values()\n",
    "mini_cell_var.values()\n",
    "total_guess = len([item for key, item in enumerate(cvs) if item > avg])\n",
    "correct_guess = len([item for key, item in enumerate(cvs) if item > avg and dependent_variables[key] == 1])\n",
    "perb_total =  len([item for key, item in enumerate(dependent_variables) if dependent_variables[key] == 1])\n",
    "print(f'correct{correct_guess}, total_guess{total_guess}, perb_total {perb_total}, accuracy {correct_guess / total_guess}')\n",
    "print(f'precision {total_guess / perb_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c75d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = []\n",
    "# test = defaultdict(int)\n",
    "# for i in high_variance_columns:\n",
    "#     m=adata.X.getcol(i).todense()\n",
    "#     mode = getMode(m.tolist()[0]) \n",
    "#     avg = sum(m.tolist()[0]) / len(m.tolist()[0])\n",
    "#     pert_and_above_zero = len([i for k, i in enumerate(m.tolist()) if i[0] > 0 and dependent_variables[k] > 0])\n",
    "#     not_pert_and_above_zero = len([i for k, i in enumerate(m.tolist()) if i[0] > 0 and dependent_variables[k] < 1])\n",
    "\n",
    "#     above_zero = len([i for k, i in enumerate(m.tolist()) if i[0] > 0])\n",
    "#     eq_zero = len([i for k, i in enumerate(m.tolist()) if i[0] == 0])\n",
    "#     test[i] = above_zero\n",
    " \n",
    "#     cellCounts = 5904\n",
    "#     if (above_zero > 30): continue # 90%\n",
    "\n",
    "#     for key,element in enumerate(m.tolist()):\n",
    "#         if element[0] > 0: count.append(key)\n",
    "    \n",
    "# print(len(set(count)))\n",
    "# count = set(count)\n",
    "# print(len([x for row, x in enumerate(count) if dependent_variables[x] > 0]),len([x for row, x in enumerate(count) if dependent_variables[x] < 1]))\n",
    "# print(len([x for row, x in enumerate(count) if dependent_variables[x] > 0]) / len([x for row, x in enumerate(count)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4eb0b6c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "12156\n"
     ]
    }
   ],
   "source": [
    "category_indices = df.groupby('perturbation').apply(lambda x: x.index.tolist() )\n",
    "most_cells = category_indices[2]\n",
    "\n",
    "most_cell_indices = []\n",
    "for i in most_cells:\n",
    "    most_cell_indices.append(adata.obs.index.get_loc(i))\n",
    "\n",
    "a=most_cell_indices[0]\n",
    "b=most_cell_indices[10]\n",
    "\n",
    "b_matrix = adata.X.getrow(b).todense().tolist()[0]\n",
    "a_matrix = adata.X.getrow(a).todense().tolist()[0]\n",
    "\n",
    "a_matrix\n",
    "print(len(most_cells))\n",
    "sum(a_matrix), sum(b_matrix)\n",
    "count = {}\n",
    "        \n",
    "distance = defaultdict(int)\n",
    "indicesAbove = defaultdict(list)\n",
    "\n",
    "for row in range(5904):\n",
    "    m = adata.X.getrow(row).todense().tolist()[0]\n",
    "    for k in high_variance_columns:\n",
    "        if (geneAverages[k]) < m[k] and m[k] < 100:\n",
    "            distance[k] += m[k]\n",
    "            indicesAbove[row].append(k)\n",
    "            \n",
    "distance_max = max(list(distance.values()))\n",
    "\n",
    "for k in distance:\n",
    "    if distance[k] == distance_max: print(k)\n",
    "#dont take any breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f01653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scipy.stats.zscore(adata.X.getcol(0).todense().tolist())\n",
    "indicesAbove = dict(indicesAbove)\n",
    "# for cell,gene,val in zip(c,g,v):\n",
    "#     if gene not in high_variance_columns: continue\n",
    "#     geneValues[gene] += val\n",
    "#     geneOccurences[gene] += 1\n",
    "#     columnMode[gene].append(val)\n",
    "\n",
    "timesAbove = defaultdict(int)\n",
    "geneAboveMeanOccurances = defaultdict(list)\n",
    "\n",
    "for row in dict(indicesAbove): \n",
    "    for column in indicesAbove[row]: \n",
    "        geneAboveMeanOccurances[column].append(row)\n",
    "        \n",
    "prob_perts = {} \n",
    "\n",
    "filteredGeneCellLists = defaultdict(list)\n",
    "\n",
    "threshold = 30\n",
    "\n",
    "for geneList in geneAboveMeanOccurances:\n",
    "    cellsWithGene = geneAboveMeanOccurances[geneList]\n",
    "    if  threshold < len(cellsWithGene) and len(cellsWithGene) < 100:\n",
    "        filteredGeneCellLists[geneList] = cellsWithGene\n",
    "\n",
    "cellToGeneEmbedding = [[] for i in range(5904)]\n",
    "\n",
    "for column in filteredGeneCellLists:\n",
    "    cellList = filteredGeneCellLists[column]\n",
    "    for cellRow in cellList:\n",
    "        cellToGeneEmbedding[cellRow].append(column)\n",
    "    \n",
    "cellToGeneEmbedding\n",
    "\n",
    "cellCount = 0\n",
    "for cellList in list(filteredGeneCellLists.values()):\n",
    "    cellCount += len(cellList)\n",
    "    \n",
    "totalCells = []\n",
    "for key in (filteredGeneCellLists.keys()):\n",
    "    cellList = filteredGeneCellLists[key]\n",
    "    totalCells += cellList\n",
    "    for cell in cellList:\n",
    "        gene = adata.var.iloc[cell].name\n",
    "        row = df.iloc[cell]\n",
    "        \n",
    "len(set(totalCells))\n",
    "\n",
    "len([item for key, item in enumerate(t_dep) if item.item() > .5 and key in totalCells])\n",
    "total = defaultdict(int)\n",
    "for row in range(500):\n",
    "    total[row] += sum(adata.X.getrow(row).data)\n",
    "avg = sum(list(total.values())) / 500\n",
    "\n",
    "counter = 0\n",
    "for key, item in enumerate(list(total.values())):\n",
    "    if item > avg:\n",
    "        counter += 1\n",
    "        \n",
    "counter\n",
    "count_per_category = df.groupby('perturbation').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a86216ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # http# ! pip install biomart\n",
    "# mito_gene_names = sc.queries.mitochondrial_genes(\"hsapiens\")\n",
    "# mito_ensembl_ids = sc.queries.mitochondrial_genes(\"hsapiens\", attrname=\"ensembl_gene_id\")\n",
    "# mito_gene_names_fly = sc.queries.mitochondrial_genes(\"dmelanogaster\", chromosome=\"mitochondrion_genome\")\n",
    "# import scanpy as sc\n",
    "# sc.queries.enrich(['KLF4', 'PAX5', 'SOX2', 'NANOG'], org=\"hsapiens\")\n",
    "# sc.queries.enrich({'set1':['KLF4', 'PAX5'], 'set2':['SOX2', 'NANOG']}, org=\"hsapiens\")\n",
    "# pbmcs = sc.datasets.pbmc68k_reduced()\n",
    "# sc.tl.rank_genes_groups(pbmcs, \"bulk_labels\")\n",
    "# sc.queries.enrich(pbmcs, \"CD34+\")\n",
    "# # pbmcs\n",
    "# category_indices = df.groupby('perturbation').apply(lambda x: x.index.tolist() )\n",
    "# df.index.get_loc('TACTTGACCCCN')\n",
    "# allRows = defaultdict(int)\n",
    "# categories = df['perturbation'].unique()\n",
    "# for i, group in enumerate(category_indices):\n",
    "#     for row in group:\n",
    "#         allRows[categories[i]] += 1\n",
    "        \n",
    "# df.groupby('perturbation')\n",
    "\n",
    "# categories\n",
    "# len(category_indices)\n",
    "\n",
    "# groupCellCounts = list(allRows.values())\n",
    "\n",
    "# nonZerosInColumn = list(test.values())\n",
    "\n",
    "# for k,v in enumerate(groupCellCounts):\n",
    "#     cellCount = nonZerosInColumn[k]\n",
    "# # from ipywidgets import interact\n",
    "# # trn_xs = [1,2,3,4,5]\n",
    "# # conts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]\n",
    "\n",
    "# # def iscore(nm, split):\n",
    "# #     col = trn_xs[nm]\n",
    "# #     return score(col, trn_y, split)\n",
    "# # interact(nm=conts, split=15.5)(iscore);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "767d5681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2000' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2000/2000 00:07&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.12272727272727273, 0.646237731733915, 162, 2963)\n",
      "(0.1143939393939394, 0.6608505997818975, 151, 3030)\n",
      "(0.27045454545454545, 0.9873500545256271, 357, 4527)\n",
      "(0.2863636363636364, 0.9814612868047983, 378, 4500)\n",
      "(0.25227272727272726, 0.9840785169029443, 333, 4512)\n",
      "(0.22272727272727272, 0.9871319520174482, 294, 4526)\n",
      "(0.19924242424242425, 0.9897491821155944, 263, 4538)\n",
      "(0.1893939393939394, 0.9928026172300981, 250, 4552)\n",
      "(0.22272727272727272, 0.9947655398037077, 294, 4561)\n",
      "(0.24242424242424243, 0.9949836423118866, 320, 4562)\n",
      "(0.25227272727272726, 0.995856052344602, 333, 4566)\n",
      "(0.23712121212121212, 0.9976008724100327, 313, 4574)\n",
      "(0.23636363636363636, 0.9989094874591058, 312, 4580)\n",
      "(0.2689393939393939, 0.9997818974918211, 355, 4584)\n",
      "(0.39015151515151514, 1.0, 515, 4585)\n",
      "(0.49696969696969695, 1.0, 656, 4585)\n",
      "(0.5507575757575758, 0.9997818974918211, 727, 4584)\n",
      "(0.7053030303030303, 0.9993456924754635, 931, 4582)\n",
      "(0.9, 0.9993456924754635, 1188, 4582)\n",
      "(0.9446969696969697, 0.9980370774263904, 1247, 4576)\n",
      "(0.9583333333333334, 0.9960741548527808, 1265, 4567)\n",
      "(0.9651515151515152, 0.9947655398037077, 1274, 4561)\n",
      "(0.9674242424242424, 0.9925845147219193, 1277, 4551)\n",
      "(0.9696969696969697, 0.9912758996728462, 1280, 4545)\n",
      "(0.9719696969696969, 0.9908396946564886, 1283, 4543)\n",
      "(0.9734848484848485, 0.9908396946564886, 1285, 4543)\n",
      "(0.9757575757575757, 0.9906215921483097, 1288, 4542)\n",
      "(0.975, 0.9906215921483097, 1287, 4542)\n",
      "(0.9757575757575757, 0.9906215921483097, 1288, 4542)\n",
      "(0.9795454545454545, 0.9908396946564886, 1293, 4543)\n",
      "(0.9787878787878788, 0.9908396946564886, 1292, 4543)\n",
      "(0.978030303030303, 0.9908396946564886, 1291, 4543)\n",
      "(0.9787878787878788, 0.9910577971646674, 1292, 4544)\n",
      "(0.9795454545454545, 0.9914940021810251, 1293, 4546)\n",
      "(0.9803030303030303, 0.9917121046892039, 1294, 4547)\n",
      "(0.9803030303030303, 0.9919302071973828, 1294, 4548)\n",
      "(0.9810606060606061, 0.9919302071973828, 1295, 4548)\n",
      "(0.9818181818181818, 0.9921483097055616, 1296, 4549)\n",
      "(0.9825757575757575, 0.9921483097055616, 1297, 4549)\n",
      "(0.9825757575757575, 0.9923664122137404, 1297, 4550)\n",
      "(0.9833333333333333, 0.9925845147219193, 1298, 4551)\n"
     ]
    }
   ],
   "source": [
    "dev = 'cuda:0'\n",
    "def test_prediction(test_predictions):\n",
    "    ctrl = test_predictions.sum(1).tolist()[0]\n",
    "    isFalse = len([sum(row) for idx, row in enumerate(test_predictions.tolist()) if sum(row) <= ctrl and t_dep[idx] == 0])\n",
    "    isTrue = len([sum(row) for idx, row in enumerate(test_predictions.tolist()) if sum(row) > ctrl and t_dep[idx] == 1])\n",
    "    allFalse = len([sum(row) for idx, row in enumerate(test_predictions.tolist()) if t_dep[idx] == 0])\n",
    "    allTrue = len([sum(row) for idx, row in enumerate(test_predictions.tolist()) if t_dep[idx] == 1])\n",
    "    return (isFalse / allFalse, isTrue / allTrue, isFalse, isTrue)\n",
    "def plot_loss(l):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    legends = []\n",
    "    plt.plot(l) #orange false ctrl\n",
    "    plt.plot([0, len([i for k,i in enumerate(rowGeneExpression.values()) if dependent_variables[k]])], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "    plt.legend(legends);\n",
    "\n",
    "numerical_values = df.select_dtypes(include=[int, float]).values.tolist()\n",
    "t_indep = torch.Tensor(numerical_values).to(dev)\n",
    "#t_indep = t_indep / vals\n",
    "#λλλλλ.requires_grad_(True)\n",
    "#3 variations, test, t_indep and t_indep+embedding\n",
    "resultant_tensor = t_indep\n",
    "encodedOutput.requires_grad_(True)\n",
    "print(encodedOutput.requires_grad) \n",
    "# resultant_tensor = torch.cat((t_indep.to(dev),encodedOutput.to(dev)), 1)\n",
    "#resultant_tensor = λλλλλ\n",
    "\n",
    "\n",
    "vals,indices = resultant_tensor.max(dim=0)\n",
    "resultant_tensor = resultant_tensor / vals\n",
    "resultant_tensor = resultant_tensor.to(dev)\n",
    "test_indep = torch.tensor([[t_dep[k].item() for i in enumerate(range(resultant_tensor.shape[1]))] for k, i in enumerate(range(resultant_tensor.shape[0]))])\n",
    "\n",
    "dim = resultant_tensor.shape[1]\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(dim,dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(dim,dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(dim, dim),\n",
    "    nn.Sigmoid()\n",
    ").to(dev)\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=.1, \n",
    "    weight_decay=0.01\n",
    ")\n",
    "# model = model.to(device)\n",
    "# input_tensor = input_tensor.to(device)\n",
    "\n",
    "n_iterations = 1000\n",
    "loss_track = []\n",
    "accuracy_track = []\n",
    "no_entropy = []\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "loss_function = torch.nn.BCELoss()\n",
    "def plot_loss_update(epoch, epochs, mb, train_loss, valid_loss):\n",
    "    \"\"\" dynamically print the loss plot during the training/validation loop.\n",
    "        expects epoch to start from 1.\n",
    "    \"\"\"\n",
    "    x = range(1, epoch+1)\n",
    "    y = np.concatenate((train_loss, valid_loss))\n",
    "    graphs = [[x,train_loss], [x,valid_loss]]\n",
    "    x_margin = 0.2\n",
    "    y_margin = 0.05\n",
    "    x_bounds = [1-x_margin, epochs+x_margin]\n",
    "    y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]\n",
    "\n",
    "    mb.update_graph(graphs, x_bounds, y_bounds)\n",
    "mb = master_bar(range(1))\n",
    "def plot_loss_update(epoch, epochs, mb, train_loss, valid_loss):\n",
    "    \"\"\" dynamically print the loss plot during the training/validation loop.\n",
    "        expects epoch to start from 1.\n",
    "    \"\"\"\n",
    "    x = range(1, epoch+1)\n",
    "    y = np.concatenate((train_loss, valid_loss))\n",
    "    graphs = [[x,train_loss], [x,valid_loss]]\n",
    "    x_margin = 0.2\n",
    "    y_margin = 0.05\n",
    "    x_bounds = [1-x_margin, epochs+x_margin]\n",
    "    y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]\n",
    "\n",
    "    mb.update_graph(graphs, x_bounds, y_bounds)\n",
    "for i in mb:    \n",
    "#for j in progress_bar(range(2000), parent=mb):\n",
    "    for j in progress_bar(range(2000)):\n",
    "        loss = loss_function(model(resultant_tensor).sum(1).sigmoid(), t_dep.to(dev))\n",
    "        optimizer.zero_grad()  # 3\n",
    "        loss.backward(retain_graph=True)  # 4\n",
    "        optimizer.step()  # 5\n",
    "        if j == 1 or j % 50 == 0:\n",
    "            test_predictions = model(resultant_tensor)\n",
    "            #print(loss.item(), test_predictions.sum().item() / 8)\n",
    "            print(test_prediction(test_predictions))\n",
    "        loss_track.append(loss.item())\n",
    "        accuracy_track.append(test_predictions.sum().item() / 8)\n",
    "        no_entropy += [test_predictions.sum().item() / 8]\n",
    "#         k = 100 * i + j\n",
    "#         x = np.arange(0, 2*k*np.pi/1000, 0.01)\n",
    "#         y1, y2 = np.cos(x), np.sin(x)\n",
    "#         graphs = [[x,y], [x,y2]]\n",
    "#         x_bounds = [0, 2*np.pi]\n",
    "#         y_bounds = [-1,1]\n",
    "        #mb.update_graph(graphs, x_bounds, y_bounds)\n",
    "        #print(loss_track, accuracy_track)\n",
    "        #plot_loss_update(j, n_iterations, mb, loss_track, accuracy_track)\n",
    "    # emulate validation sub-loop\n",
    "        #for batch in progress_bar(range(2), parent=mb): sleep(0.2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "af4fee16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1257"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(0, 2*k*np.pi/1000, 0.01)\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14911a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_from_scratch(row):\n",
    "    probability = 0\n",
    "    for item in row:\n",
    "        probability += 1\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d025c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Replace NaN values with 0 only in numerical columns\n",
    "df[numerical_columns] = df[numerical_columns].fillna(0)\n",
    "\n",
    "numerical_values = df.select_dtypes(include=[int, float]).values.tolist()\n",
    "numerical_values\n",
    "rowGeneExpression = defaultdict(int)\n",
    "\n",
    "hv_genes = set(list(var_df[var_df['highly_variable'] == True].index))\n",
    "normal_genes = (list(adata.var_names))\n",
    "\n",
    "high_variance_columns = set([ i for i,val in enumerate(normal_genes) if val in hv_genes ])\n",
    "\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Replace NaN values with 0 only in numerical columns\n",
    "df[numerical_columns] = df[numerical_columns].fillna(0)\n",
    "\n",
    "sums = []\n",
    "\n",
    "column_averages = defaultdict(list)\n",
    "rowGeneExpression = defaultdict(int)\n",
    "rows, columns, vals = found\n",
    "high_variance = set(high_variance_columns)\n",
    "row_id = 0\n",
    "\n",
    "embedLayer = []\n",
    "for i in high_variance_columns:\n",
    "        intermediate = []\n",
    "        for i in adata.X.getcol(i).toarray():\n",
    "            intermediate.append(i[0])\n",
    "        embedLayer.append(intermediate)\n",
    "\n",
    "mat_for_embed = np.random.rand(t_dep.shape[0], 200)\n",
    "for key,col in enumerate(list(high_variance_columns)[:200]):\n",
    "    m= adata.X.getcol(col)\n",
    "    m = m.todense().tolist()\n",
    "    for row,val in enumerate(m):\n",
    "        mat_for_embed[row, key] = val[0]\n",
    "\n",
    "a = mat_for_embed\n",
    "plt.imshow(a[:80], cmap='nipy_spectral_r', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b21b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "cont_keys = {}\n",
    "for key in filteredGeneCellLists:\n",
    "    cont_keys[key] = count\n",
    "    count += 1\n",
    "continuousFilteredGeneCellLists = {}\n",
    "for k in list(filteredGeneCellLists.keys()):\n",
    "    continuousFilteredGeneCellLists[cont_keys[k]] = filteredGeneCellLists[k]\n",
    "#continuousFilteredGeneCellLists\n",
    "#cont_keys\n",
    "#len(list(continuousFilteredGeneCellLists.keys()))\n",
    "#cellCountWithinGroup\n",
    "#zscore\n",
    "#continuousFilteredGeneCellLists check\n",
    "# x = cells in group(s) , cellCountWithinGroup\n",
    "# y = genes affected \n",
    "# z = cluster number\n",
    "#for each cell\n",
    "#make a graph -> \n",
    "#negative * negative = positive, \n",
    "#x  cluster \"name\" or index (clusters should change)\n",
    "#y = genes above/below threshold \n",
    "#z = total dist above threshold\n",
    "#convert 200 dimensions to 3\n",
    "cellGroups = [0 for i in list(range(5905))]\n",
    "cellGroupLengths = [0 for i in list(range(5905))]\n",
    "cellDistCounts = [0 for i in list(range(5905))]\n",
    "for column in continuousFilteredGeneCellLists:\n",
    "    for cell in continuousFilteredGeneCellLists[column]:\n",
    "        cellGroups[cell] = column\n",
    "        cellGroupLengths[cell] = len(continuousFilteredGeneCellLists[column])\n",
    "for idx, row in enumerate(mat_for_embed):\n",
    "    for val in row: \n",
    "        cellDistCounts[idx] += val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9725ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "data = torch.Tensor(mat_for_embed)\n",
    "cat_ = [i for i in (cellGroups,\n",
    "cellGroupLengths,\n",
    "cellDistCounts)]\n",
    "\n",
    "mat2 = []\n",
    "for col in cat_:\n",
    "    l = []\n",
    "    for row in col: \n",
    "        l.append(row)\n",
    "    mat2.append(l)\n",
    "am = torch.tensor(mat2)\n",
    "\n",
    "def customGeneMatrixFindPerturbations(M):\n",
    "    m = torch.ones(M.shape[1], M.shape[0])\n",
    "\n",
    "#     m = M @ m\n",
    "#     m = m @ torch.ones(5905, 50)\n",
    "#     m = m @ torch.ones(50, 32).triu()\n",
    "#     m = m @ torch.eye(32, 3).cos()\n",
    "\n",
    "    return am#.float().t()\n",
    "\n",
    "class λλλ(nn.Module):\n",
    "    def __init__(self, idn, edn):\n",
    "        super(λλλ, self).__init__()\n",
    "        self.λ = nn.Sequential(nn.Linear(idn, edn))\n",
    "    def forward(self, x):\n",
    "        return customGeneMatrixFindPerturbations(x)\n",
    "    data = torch.Tensor(mat_for_embed)\n",
    "\n",
    "model = λλλ(data.shape[1], 3)\n",
    "λλλλλ = model(data)\n",
    "λλλλλ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ee761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.ones(200, 500)\n",
    "# m = nn.Conv1d(5905, 5905, 1,  stride=1)\n",
    "# #m2 = nn.Conv1d(200, 5905, 1,  stride=1)\n",
    "# output = m(data)\n",
    "# #output = m2(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae2e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvAutoencoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "#         # Encoder\n",
    "#         self.conv1 = nn.Conv2d(200, 50, 3)\n",
    "#         self.conv2 = nn.Conv2d(50, 32, 3)\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 7)\n",
    "#         #self.conv1 = nn.Conv2d(1, 16, 3, stride=2, padding=1)\n",
    "#         #self.conv2 = nn.Conv2d(16, 32, 3, stride=2, padding=1)\n",
    "#         #self.conv3 = nn.Conv2d(32, 64, 7)\n",
    "        \n",
    "#         # Decoder\n",
    "#         self.deconv1 = nn.ConvTranspose2d(64, 32, 7)\n",
    "#         self.deconv2 = nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1)\n",
    "#         self.deconv3 = nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Encoder\n",
    "#         x = self.relu(self.conv1(x))\n",
    "#         x = self.relu(self.conv2(x))\n",
    "#         x = self.relu(self.conv3(x))\n",
    "#         # Decoder\n",
    "#         #x = self.relu(self.deconv1(x))\n",
    "#         #x = self.relu(self.deconv2(x))\n",
    "#         #x = self.sigmoid(self.deconv3(x))\n",
    "#         return x\n",
    "# from torch import nn, torch\n",
    "# m = nn.Conv1d(5905, 5905, 1,  stride=1)\n",
    "# m2 = nn.Conv1d(5905, 5905, 1,  stride=1)\n",
    "# input = torch.randn(5905, 200)\n",
    "# output = m(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e041ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "a = mat_for_embed\n",
    "plt.imshow(λλλλλ[:3,].detach(), cmap='nipy_spectral_r', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "18c75c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5905, 3])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just get it working - improve it later\n",
    "from torch import nn\n",
    "import torch\n",
    "def conv(ni, nf, ks=3, stride=1, act=True):\n",
    "    res = nn.Conv1d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n",
    "    if act: res = nn.Sequential(res, nn.ReLU())\n",
    "    return res\n",
    "\n",
    "def deconv(ni, nf, ks=3, act=True):\n",
    "    layers = [\n",
    "    #    nn.UpsamplingNearest2d(scale_factor=2),\n",
    "              nn.Conv2d(ni, nf, stride=1, kernel_size=ks, padding=ks//2)\n",
    "    ]\n",
    "    if act: layers.append(nn.ReLU())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "finishDemoBy6 = nn.Sequential(\n",
    "    torch.nn.Linear(200, 200),\n",
    "    conv(5905,5905, 3),       \n",
    "    nn.MaxPool1d(101, stride=1),\n",
    "    conv(5905,5905),\n",
    "    nn.BatchNorm1d(100),\n",
    "    nn.MaxPool1d(51, stride=1),\n",
    "    conv(5905,5905), \n",
    "    nn.MaxPool1d(48, stride=1),\n",
    "    nn.Sigmoid()\n",
    ").to('cuda:0')\n",
    "opt = optim.SGD(finishDemoBy6.parameters(), lr=0.01)\n",
    "loss_function2 = torch.nn.MSELoss()\n",
    "data = torch.Tensor(mat_for_embed).cuda()\n",
    "for i in range(50):\n",
    "    encodedOutput = (finishDemoBy6(data))\n",
    "    #loss = loss_function2(encodedOutput.sum(1), t_dep.cuda())\n",
    "    #loss.backward()\n",
    "    opt.step()\n",
    "#encodedOutput.shape\n",
    "#data.shape\n",
    "data.shape\n",
    "(finishDemoBy6(data)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encodedOutput\n",
    "# tested_output = [encodedOutput[i] for i in range(5905) if t_dep[i] == 1]\n",
    "# len(tested_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "489e50c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find gradient of row\n",
    "#given two rows that belong to same perturbation -> return identical or similar values\n",
    "#given a matrix -> return mx5 vals that can be transformed into a p-val\n",
    "#capture the 'features' that can be used to reconstruct -> molecular response\n",
    "# find the molecular response of each phenotype interaction or simply the gene by itself\n",
    "#gradient ascent -> descent -> find distributions -> sparsify them\n",
    "#ring that captures relevant known info about you and stores it cryptographically \n",
    "#given n rows and a matrix -> return a tuple that can be used to identify rows which belong to a perturbation response\n",
    "#given an expression matrix -> group cells by perturbation profiles\n",
    "#transcriptomics, genomics, proteinomics, metabolomics\n",
    "#recorded actions -> comic generator\n",
    "#script -> comic generator\n",
    "#comic -> animation generator\n",
    "#$https://www.youtube.com/watch?v=DzNmUNvnB04\n",
    "#plot the matrix before + after - 200x6k to 3x6k -> bright colors for rows with perturbations \n",
    "#perturbations defined as belonging to a group of rows that have multiple columns that are covarying from mean-zscore\n",
    "#makeCoolStuff = [[float(k) for k in range(5905)] for i in range(200)]\n",
    "#https://explained.ai/regularization/index.html\n",
    "#oft constraint with non-regularized loss function (blue-red) term and penalty term (orange).\n",
    "#invent a new architecture \n",
    "#that captures probability of perturbation across a matrix\n",
    "#https://www.10xgenomics.com/resources/datasets/5-k-a-549-lung-carcinoma-cells-no-treatment-transduced-with-a-crispr-pool-3-1-standard-6-0-0\n",
    "# all_url = [\n",
    "# #     \"https://zenodo.org/record/7416068/files/AdamsonWeissman2016_GSM2406675_10X001.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/AdamsonWeissman2016_GSM2406677_10X005.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/AdamsonWeissman2016_GSM2406681_10X010.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/AissaBenevolenskaya2021.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/ChangYe2021.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/DatlingerBock2017.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/DatlingerBock2021.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/DixitRegev2016.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/FrangiehIzar2021_protein.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/FrangiehIzar2021_RNA.h5ad?download=1\",\n",
    "# #     \"https://zenodo.org/record/7416068/files/GasperiniShendure2019_atscale.h5ad?download=1\",\n",
    "    \n",
    "#     \"https://zenodo.org/record/7416068/files/GasperiniShendure2019_highMOI.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/GasperiniShendure2019_lowMOI.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/GehringPachter2019.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/McFarlandTsherniak2020.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/NormanWeissman2019_filtered.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/PapalexiSatija2021_eccite_arrayed_protein.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/PapalexiSatija2021_eccite_arrayed_RNA.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/PapalexiSatija2021_eccite_protein.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/PapalexiSatija2021_eccite_RNA.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/ReplogleWeissman2022_K562_essential.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/ReplogleWeissman2022_K562_gwps.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/ReplogleWeissman2022_rpe1.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SchiebingerLander2019_GSE106340.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SchiebingerLander2019_GSE115943.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SchraivogelSteinmetz2020_TAP_SCREEN__chromosome_11_screen.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SchraivogelSteinmetz2020_TAP_SCREEN__chromosome_8_screen.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/ShifrutMarson2018.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SrivatsanTrapnell2020_sciplex2.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SrivatsanTrapnell2020_sciplex3.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/SrivatsanTrapnell2020_sciplex4.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/TianKampmann2019_day7neuron.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/TianKampmann2019_iPSC.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/TianKampmann2021_CRISPRa.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/TianKampmann2021_CRISPRi.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/WeinrebKlein2020.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/XieHon2017.h5ad?download=1\",\n",
    "#     \"https://zenodo.org/record/7416068/files/ZhaoSims2021.h5ad?download=1\"\n",
    "# ]\n",
    "#scarches.dataset.remove_sparsity(adata)\n",
    "#https://docs.scarches.org/en/latest/api/models.html\n",
    "# mdata = muon.read_10x_h5(\"pbmc_10k_protein_v3_filtered_feature_bc_matrix.h5\")\n",
    "# scvi.model.TOTALVI.setup_mudata(mdata, modalities={\"rna_layer\": \"rna\": \"protein_layer\": \"prot\"})\n",
    "# vae = scvi.model.TOTALVI(mdata)\n",
    "#https://docs.scvi-tools.org/en/stable/api/reference/scvi.module.LDVAE.html\n",
    "#[i for i in test_predictions.tolist() if i < 1]\n",
    "# Regularization in Logistic Regression\n",
    "# Regularization is extremely important in logistic regression modeling. Without regularization, the asymptotic nature of logistic regression would keep driving loss towards 0 in high dimensions. Consequently, most logistic regression models use one of the following two strategies to dampen model complexity:\n",
    "# L2 regularization.\n",
    "# Early stopping, that is, limiting the number of training steps or the learning rate.\n",
    "# (We'll discuss a third strategy—L1 regularization—in a later module.)\n",
    "# Imagine that you assign a unique id to each example, and map each id to its own feature. If you don't specify a regularization function, the model will become completely overfit. That's because the model would try to drive loss to zero on all examples and never get there, driving the weights for each indicator feature to +infinity or -infinity. This can happen in high dimensional data with feature crosses, when there’s a huge mass of rare crosses that happen only on one example each.\n",
    "# Fortunately, using L2 or early stopping will prevent this problem.\n",
    "#[ x for x in [iden(sum(item), 10)  for item in test_predictions.tolist()] if x > .1]\n",
    "#plot(loss_track)\n",
    "def plot_loss(l):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    legends = []\n",
    "#     blue = [i for k,i in enumerate(rowGeneExpression.values()) if dependent_variables[k]]\n",
    "#     oj =[i for k,i in enumerate(rowGeneExpression.values()) if not dependent_variables[k]]\n",
    "#     blue.sort()\n",
    "#     oj.sort()\n",
    "#     plt.plot((blue)) #blue true peturbation \n",
    "    plt.plot(l) #orange false ctrl\n",
    "    #legends.append('param %d' % i)\n",
    "    plt.plot([0, len([i for k,i in enumerate(rowGeneExpression.values()) if dependent_variables[k]])], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "    plt.legend(legends);\n",
    "# #https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02021-3\n",
    "# # Medicine Finding anomalies in radiology images, including CT, MRI, and X-ray images; counting features in pathology slides; measuring features in ultrasounds; diagnosing diabetic retinopathy\n",
    "# #Biology Folding proteins; classifying proteins; many genomics tasks, such as tumor-normal sequencing and classifying clinically actionable genetic mutations; cell classification; analyzing protein/protein interactions\n",
    "# #Other applications Financial and logistical forecasting, text to speech, and much more…\n",
    "# # humor analysis - larry david vs seinfeld ? \n",
    "#https://www.kaggle.com/code/jhoward/why-you-should-use-a-framework\n",
    "#handle \"values outside of domain\" by \"SVM\"\n",
    "#random forest classifier\n",
    "#logisitc regression - hard to get right\n",
    "#correct transformations, outlier handling, correct interactions\n",
    "#os.listdir('./data_sets')\n",
    "#wget -m http://www.example.com 2>&1 | grep '^--' | awk '{ print $3 }' | grep -v '\\.\\(css\\|js\\|png\\|gif\\|jpg\\|JPG\\)$' > urls.txt\n",
    "#https://academic.oup.com/bib/article/22/4/bbaa268/5943793\n",
    "#plot(loss_track)\n",
    "#https://terrytao.files.wordpress.com/2011/02/matrix-book.pdf\n",
    "#https://academic.oup.com/bioinformatics/article/36/Supplement_2/i610/6055927?login=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3b693a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #sc.pl.StackedViolin(adata, , groupby='', use_raw=None, log=False, num_categories=7, categories_order=None, title=None, figsize=None, gene_symbols=None, var_group_positions=None, var_group_labels=None, var_group_rotation=None, layer=None, standard_scale=None, ax=None, vmin=None, vmax=None, vcenter=None, norm=None)\n",
    "# sc.pl.StackedViolin(adata, list(hv_genes), groupby='perturbation', dendrogram=True).show()\n",
    "# hg = list(hv_genes)[100:]\n",
    "# sc.pl.DotPlot(adata, hg,  groupby='perturbation').show()\n",
    "# sc.pl.MatrixPlot(adata, hg, groupby='perturbation').show()\n",
    "# first = adata.X.A[:100]\n",
    "# second = adata.X.T.A[:100]\n",
    "# perturbations = []\n",
    "# for key, row in enumerate(first):\n",
    "#     trackPerts = []\n",
    "#     for column in row:\n",
    "#         if column > 0: trackPerts.append(column)\n",
    "#     print(t_dep[key].item(), len(trackPerts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fdb8099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_demo(data): return data.squeeze(0)\n",
    "#https://datahacker.rs/003-gans-autoencoder-implemented-with-pytorch/\n",
    "#https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/\n",
    "#https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf\n",
    "#https://www.cs.utoronto.ca/~hinton/absps/cogscibm.pdf\n",
    "#Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\n",
    "# Pierre-Antoine Manzagol. 2008. Extracting and\n",
    "# composing robust features with denoising autoencoders. In Proceedings of the 25th international\n",
    "# conference on Machine learning, pages 1096–1103.\n",
    "# ACM.\n",
    "#https://github.com/fastai/course22p2/blob/master/nbs/08_autoencoder.ipynb\n",
    "#file:///Users/adnanwahab/Downloads/Molecular%20Systems%20Biology%20-%202016%20-%20Angermueller.pdf\n",
    "#https://www.cell.com/patterns/pdf/S2666-3899(21)00001-5.pdf\n",
    "#https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bfaf76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
