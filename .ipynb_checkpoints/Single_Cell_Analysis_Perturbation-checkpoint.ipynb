{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 987,
   "id": "8d4fddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas \n",
    "pandas.set_option('mode.use_inf_as_na', True)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# This is required to catch warnings when the multiprocessing module is used\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "# import pertpy as pt\n",
    "import scanpy as sc\n",
    "\n",
    "import pertpy as pt\n",
    "adata = pt.dt.kang_2018()\n",
    "from torch import tensor\n",
    "import torch, numpy as np, pandas as pd\n",
    "np.set_printoptions(linewidth=140)\n",
    "torch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\n",
    "pd.set_option('display.width', 140)\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "id": "ce738036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: adata.X seems to be already log-transformed.\n"
     ]
    }
   ],
   "source": [
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata)\n",
    "\n",
    "#hv_genes = filtered_keys = df[df['column_name'] == True].index.tolist()\n",
    "\n",
    "#normal_genes = filtered_keys = df[df['column_name'] == False].index.tolist()\n",
    "\n",
    "#print(hv_genes, normal_genes)\n",
    "hv_genes = (list(adata.var[adata.var['highly_variable'] == True].index))\n",
    "normal_genes = (list(adata.var_names))\n",
    "\n",
    "hv_columns = [i for i,val in enumerate(normal_genes) if val in hv_genes]\n",
    "#hv_genes\n",
    "len(hv_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "id": "5024bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess\n",
    "#batchcontrol\n",
    "#clustering\n",
    "#cell type annotation\n",
    "#embedding\n",
    "#reverse transcriptome\n",
    "#differential expression\n",
    "\n",
    "#rna velocity -> predicts the future state \n",
    "#predict the probability of unseen perturbations\n",
    "#test: first data set => output list of perturbations \n",
    "# take a 2nd dataset and get list of perturbations + their probability \n",
    "\n",
    "\n",
    "#representation learning (in particular, self-supervised, multi-view, and transfer learning\n",
    "#https://registry.opendata.aws/tabula-muris/#usageexamples\n",
    "\n",
    "#see what preprocessing is needed to get better accuracy\n",
    "\n",
    "##load data\n",
    "\n",
    "##import premade model => output list of predicted unseen perturbation\n",
    "sc.pp.calculate_qc_metrics(adata)\n",
    "\n",
    "#write custom model. get better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "619d5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_expressed = [\"string1\", \"string2\", \"string3\"]\n",
    "\n",
    "    \n",
    "# strings = np.array(gene_expressed)\n",
    "# #adata.obs['genes_expressed'] = strings\n",
    "# coordinates = []\n",
    "cx = adata.X\n",
    "cx = cx.tocoo()\n",
    "\n",
    "# #delete non highly variable genes from matrix\n",
    "\n",
    "\n",
    "# for i,j,v in zip(cx.row, cx.col, cx.data):\n",
    "#     if i in hv_columns: coordinates += [[i,j,v]]\n",
    "# for coord in coordinates:\n",
    "#     coord[1] = adata.var_names[coord[1]]\n",
    "\n",
    "#batch removal - one patient at at time\n",
    "#one cell_type at a time \n",
    "#index cell type\n",
    "#index ctrl/stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "id": "e04c40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "rowGeneExpression = defaultdict(int)\n",
    "rowGeneExpression2 = defaultdict(dict)\n",
    "\n",
    "\n",
    "import math\n",
    "math.floor\n",
    "df = adata.obs\n",
    "end = 50 if True else -1\n",
    "for column in hv_columns:\n",
    "    for row_id in range(math.floor(float(df.shape[0])))[:end]:\n",
    "        rowGeneExpression[row_id] += adata.X[row_id, column]\n",
    "        #rowGeneExpression2[row_id][column] = adata.X[row_id, column]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "id": "482a60c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35.94687108695507,\n",
       " 33.196212351322174,\n",
       " 7.205496981739998,\n",
       " 35.48313131928444,\n",
       " 26.742908969521523,\n",
       " 11.36419740319252,\n",
       " 24.907880529761314,\n",
       " 10.883503705263138,\n",
       " 29.542999163269997,\n",
       " 11.383434176445007,\n",
       " 10.421529799699783,\n",
       " 15.752853259444237,\n",
       " 10.594988703727722,\n",
       " 8.035826236009598,\n",
       " 18.963955223560333,\n",
       " 10.628947839140892,\n",
       " 32.24674420058727,\n",
       " 15.521971955895424,\n",
       " 12.094075202941895,\n",
       " 8.829016640782356,\n",
       " 24.186710730195045,\n",
       " 14.787614285945892,\n",
       " 11.188271582126617,\n",
       " 11.223772212862968,\n",
       " 9.65641550719738,\n",
       " 15.611911222338676,\n",
       " 11.778271540999413,\n",
       " 19.134468212723732,\n",
       " 12.935988828539848,\n",
       " 11.119527280330658,\n",
       " 20.593149721622467,\n",
       " 12.315789371728897,\n",
       " 37.85372845828533,\n",
       " 13.421977430582047,\n",
       " 10.79379317164421,\n",
       " 10.98905485868454,\n",
       " 11.481004521250725,\n",
       " 9.30767348408699,\n",
       " 16.307302594184875,\n",
       " 10.960439682006836,\n",
       " 36.655866891145706,\n",
       " 22.592174723744392,\n",
       " 23.983603835105896,\n",
       " 12.39954560995102,\n",
       " 15.422866806387901,\n",
       " 18.242857322096825,\n",
       " 10.670321449637413,\n",
       " 40.28811080753803,\n",
       " 10.807201787829399,\n",
       " 36.1939612030983,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 1170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "id": "37853254",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = adata.obs\n",
    "dependent_variables =  [rowGeneExpression[row] for row in range(df.shape[0])]\n",
    "df['geneExpressionCount'] = dependent_variables\n",
    "numerical_values = df.select_dtypes(include=[int, float]).values.tolist()\n",
    "dependent_variables = [x for x in dependent_variables]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "id": "9aa9e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "id": "01957362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.9469, 33.1962,  7.2055, 35.4831, 26.7429, 11.3642, 24.9079,  ...,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 1107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_dep = tensor(dependent_variables) # pertrubations\n",
    "t_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "id": "17156f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  3017.0000,    877.0000,    -27.6404,     14.9666,      9.0000,   1704.0000,    711.0000,     35.9469],\n",
       "        [  2481.0000,    713.0000,    -27.4936,     28.9249,      9.0000,   1614.0000,    662.0000,     33.1962],\n",
       "        [   703.0000,    337.0000,    -10.4682,     -5.9844,      3.0000,    908.0000,    337.0000,      7.2055],\n",
       "        [  3420.0000,    850.0000,    -24.3680,     20.4293,      9.0000,   1738.0000,    653.0000,     35.4831],\n",
       "        [  3158.0000,   1111.0000,     27.9522,     24.1597,      4.0000,   1857.0000,    928.0000,     26.7429],\n",
       "        [  1869.0000,    635.0000,     -0.4702,    -25.3987,      5.0000,   1525.0000,    634.0000,     11.3642],\n",
       "        [  1142.0000,    436.0000,    -15.9062,     20.0853,      9.0000,   1157.0000,    436.0000,     24.9079],\n",
       "        ...,\n",
       "        [   635.0000,    424.0000,     -6.6479,     -5.5475,      3.0000,    882.0000,    423.0000,      0.0000],\n",
       "        [  1340.0000,    480.0000,      7.7202,     33.3402,      8.0000,   1324.0000,    480.0000,      0.0000],\n",
       "        [  1033.0000,    468.0000,     18.2683,      1.0582,      6.0000,   1128.0000,    468.0000,      0.0000],\n",
       "        [  2116.0000,    819.0000,    -11.5631,      2.5741,      4.0000,   1669.0000,    799.0000,      0.0000],\n",
       "        [  1522.0000,    523.0000,     25.1424,      6.6038,      6.0000,   1422.0000,    523.0000,      0.0000],\n",
       "        [  1143.0000,    503.0000,     14.3597,     10.9656,      6.0000,   1185.0000,    503.0000,      0.0000],\n",
       "        [  1031.0000,    421.0000,     14.5721,     -4.7139,      5.0000,   1144.0000,    419.0000,      0.0000]])"
      ]
     },
     "execution_count": 1108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_indep = tensor(numerical_values, dtype=torch.float)\n",
    "t_indep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "id": "955bcb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals,indices = t_indep.max(dim=0)\n",
    "t_indep = t_indep / vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "id": "2926f2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0070,  0.1443,  0.1673,  0.1378, -0.4768, -0.0260, -0.2365, -0.2289],\n",
       "        [-0.0058,  0.1173,  0.1664,  0.2664, -0.4768, -0.0246, -0.2202, -0.2114],\n",
       "        [-0.0016,  0.0555,  0.0634, -0.0551, -0.1589, -0.0139, -0.1121, -0.0459],\n",
       "        [-0.0079,  0.1399,  0.1475,  0.1882, -0.4768, -0.0265, -0.2172, -0.2260],\n",
       "        [-0.0073,  0.1828, -0.1692,  0.2225, -0.2119, -0.0284, -0.3087, -0.1703],\n",
       "        [-0.0043,  0.1045,  0.0028, -0.2339, -0.2649, -0.0233, -0.2109, -0.0724],\n",
       "        [-0.0026,  0.0717,  0.0963,  0.1850, -0.4768, -0.0177, -0.1450, -0.1586],\n",
       "        ...,\n",
       "        [-0.0015,  0.0698,  0.0402, -0.0511, -0.1589, -0.0135, -0.1407, -0.0000],\n",
       "        [-0.0031,  0.0790, -0.0467,  0.3071, -0.4238, -0.0202, -0.1597, -0.0000],\n",
       "        [-0.0024,  0.0770, -0.1106,  0.0097, -0.3178, -0.0172, -0.1557, -0.0000],\n",
       "        [-0.0049,  0.1348,  0.0700,  0.0237, -0.2119, -0.0255, -0.2658, -0.0000],\n",
       "        [-0.0035,  0.0861, -0.1522,  0.0608, -0.3178, -0.0217, -0.1740, -0.0000],\n",
       "        [-0.0027,  0.0828, -0.0869,  0.1010, -0.3178, -0.0181, -0.1673, -0.0000],\n",
       "        [-0.0024,  0.0693, -0.0882, -0.0434, -0.2649, -0.0175, -0.1394, -0.0000]])"
      ]
     },
     "execution_count": 1110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_indep*coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "id": "8fc0dfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_indep = t_indep / vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "id": "7f25fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_variables = pd.DataFrame(numerical_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "id": "f6d67306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.transforms import RandomSplitter\n",
    "trn_split,val_split=RandomSplitter(seed=42)(independent_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "id": "42738fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\n",
    "trn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\n",
    "#len(trn_indep),len(val_indep)\n",
    "#dropout, svd, latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "id": "e0499927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; 0.532; {'nCount_RNA': tensor(0.3870), 'nFeature_RNA': tensor(0.1283), 'tsne1': tensor(-0.0261), 'tsne2': tensor(-0.0653), 'cluster': tensor(-0.0117), 'nCount_SCT': tensor(0.1363), 'nFeature_SCT': tensor(-0.3976), 'geneExpressionCount': tensor(-0.2811)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5686"
      ]
     },
     "execution_count": 1169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(443)\n",
    "\n",
    "def calc_preds(coeffs, indeps): return (coeffs * indeps).sum(axis=1)\n",
    "def calc_preds(coeffs, indeps): return torch.sigmoid((indeps*coeffs).sum(axis=1))\n",
    "#def calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1)\n",
    "#def calc_preds(coeffs, indeps): return torch.sigmoid(indeps@coeffs)\n",
    "\n",
    "def calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()\n",
    "def init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_()\n",
    "def update_coeffs(coeffs, lr): \n",
    "    coeffs.sub_(coeffs.grad * lr)\n",
    "    coeffs.grad.zero_()\n",
    "\n",
    "def one_epoch(coeffs, lr):\n",
    "    loss = calc_loss(coeffs, trn_indep, trn_dep)\n",
    "    loss.backward()\n",
    "    with torch.no_grad(): update_coeffs(coeffs, lr)\n",
    "    print(f\"{loss:.3f}\", end=\"; \")\n",
    "    \n",
    "m = torch.nn.Softmax(dim=1)\n",
    "input = torch.randn(2, 3)\n",
    "one = m(input)\n",
    "#print(one, input)\n",
    "\n",
    "# def init_coeffs(): \n",
    "#     hiddens = [10, 10] # <-- set this to the size of each hidden layer you want_ \n",
    "#     sizes = [n_coeff] + hiddens + [1] \n",
    "#     n = len(sizes) \n",
    "#     layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]  for i in range(n-1)] \n",
    "#     consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)] \n",
    "#     for l in layers+consts:\n",
    "#         l.requires_grad_() \n",
    "#     return layers,consts\n",
    "\n",
    "# import torch.nn.functional as F \n",
    "# def calc_preds(coeffs, indeps): \n",
    "#     layers,consts = coeffs \n",
    "#     n = len(layers) \n",
    "#     res = indeps \n",
    "#     for i,l in enumerate(layers): \n",
    "#         res = res@l + consts[i] \n",
    "#         if i!=n-1: res = F.relu(res) \n",
    "#     return torch.sigmoid(res)\n",
    "\n",
    "# def update_coeffs(coeffs, lr): \n",
    "#     layers, consts = coeffs \n",
    "#     for layer in layers+consts: \n",
    "#         layer.sub_(layer.grad * lr) \n",
    "#         layer.grad.zero_()\n",
    "        \n",
    "def train_model(epochs=30, lr=4):\n",
    "    coeffs = init_coeffs()\n",
    "    for i in range(epochs): \n",
    "        one_epoch(coeffs, lr=lr)\n",
    "#         print(coeffs)\n",
    "#         coeffs = m(coeffs)\n",
    "    return coeffs\n",
    "\n",
    "coeffs = train_model(33, lr=.02)\n",
    "\n",
    "indep_cols = ['nCount_RNA' ,                \n",
    "'nFeature_RNA'    ,             \n",
    "'tsne1'     ,                 \n",
    "'tsne2'  ,                   \n",
    "'cluster'    ,                  \n",
    "'nCount_SCT'   ,              \n",
    "'nFeature_SCT',\n",
    "'geneExpressionCount'\n",
    "]    \n",
    "def show_coeffs(): return dict(zip(indep_cols, coeffs.requires_grad_(False)))\n",
    "print(show_coeffs())\n",
    "len([prob for prob in calc_preds(coeffs, t_indep) if prob > .5]) # should be 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "id": "14ce99b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4509545564651489,\n",
       " 0.4509437382221222,\n",
       " 0.4839145243167877,\n",
       " 0.45091331005096436,\n",
       " 0.47732439637184143,\n",
       " 0.4733034670352936,\n",
       " 0.45133858919143677,\n",
       " 0.4733440577983856,\n",
       " 0.45113489031791687,\n",
       " 0.4627237021923065,\n",
       " 0.446952760219574,\n",
       " 0.48351556062698364,\n",
       " 0.4733254611492157,\n",
       " 0.47360503673553467,\n",
       " 0.47823014855384827,\n",
       " 0.4786529839038849,\n",
       " 0.45100000500679016,\n",
       " 0.4467613399028778,\n",
       " 0.4732779264450073,\n",
       " 0.4735549986362457,\n",
       " 0.45134130120277405,\n",
       " 0.4730464220046997,\n",
       " 0.4786413013935089,\n",
       " 0.4469107687473297,\n",
       " 0.47341179847717285,\n",
       " 0.4467383921146393,\n",
       " 0.4732570946216583,\n",
       " 0.4516659677028656,\n",
       " 0.47324123978614807,\n",
       " 0.47862011194229126,\n",
       " 0.48333826661109924,\n",
       " 0.4734020233154297,\n",
       " 0.45077699422836304,\n",
       " 0.4836588501930237,\n",
       " 0.47327563166618347,\n",
       " 0.4732299745082855,\n",
       " 0.4732414484024048,\n",
       " 0.4838330149650574,\n",
       " 0.47824567556381226,\n",
       " 0.4732862114906311,\n",
       " 0.4508187472820282,\n",
       " 0.45140066742897034,\n",
       " 0.4937223792076111,\n",
       " 0.44692304730415344,\n",
       " 0.4783414900302887,\n",
       " 0.4781681001186371,\n",
       " 0.47322478890419006,\n",
       " 0.450693815946579,\n",
       " 0.4733642339706421,\n",
       " 0.45071280002593994,\n",
       " 0.48423057794570923,\n",
       " 0.45237427949905396,\n",
       " 0.47385331988334656,\n",
       " 0.4948105216026306,\n",
       " 0.4524249732494354,\n",
       " 0.4738887548446655,\n",
       " 0.4791125953197479,\n",
       " 0.4791223406791687,\n",
       " 0.4739135801792145,\n",
       " 0.45238062739372253,\n",
       " 0.479105144739151,\n",
       " 0.4473992884159088,\n",
       " 0.4630964994430542,\n",
       " 0.4791441261768341,\n",
       " 0.49476081132888794,\n",
       " 0.4739346504211426,\n",
       " 0.4632296562194824,\n",
       " 0.4738149642944336,\n",
       " 0.4738325774669647,\n",
       " 0.4523845911026001,\n",
       " 0.4737762212753296,\n",
       " 0.4524522125720978,\n",
       " 0.47380656003952026,\n",
       " 0.4842129647731781,\n",
       " 0.4631924629211426,\n",
       " 0.4473715126514435,\n",
       " 0.4842205047607422,\n",
       " 0.4524384140968323,\n",
       " 0.473940908908844,\n",
       " 0.4738645553588867,\n",
       " 0.47386303544044495,\n",
       " 0.4523635506629944,\n",
       " 0.4738259017467499,\n",
       " 0.47368890047073364,\n",
       " 0.44732609391212463,\n",
       " 0.47391408681869507,\n",
       " 0.4524918496608734,\n",
       " 0.4473983943462372,\n",
       " 0.47390106320381165,\n",
       " 0.47376638650894165,\n",
       " 0.47388970851898193,\n",
       " 0.4737885296344757,\n",
       " 0.45239967107772827,\n",
       " 0.44740530848503113,\n",
       " 0.48945152759552,\n",
       " 0.47382691502571106,\n",
       " 0.47374582290649414,\n",
       " 0.44744253158569336,\n",
       " 0.4525239169597626,\n",
       " 0.4736000597476959,\n",
       " 0.4738229513168335,\n",
       " 0.473849892616272,\n",
       " 0.49480941891670227,\n",
       " 0.44740769267082214,\n",
       " 0.46319296956062317,\n",
       " 0.45234647393226624,\n",
       " 0.4739163815975189,\n",
       " 0.4524139165878296,\n",
       " 0.473819375038147,\n",
       " 0.4523928165435791,\n",
       " 0.4790194630622864,\n",
       " 0.44740262627601624,\n",
       " 0.4473942518234253,\n",
       " 0.4948102831840515,\n",
       " 0.4737478196620941,\n",
       " 0.4791218936443329,\n",
       " 0.47385212779045105,\n",
       " 0.47373875975608826,\n",
       " 0.4525645971298218,\n",
       " 0.4739505350589752,\n",
       " 0.4524519145488739,\n",
       " 0.4525035619735718,\n",
       " 0.4526005983352661,\n",
       " 0.4790765941143036,\n",
       " 0.4474537968635559,\n",
       " 0.47909125685691833,\n",
       " 0.4790427088737488,\n",
       " 0.4791215658187866,\n",
       " 0.48420313000679016,\n",
       " 0.46293094754219055,\n",
       " 0.49477750062942505,\n",
       " 0.47384607791900635,\n",
       " 0.4789702296257019,\n",
       " 0.47382569313049316,\n",
       " 0.47383177280426025,\n",
       " 0.4523986279964447,\n",
       " 0.47394245862960815,\n",
       " 0.47393083572387695,\n",
       " 0.4525470733642578,\n",
       " 0.45240482687950134,\n",
       " 0.46312832832336426,\n",
       " 0.4738025665283203,\n",
       " 0.49478933215141296,\n",
       " 0.45253896713256836,\n",
       " 0.48421144485473633,\n",
       " 0.47907406091690063,\n",
       " 0.45236217975616455,\n",
       " 0.4947919547557831,\n",
       " 0.45237183570861816,\n",
       " 0.4631899893283844,\n",
       " 0.45227769017219543,\n",
       " 0.45238399505615234,\n",
       " 0.47386935353279114,\n",
       " 0.47910571098327637,\n",
       " 0.47378009557724,\n",
       " 0.4737410843372345,\n",
       " 0.45250096917152405,\n",
       " 0.4524509906768799,\n",
       " 0.47394078969955444,\n",
       " 0.4739215672016144,\n",
       " 0.45239585638046265,\n",
       " 0.4738253653049469,\n",
       " 0.45247140526771545,\n",
       " 0.4739089906215668,\n",
       " 0.47375085949897766,\n",
       " 0.4524659216403961,\n",
       " 0.44741714000701904,\n",
       " 0.46322697401046753,\n",
       " 0.46314090490341187,\n",
       " 0.4790283143520355,\n",
       " 0.47383859753608704,\n",
       " 0.4739019274711609,\n",
       " 0.4473932385444641,\n",
       " 0.4473753571510315,\n",
       " 0.47911006212234497,\n",
       " 0.4524257183074951,\n",
       " 0.4738854169845581,\n",
       " 0.473940908908844,\n",
       " 0.4474213421344757,\n",
       " 0.4473487138748169,\n",
       " 0.47379270195961,\n",
       " 0.473858118057251,\n",
       " 0.4474174380302429,\n",
       " 0.49478042125701904,\n",
       " 0.47373929619789124,\n",
       " 0.47905233502388,\n",
       " 0.47911176085472107,\n",
       " 0.4523538053035736,\n",
       " 0.4737328290939331,\n",
       " 0.45241567492485046,\n",
       " 0.47377774119377136,\n",
       " 0.45255473256111145,\n",
       " 0.46321767568588257,\n",
       " 0.45243293046951294,\n",
       " 0.4474106431007385,\n",
       " 0.4737238883972168,\n",
       " 0.452558696269989,\n",
       " 0.4739428162574768,\n",
       " 0.4791136384010315,\n",
       " 0.4474271833896637,\n",
       " 0.4524451494216919,\n",
       " 0.44739872217178345,\n",
       " 0.47390347719192505,\n",
       " 0.45238715410232544,\n",
       " 0.4790785610675812,\n",
       " 0.4738452732563019,\n",
       " 0.45236295461654663,\n",
       " 0.4842081665992737,\n",
       " 0.4474172294139862,\n",
       " 0.44741132855415344,\n",
       " 0.4791243076324463,\n",
       " 0.45243147015571594,\n",
       " 0.4525392949581146,\n",
       " 0.4524178206920624,\n",
       " 0.47369831800460815,\n",
       " 0.452513188123703,\n",
       " 0.4524003267288208,\n",
       " 0.4631846845149994,\n",
       " 0.4738522469997406,\n",
       " 0.4522625803947449,\n",
       " 0.473798930644989,\n",
       " 0.4738416373729706,\n",
       " 0.47902849316596985,\n",
       " 0.4790661931037903,\n",
       " 0.4790949821472168,\n",
       " 0.4784691333770752,\n",
       " 0.47389110922813416,\n",
       " 0.4524669945240021,\n",
       " 0.4631933867931366,\n",
       " 0.4524555802345276,\n",
       " 0.44738537073135376,\n",
       " 0.47908633947372437,\n",
       " 0.47908732295036316,\n",
       " 0.4737493097782135,\n",
       " 0.473869264125824,\n",
       " 0.4525838792324066,\n",
       " 0.4738309383392334,\n",
       " 0.4523650109767914,\n",
       " 0.47371163964271545,\n",
       " 0.447416752576828,\n",
       " 0.4842418134212494,\n",
       " 0.44738584756851196,\n",
       " 0.4737608730792999,\n",
       " 0.47386643290519714,\n",
       " 0.45238369703292847,\n",
       " 0.44740521907806396,\n",
       " 0.4737405776977539,\n",
       " 0.4524276554584503,\n",
       " 0.4524853527545929,\n",
       " 0.4523792266845703,\n",
       " 0.4631494879722595,\n",
       " 0.452432245016098,\n",
       " 0.4738694727420807,\n",
       " 0.47381123900413513,\n",
       " 0.4790923297405243,\n",
       " 0.4473912715911865,\n",
       " 0.4789977967739105,\n",
       " 0.4790506362915039,\n",
       " 0.47376498579978943,\n",
       " 0.47391510009765625,\n",
       " 0.452424556016922,\n",
       " 0.45242342352867126,\n",
       " 0.45245909690856934,\n",
       " 0.473807156085968,\n",
       " 0.4791293442249298,\n",
       " 0.452481210231781,\n",
       " 0.47375327348709106,\n",
       " 0.4524766802787781,\n",
       " 0.47375649213790894,\n",
       " 0.4524405896663666,\n",
       " 0.479127436876297,\n",
       " 0.4473991096019745,\n",
       " 0.45237573981285095,\n",
       " 0.4523634612560272,\n",
       " 0.4523812234401703,\n",
       " 0.45239847898483276,\n",
       " 0.47376391291618347,\n",
       " 0.4790334105491638,\n",
       " 0.4789465069770813,\n",
       " 0.4790642261505127,\n",
       " 0.45240887999534607,\n",
       " 0.47910669445991516,\n",
       " 0.47911882400512695,\n",
       " 0.4790442883968353,\n",
       " 0.44743457436561584,\n",
       " 0.4842032492160797,\n",
       " 0.4474627673625946,\n",
       " 0.4738388657569885,\n",
       " 0.47910839319229126,\n",
       " 0.47381412982940674,\n",
       " 0.4631841778755188,\n",
       " 0.4525478482246399,\n",
       " 0.47901809215545654,\n",
       " 0.4791511297225952,\n",
       " 0.4523763358592987,\n",
       " 0.4790307283401489,\n",
       " 0.47384965419769287,\n",
       " 0.4524593949317932,\n",
       " 0.4474017918109894,\n",
       " 0.4738408327102661,\n",
       " 0.4842115044593811,\n",
       " 0.452260285615921,\n",
       " 0.452440470457077,\n",
       " 0.45238152146339417,\n",
       " 0.4738287925720215,\n",
       " 0.45234614610671997,\n",
       " 0.4631834626197815,\n",
       " 0.4523994028568268,\n",
       " 0.4736713171005249,\n",
       " 0.4524151682853699,\n",
       " 0.4842751920223236,\n",
       " 0.4790845811367035,\n",
       " 0.47388043999671936,\n",
       " 0.47389110922813416,\n",
       " 0.47389987111091614,\n",
       " 0.4632188379764557,\n",
       " 0.47383439540863037,\n",
       " 0.47906461358070374,\n",
       " 0.44739848375320435,\n",
       " 0.4523666203022003,\n",
       " 0.47375011444091797,\n",
       " 0.4737839996814728,\n",
       " 0.45241546630859375,\n",
       " 0.4523782730102539,\n",
       " 0.47382470965385437,\n",
       " 0.44739586114883423,\n",
       " 0.47372570633888245,\n",
       " 0.49478086829185486,\n",
       " 0.4737910032272339,\n",
       " 0.47382745146751404,\n",
       " 0.47386690974235535,\n",
       " 0.4474116265773773,\n",
       " 0.46319541335105896,\n",
       " 0.4737720489501953,\n",
       " 0.45238810777664185,\n",
       " 0.4789949059486389,\n",
       " 0.46314918994903564,\n",
       " 0.4524298310279846,\n",
       " 0.4630783200263977,\n",
       " 0.4737440049648285,\n",
       " 0.4738253057003021,\n",
       " 0.48421138525009155,\n",
       " 0.4737464189529419,\n",
       " 0.4474017322063446,\n",
       " 0.45251455903053284,\n",
       " 0.4739173352718353,\n",
       " 0.47375568747520447,\n",
       " 0.4739134907722473,\n",
       " 0.4947894513607025,\n",
       " 0.45249316096305847,\n",
       " 0.44740527868270874,\n",
       " 0.45238053798675537,\n",
       " 0.4523448050022125,\n",
       " 0.47379812598228455,\n",
       " 0.4629005491733551,\n",
       " 0.47375205159187317,\n",
       " 0.4739616811275482,\n",
       " 0.47908681631088257,\n",
       " 0.4474206864833832,\n",
       " 0.47388821840286255,\n",
       " 0.4474000632762909,\n",
       " 0.4631926715373993,\n",
       " 0.4842154085636139,\n",
       " 0.4523766338825226,\n",
       " 0.4738106429576874,\n",
       " 0.47383466362953186,\n",
       " 0.45241615176200867,\n",
       " 0.4474339485168457,\n",
       " 0.4473741054534912,\n",
       " 0.4789387285709381,\n",
       " 0.49477165937423706,\n",
       " 0.4631715714931488,\n",
       " 0.45254847407341003,\n",
       " 0.4523954689502716,\n",
       " 0.4739299714565277,\n",
       " 0.452256441116333,\n",
       " 0.4522598385810852,\n",
       " 0.4738852083683014,\n",
       " 0.452423095703125,\n",
       " 0.4738200604915619,\n",
       " 0.47381702065467834,\n",
       " 0.4738044738769531,\n",
       " 0.45241424441337585,\n",
       " 0.4737713038921356,\n",
       " 0.4738023579120636,\n",
       " 0.45243361592292786,\n",
       " 0.4524591565132141,\n",
       " 0.47904857993125916,\n",
       " 0.4524728059768677,\n",
       " 0.45241254568099976,\n",
       " 0.47909432649612427,\n",
       " 0.4524019658565521,\n",
       " 0.4841991066932678,\n",
       " 0.49477678537368774,\n",
       " 0.47908473014831543,\n",
       " 0.47379618883132935,\n",
       " 0.44741544127464294,\n",
       " 0.4522611200809479,\n",
       " 0.4738292098045349,\n",
       " 0.49480947852134705,\n",
       " 0.473734587430954,\n",
       " 0.48944777250289917,\n",
       " 0.4737584590911865,\n",
       " 0.4739004671573639,\n",
       " 0.4525645971298218,\n",
       " 0.45246627926826477,\n",
       " 0.4789624512195587,\n",
       " 0.447412371635437,\n",
       " 0.4523923397064209,\n",
       " 0.49481654167175293,\n",
       " 0.4790000021457672,\n",
       " 0.4474477469921112,\n",
       " 0.47907254099845886,\n",
       " 0.44739091396331787,\n",
       " 0.49476557970046997,\n",
       " 0.4737706780433655,\n",
       " 0.49480557441711426,\n",
       " 0.45246943831443787,\n",
       " 0.4737951159477234,\n",
       " 0.4523794651031494,\n",
       " 0.4523906409740448,\n",
       " 0.479117751121521,\n",
       " 0.4790807366371155,\n",
       " 0.4738183915615082,\n",
       " 0.47909069061279297,\n",
       " 0.47909873723983765,\n",
       " 0.47376424074172974,\n",
       " 0.4736756384372711,\n",
       " 0.447422057390213,\n",
       " 0.4739001989364624,\n",
       " 0.47382038831710815,\n",
       " 0.49480772018432617,\n",
       " 0.4947660565376282,\n",
       " 0.47895053029060364,\n",
       " 0.45246368646621704,\n",
       " 0.447393000125885,\n",
       " 0.4842003285884857,\n",
       " 0.4790874123573303,\n",
       " 0.4525222182273865,\n",
       " 0.45240697264671326,\n",
       " 0.47372904419898987,\n",
       " 0.45240989327430725,\n",
       " 0.45242902636528015,\n",
       " 0.47383424639701843,\n",
       " 0.47384291887283325,\n",
       " 0.4523999094963074,\n",
       " 0.44741904735565186,\n",
       " 0.47909629344940186,\n",
       " 0.47378066182136536,\n",
       " 0.4473896622657776,\n",
       " 0.4737589955329895,\n",
       " 0.46315768361091614,\n",
       " 0.44739633798599243,\n",
       " 0.4524008333683014,\n",
       " 0.4790717661380768,\n",
       " 0.4791119396686554,\n",
       " 0.452281653881073,\n",
       " 0.4739007353782654,\n",
       " 0.45237311720848083,\n",
       " 0.47371265292167664,\n",
       " 0.4524337351322174,\n",
       " 0.46323660016059875,\n",
       " 0.4524046778678894,\n",
       " 0.4473719894886017,\n",
       " 0.4791311025619507,\n",
       " 0.44746336340904236,\n",
       " 0.4474025368690491,\n",
       " 0.45237472653388977,\n",
       " 0.47374582290649414,\n",
       " 0.47378918528556824,\n",
       " 0.47373849153518677,\n",
       " 0.46316927671432495,\n",
       " 0.47846806049346924,\n",
       " 0.473686546087265,\n",
       " 0.46319133043289185,\n",
       " 0.4738554060459137,\n",
       " 0.4790807366371155,\n",
       " 0.4791233241558075,\n",
       " 0.4791201949119568,\n",
       " 0.4524160921573639,\n",
       " 0.4738316237926483,\n",
       " 0.47371432185173035,\n",
       " 0.47909441590309143,\n",
       " 0.4737206995487213,\n",
       " 0.479060560464859,\n",
       " 0.4525618255138397,\n",
       " 0.4631335437297821,\n",
       " 0.4737330377101898,\n",
       " 0.4631786346435547,\n",
       " 0.45240089297294617,\n",
       " 0.47903281450271606,\n",
       " 0.4947877526283264,\n",
       " 0.47373664379119873,\n",
       " 0.4523981809616089,\n",
       " 0.4947640597820282,\n",
       " 0.47904518246650696,\n",
       " 0.46319374442100525,\n",
       " 0.47909799218177795,\n",
       " 0.4739130437374115,\n",
       " 0.47377049922943115,\n",
       " 0.47903555631637573,\n",
       " 0.4524809718132019,\n",
       " 0.45239296555519104,\n",
       " 0.4474080204963684,\n",
       " 0.4523444175720215,\n",
       " 0.47900426387786865,\n",
       " 0.45243313908576965,\n",
       " 0.47371211647987366,\n",
       " 0.4790264666080475,\n",
       " 0.4790583550930023,\n",
       " 0.4948037564754486,\n",
       " 0.44739797711372375,\n",
       " 0.45235610008239746,\n",
       " 0.4474361836910248,\n",
       " 0.47381022572517395,\n",
       " 0.45238161087036133,\n",
       " 0.479115754365921,\n",
       " 0.4523725211620331,\n",
       " 0.47369951009750366,\n",
       " 0.4737728536128998,\n",
       " 0.4524482786655426,\n",
       " 0.4524378180503845,\n",
       " 0.47911977767944336,\n",
       " 0.48419496417045593,\n",
       " 0.4738396406173706,\n",
       " 0.4738217294216156,\n",
       " 0.4736141264438629,\n",
       " 0.45235931873321533,\n",
       " 0.4474104642868042,\n",
       " 0.473888099193573,\n",
       " 0.47376713156700134,\n",
       " 0.47375887632369995,\n",
       " 0.4739432632923126,\n",
       " 0.4737568497657776,\n",
       " 0.47378796339035034,\n",
       " 0.4737774133682251,\n",
       " 0.45247653126716614,\n",
       " 0.4737125635147095,\n",
       " 0.4523771107196808,\n",
       " 0.4524686634540558,\n",
       " 0.4737972021102905,\n",
       " 0.4473985731601715,\n",
       " 0.47381144762039185,\n",
       " 0.4790686070919037,\n",
       " 0.4738554358482361,\n",
       " 0.4738411009311676,\n",
       " 0.4738236963748932,\n",
       " 0.47393542528152466,\n",
       " 0.473799467086792,\n",
       " 0.473772257566452,\n",
       " 0.4523734450340271,\n",
       " 0.47382569313049316,\n",
       " 0.47900786995887756,\n",
       " 0.4737887382507324,\n",
       " 0.47383418679237366,\n",
       " 0.4735373556613922,\n",
       " 0.47387316823005676,\n",
       " 0.4738351106643677,\n",
       " 0.47367504239082336,\n",
       " 0.4738316833972931,\n",
       " 0.4474102258682251,\n",
       " 0.4790976047515869,\n",
       " 0.4523921012878418,\n",
       " 0.47376295924186707,\n",
       " 0.45242074131965637,\n",
       " 0.47369158267974854,\n",
       " 0.4632016122341156,\n",
       " 0.4737907946109772,\n",
       " 0.4631836414337158,\n",
       " 0.45247912406921387,\n",
       " 0.4790973365306854,\n",
       " 0.4524969756603241,\n",
       " 0.47384098172187805,\n",
       " 0.4524686634540558,\n",
       " 0.47905755043029785,\n",
       " 0.47909969091415405,\n",
       " 0.45239028334617615,\n",
       " 0.4841988682746887,\n",
       " 0.4791075587272644,\n",
       " 0.47911542654037476,\n",
       " 0.479066401720047,\n",
       " 0.45237165689468384,\n",
       " 0.48421016335487366,\n",
       " 0.4738238751888275,\n",
       " 0.4523410201072693,\n",
       " 0.447422057390213,\n",
       " 0.47369521856307983,\n",
       " 0.45239877700805664,\n",
       " 0.473803848028183,\n",
       " 0.48421385884284973,\n",
       " 0.47384223341941833,\n",
       " 0.4737853705883026,\n",
       " 0.46317005157470703,\n",
       " 0.4737216532230377,\n",
       " 0.46316948533058167,\n",
       " 0.4737761318683624,\n",
       " 0.4738706648349762,\n",
       " 0.49475812911987305,\n",
       " 0.4738576412200928,\n",
       " 0.4474256932735443,\n",
       " 0.47388237714767456,\n",
       " 0.4738048017024994,\n",
       " 0.452389121055603,\n",
       " 0.4522657096385956,\n",
       " 0.47380053997039795,\n",
       " 0.4791101813316345,\n",
       " 0.4738328456878662,\n",
       " 0.4738890826702118,\n",
       " 0.45236027240753174,\n",
       " 0.4524366855621338,\n",
       " 0.45236775279045105,\n",
       " 0.4736674427986145,\n",
       " 0.4474142789840698,\n",
       " 0.4738195836544037,\n",
       " 0.47381043434143066,\n",
       " 0.47375792264938354,\n",
       " 0.4523441791534424,\n",
       " 0.4523533582687378,\n",
       " 0.4524615406990051,\n",
       " 0.4523528218269348,\n",
       " 0.4525356590747833,\n",
       " 0.45234790444374084,\n",
       " 0.479056179523468,\n",
       " 0.45257508754730225,\n",
       " 0.48422738909721375,\n",
       " 0.4737952947616577,\n",
       " 0.47390395402908325,\n",
       " 0.45227086544036865,\n",
       " 0.47393667697906494,\n",
       " 0.4737567603588104,\n",
       " 0.45242631435394287,\n",
       " 0.4523886442184448,\n",
       " 0.4525619149208069,\n",
       " 0.473884254693985,\n",
       " 0.47384169697761536,\n",
       " 0.4737410247325897,\n",
       " 0.45243850350379944,\n",
       " 0.44744598865509033,\n",
       " 0.4790773391723633,\n",
       " 0.45238780975341797,\n",
       " 0.447412371635437,\n",
       " 0.47377023100852966,\n",
       " 0.4737238883972168,\n",
       " 0.47376498579978943,\n",
       " 0.47387826442718506,\n",
       " 0.47390854358673096,\n",
       " 0.4790935516357422,\n",
       " 0.47383201122283936,\n",
       " 0.47381433844566345,\n",
       " 0.45236337184906006,\n",
       " 0.45235827565193176,\n",
       " 0.4738808274269104,\n",
       " 0.4790392220020294,\n",
       " 0.4737948477268219,\n",
       " 0.47372967004776,\n",
       " 0.4738401472568512,\n",
       " 0.45226648449897766,\n",
       " 0.46317198872566223,\n",
       " 0.47375965118408203,\n",
       " 0.47373166680336,\n",
       " 0.45226454734802246,\n",
       " 0.47377675771713257,\n",
       " 0.4738299548625946,\n",
       " 0.4680570363998413,\n",
       " 0.447405606508255,\n",
       " 0.47366246581077576,\n",
       " 0.4524150788784027,\n",
       " 0.4737482964992523,\n",
       " 0.4738244414329529,\n",
       " 0.45238906145095825,\n",
       " 0.47388145327568054,\n",
       " 0.4474288523197174,\n",
       " 0.452411025762558,\n",
       " 0.4524015486240387,\n",
       " 0.4737471044063568,\n",
       " 0.4737876355648041,\n",
       " 0.4523780047893524,\n",
       " 0.4524598717689514,\n",
       " 0.47911784052848816,\n",
       " 0.4737754166126251,\n",
       " 0.4524197578430176,\n",
       " 0.4473629295825958,\n",
       " 0.48420417308807373,\n",
       " 0.4736813008785248,\n",
       " 0.45241907238960266,\n",
       " 0.4522629678249359,\n",
       " 0.473848819732666,\n",
       " 0.45234307646751404,\n",
       " 0.4473898112773895,\n",
       " 0.4947839081287384,\n",
       " 0.45239633321762085,\n",
       " 0.4474011957645416,\n",
       " 0.46318620443344116,\n",
       " 0.473383367061615,\n",
       " 0.45240747928619385,\n",
       " 0.47913306951522827,\n",
       " 0.4523589611053467,\n",
       " 0.4474134147167206,\n",
       " 0.4738689363002777,\n",
       " 0.47391369938850403,\n",
       " 0.45245519280433655,\n",
       " 0.47379088401794434,\n",
       " 0.47394099831581116,\n",
       " 0.4790334701538086,\n",
       " 0.47377270460128784,\n",
       " 0.45252734422683716,\n",
       " 0.45244327187538147,\n",
       " 0.49480119347572327,\n",
       " 0.46316248178482056,\n",
       " 0.45244300365448,\n",
       " 0.4737895131111145,\n",
       " 0.4791273772716522,\n",
       " 0.4524505138397217,\n",
       " 0.47386112809181213,\n",
       " 0.45239511132240295,\n",
       " 0.4522719085216522,\n",
       " 0.45249760150909424,\n",
       " 0.45239925384521484,\n",
       " 0.47377365827560425,\n",
       " 0.473884254693985,\n",
       " 0.4790925681591034,\n",
       " 0.47378823161125183,\n",
       " 0.4737110435962677,\n",
       " 0.4790526330471039,\n",
       " 0.4737900495529175,\n",
       " 0.45255956053733826,\n",
       " 0.4737304151058197,\n",
       " 0.47388404607772827,\n",
       " 0.47899800539016724,\n",
       " 0.47900697588920593,\n",
       " 0.4739200174808502,\n",
       " 0.45246896147727966,\n",
       " 0.47374042868614197,\n",
       " 0.47377511858940125,\n",
       " 0.4791002869606018,\n",
       " 0.473869264125824,\n",
       " 0.45244961977005005,\n",
       " 0.47382354736328125,\n",
       " 0.46320492029190063,\n",
       " 0.4524029791355133,\n",
       " 0.47385844588279724,\n",
       " 0.4523888826370239,\n",
       " 0.4842052459716797,\n",
       " 0.47391027212142944,\n",
       " 0.4632386863231659,\n",
       " 0.4739183187484741,\n",
       " 0.473827064037323,\n",
       " 0.4632307291030884,\n",
       " 0.4790590703487396,\n",
       " 0.4739351272583008,\n",
       " 0.47374480962753296,\n",
       " 0.4739464819431305,\n",
       " 0.4737021327018738,\n",
       " 0.47377824783325195,\n",
       " 0.4631289839744568,\n",
       " 0.45237502455711365,\n",
       " 0.48422771692276,\n",
       " 0.47385725378990173,\n",
       " 0.4737572968006134,\n",
       " 0.4739173352718353,\n",
       " 0.47909677028656006,\n",
       " 0.4842158555984497,\n",
       " 0.4737238883972168,\n",
       " 0.4473726749420166,\n",
       " 0.4736953377723694,\n",
       " 0.4737633764743805,\n",
       " 0.4474261701107025,\n",
       " 0.4738689363002777,\n",
       " 0.4474295675754547,\n",
       " 0.44741711020469666,\n",
       " 0.44742682576179504,\n",
       " 0.4738437831401825,\n",
       " 0.4525684714317322,\n",
       " 0.4631671905517578,\n",
       " 0.4736936092376709,\n",
       " 0.4523780047893524,\n",
       " 0.4790613651275635,\n",
       " 0.4738277196884155,\n",
       " 0.46320995688438416,\n",
       " 0.48421117663383484,\n",
       " 0.44741544127464294,\n",
       " 0.4790935516357422,\n",
       " 0.47376248240470886,\n",
       " 0.45236217975616455,\n",
       " 0.4738359749317169,\n",
       " 0.44742488861083984,\n",
       " 0.45245540142059326,\n",
       " 0.4525773823261261,\n",
       " 0.4738641083240509,\n",
       " 0.4525515139102936,\n",
       " 0.47906365990638733,\n",
       " 0.4738554060459137,\n",
       " 0.47910165786743164,\n",
       " 0.45238667726516724,\n",
       " 0.47380971908569336,\n",
       " 0.4736015200614929,\n",
       " 0.47376322746276855,\n",
       " 0.4524453282356262,\n",
       " 0.4739316999912262,\n",
       " 0.4524199664592743,\n",
       " 0.47385919094085693,\n",
       " 0.4527529180049896,\n",
       " 0.4631866216659546,\n",
       " 0.47364097833633423,\n",
       " 0.46316733956336975,\n",
       " 0.44740062952041626,\n",
       " 0.4631996154785156,\n",
       " 0.4842067062854767,\n",
       " 0.47383061051368713,\n",
       " 0.4523725211620331,\n",
       " 0.46317026019096375,\n",
       " 0.4523981809616089,\n",
       " 0.44739213585853577,\n",
       " 0.4523598551750183,\n",
       " 0.45250892639160156,\n",
       " 0.4739026725292206,\n",
       " 0.46318596601486206,\n",
       " 0.45237863063812256,\n",
       " 0.4523814618587494,\n",
       " 0.4738614857196808,\n",
       " 0.4784735143184662,\n",
       " 0.47384098172187805,\n",
       " 0.4737829864025116,\n",
       " 0.47912633419036865,\n",
       " 0.4523663818836212,\n",
       " 0.47908392548561096,\n",
       " 0.4738982915878296,\n",
       " 0.45242592692375183,\n",
       " 0.484198659658432,\n",
       " 0.47373899817466736,\n",
       " 0.4524070620536804,\n",
       " 0.49480223655700684,\n",
       " 0.47900205850601196,\n",
       " 0.45244529843330383,\n",
       " 0.4738287925720215,\n",
       " 0.45237165689468384,\n",
       " 0.4890497624874115,\n",
       " 0.47389185428619385,\n",
       " 0.44740092754364014,\n",
       " 0.4738517105579376,\n",
       " 0.4739089906215668,\n",
       " 0.45238086581230164,\n",
       " 0.46293365955352783,\n",
       " 0.47389987111091614,\n",
       " 0.4524051249027252,\n",
       " 0.4947717785835266,\n",
       " 0.47381702065467834,\n",
       " 0.4523967206478119,\n",
       " 0.47907203435897827,\n",
       " 0.4525268077850342,\n",
       " 0.47381702065467834,\n",
       " 0.4525819420814514,\n",
       " 0.47911325097084045,\n",
       " 0.4523954689502716,\n",
       " 0.45235568284988403,\n",
       " 0.4474293291568756,\n",
       " 0.4738211929798126,\n",
       " 0.45235180854797363,\n",
       " 0.4842667579650879,\n",
       " 0.45226654410362244,\n",
       " 0.473682165145874,\n",
       " 0.47382354736328125,\n",
       " 0.4784735441207886,\n",
       " 0.4739235043525696,\n",
       " 0.4842117428779602,\n",
       " 0.4473995864391327,\n",
       " 0.4523577094078064,\n",
       " 0.4523545801639557,\n",
       " 0.4737667441368103,\n",
       " 0.49478062987327576,\n",
       " 0.45244690775871277,\n",
       " 0.47386959195137024,\n",
       " 0.47392934560775757,\n",
       " 0.4737411141395569,\n",
       " 0.44750329852104187,\n",
       " 0.45242342352867126,\n",
       " 0.4523737132549286,\n",
       " 0.463207483291626,\n",
       " 0.4474242925643921,\n",
       " 0.4739408493041992,\n",
       " 0.4474276602268219,\n",
       " 0.4474106431007385,\n",
       " 0.47909703850746155,\n",
       " 0.447415292263031,\n",
       " 0.47392773628234863,\n",
       " 0.4738973081111908,\n",
       " 0.45247089862823486,\n",
       " 0.4738604724407196,\n",
       " 0.4524681866168976,\n",
       " 0.4737693667411804,\n",
       " 0.4631856381893158,\n",
       " 0.47391846776008606,\n",
       " 0.46314796805381775,\n",
       " 0.47387900948524475,\n",
       " 0.4523642361164093,\n",
       " 0.452353298664093,\n",
       " 0.48420819640159607,\n",
       " 0.4739384353160858,\n",
       " 0.49480947852134705,\n",
       " 0.4523763358592987,\n",
       " 0.47902894020080566,\n",
       " 0.4738485813140869,\n",
       " 0.4525213837623596,\n",
       " 0.47905388474464417,\n",
       " 0.4473389983177185,\n",
       " 0.4738350510597229,\n",
       " 0.44744130969047546,\n",
       " 0.47381123900413513,\n",
       " 0.4523966312408447,\n",
       " 0.4738088846206665,\n",
       " 0.47372904419898987,\n",
       " 0.45246657729148865,\n",
       " 0.4523462951183319,\n",
       " 0.44740474224090576,\n",
       " 0.4739047586917877,\n",
       " 0.4790796637535095,\n",
       " 0.45240288972854614,\n",
       " 0.4737427234649658,\n",
       " 0.45226776599884033,\n",
       " 0.4791070520877838,\n",
       " 0.4739014804363251,\n",
       " 0.45225927233695984,\n",
       " 0.447399765253067,\n",
       " 0.4523877501487732,\n",
       " 0.4737355709075928,\n",
       " 0.4524359703063965,\n",
       " 0.47390031814575195,\n",
       " 0.4738914370536804,\n",
       " 0.4474063217639923,\n",
       " 0.4523364305496216,\n",
       " 0.47389474511146545,\n",
       " 0.4739118218421936,\n",
       " 0.49479442834854126,\n",
       " 0.4739411771297455,\n",
       " 0.4737083911895752,\n",
       " 0.44740912318229675,\n",
       " 0.48420122265815735,\n",
       " 0.4947658181190491,\n",
       " 0.4737653136253357,\n",
       " 0.447382390499115,\n",
       " 0.4524211883544922,\n",
       " 0.47373276948928833,\n",
       " 0.4791073501110077,\n",
       " 0.47380009293556213,\n",
       " 0.45227375626564026,\n",
       " 0.47381845116615295,\n",
       " 0.4631674885749817,\n",
       " 0.4737187623977661,\n",
       " 0.47387221455574036,\n",
       " 0.4947805404663086,\n",
       " 0.4524346888065338,\n",
       " 0.45238766074180603,\n",
       " 0.47383102774620056,\n",
       " 0.44741180539131165,\n",
       " 0.44741290807724,\n",
       " 0.4473857581615448,\n",
       " 0.45253613591194153,\n",
       " 0.447403222322464,\n",
       " 0.4739374816417694,\n",
       " 0.45236822962760925,\n",
       " 0.47378093004226685,\n",
       " 0.47911006212234497,\n",
       " 0.4738026559352875,\n",
       " 0.45242297649383545,\n",
       " 0.45241889357566833,\n",
       " 0.4737260341644287,\n",
       " 0.4738849997520447,\n",
       " 0.47368183732032776,\n",
       " 0.47373268008232117,\n",
       " 0.46322396397590637,\n",
       " 0.44742029905319214,\n",
       " 0.4523666799068451,\n",
       " 0.4525720775127411,\n",
       " 0.47910362482070923,\n",
       " 0.4738301932811737,\n",
       " 0.4842003285884857,\n",
       " 0.4631866216659546,\n",
       " 0.45237934589385986,\n",
       " 0.4738638401031494,\n",
       " 0.4631538391113281,\n",
       " 0.47377195954322815,\n",
       " 0.4522680342197418,\n",
       " 0.47912946343421936,\n",
       " 0.4791271388530731,\n",
       " 0.47370240092277527,\n",
       " 0.4738006293773651,\n",
       " 0.473833829164505,\n",
       " 0.4789488911628723,\n",
       " 0.46319910883903503,\n",
       " 0.45226678252220154,\n",
       " 0.47382834553718567,\n",
       " 0.4738396406173706,\n",
       " 0.45237892866134644,\n",
       " 0.47382867336273193,\n",
       " 0.47907450795173645,\n",
       " 0.47391048073768616,\n",
       " 0.4790484607219696,\n",
       " 0.45241233706474304,\n",
       " 0.48423638939857483,\n",
       " 0.4631679058074951,\n",
       " ...]"
      ]
     },
     "execution_count": 1168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[prob.item() for prob in calc_preds(coeffs, t_indep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "id": "abe6ff39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3870,  0.1283, -0.0261, -0.0653, -0.0117,  0.1363, -0.3976, -0.2811])"
      ]
     },
     "execution_count": 1161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "id": "e22a96b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0020)"
      ]
     },
     "execution_count": 1160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)>0.1)).float().mean()\n",
    "\n",
    "acc(coeffs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "id": "66dfff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install --quiet hyperopt\n",
    "# !pip install --quiet \"ray[tune]\"\n",
    "# !pip install --quiet scvi-colab\n",
    "# from scvi_colab import install\n",
    "\n",
    "# install()\n",
    "\n",
    "\n",
    "# import ray\n",
    "# import scanpy as sc\n",
    "# import scvi\n",
    "# from ray import tune\n",
    "# from scvi import autotune\n",
    "# model_cls = scvi.model.SCVI\n",
    "# model_cls.setup_anndata(adata)\n",
    "# scvi_tuner = autotune.ModelTuner(model_cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "id": "385450cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarkResults():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "id": "7a58542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scgen\n",
    "adata.obs.rename({\"label\": \"condition\"}, axis=1, inplace=True)\n",
    "adata.obs[\"condition\"].replace({\"ctrl\": \"control\", \"stim\": \"stimulated\"}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02605-1\n",
    "\n",
    "# adata_t = adata[\n",
    "#     ~(\n",
    "#         (adata.obs[\"cell_type\"] == \"CD4 T cells\")\n",
    "#         & (adata.obs[\"condition\"] == \"stimulated\")\n",
    "#     )\n",
    "# ].copy()\n",
    "\n",
    "# cd4t_stim = adata[\n",
    "#     (\n",
    "#         (adata.obs[\"cell_type\"] == \"CD4 T cells\")\n",
    "#         & (adata.obs[\"condition\"] == \"stimulated\")\n",
    "#     )\n",
    "# ].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# scgen.SCGEN.setup_anndata(adata_t, batch_key=\"condition\", labels_key=\"cell_type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "id": "05b5f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = scgen.SCGEN(adata_t, n_hidden=800, n_latent=100, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "214e901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train(\n",
    "#     max_epochs=1, batch_size=32, early_stopping=True, early_stopping_patience=25\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "4f6f2f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata_t.obsm[\"scgen\"] = model.get_latent_representation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "84889a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pp.neighbors(adata_t, use_rep=\"scgen\")\n",
    "# sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "5ec93614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "6a4a815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred, delta = model.predict(\n",
    "#     ctrl_key=\"control\", stim_key=\"stimulated\", celltype_to_predict=\"CD4 T cells\"\n",
    "# )\n",
    "\n",
    "# # we annotate the predicted cells to distinguish them later from ground truth cells.\n",
    "# pred.obs[\"condition\"] = \"prediction\"\n",
    "# print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c5ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "8d6ca81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "b7007635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train = adata\n",
    "# ctrl_adata = adata[\n",
    "#     ((adata.obs[\"cell_type\"] == \"CD4 T cells\") & (adata.obs[\"condition\"] == \"control\"))\n",
    "# ]\n",
    "# stim_adata = train[((train.obs['cell_type'] == 'CD4T') & (train.obs['condition'] == 'stimulated'))]\n",
    "\n",
    "# # concatenate pred, control and real CD4 T cells in to one object\n",
    "# eval_adata = ctrl_adata.concatenate(cd4t_stim, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "c1bc85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.tl.pca(eval_adata)\n",
    "# sc.pl.pca(eval_adata, color=\"condition\", frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "66b16e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd4t_adata = adata[adata.obs[\"cell_type\"] == \"CD4 T cells\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "3508e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.tl.rank_genes_groups(cd4t_adata, groupby=\"condition\", method=\"wilcoxon\")\n",
    "# diff_genes = cd4t_adata.uns[\"rank_genes_groups\"][\"names\"][\"stimulated\"]\n",
    "# diff_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f7e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e4f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_value = model.reg_mean_plot(\n",
    "#     eval_adata,\n",
    "#     axis_keys={\"x\": \"predicted stimulated\", \"y\": \"stimulated\"},\n",
    "#     gene_list=diff_genes[:10],\n",
    "#     top_100_genes=diff_genes,\n",
    "#     labels={\"x\": \"predicted\", \"y\": \"ground truth\"},\n",
    "#     show=True,\n",
    "#     legend=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c0260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pl.violin(eval_adata, keys=\"ISG15\", groupby=\"condition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pertpy as pt\n",
    "# import muon as mu\n",
    "# import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d4a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdata = pt.dt.papalexi_2021()\n",
    "# for col in mdata.obs: print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a027076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pp.normalize_total(mdata[\"rna\"])\n",
    "# sc.pp.log1p(mdata[\"rna\"])\n",
    "# sc.pp.highly_variable_genes(mdata[\"rna\"], subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ffac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu.prot.pp.clr(mdata[\"adt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe04851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pp.pca(mdata[\"rna\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5308a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate neighbors with the cosine distance similarly to the original Seurat implementation\n",
    "# sc.pp.neighbors(mdata[\"rna\"], metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0721f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.tl.umap(mdata[\"rna\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40747691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pl.umap(mdata[\"rna\"], color=[\"replicate\", \"Phase\", \"perturbation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcb4db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f25d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ms = pt.tl.Mixscape()\n",
    "\n",
    "# ms.perturbation_signature(\n",
    "#     mdata[\"rna\"],\n",
    "#     pert_key=\"perturbation\",\n",
    "#     control=\"NT\",\n",
    "#     split_by=\"replicate\",\n",
    "#     n_neighbors=20,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a copy of the object to recalculate the PCA.\n",
    "# Alternatively we could replace the X of the RNA part of our MuData object with the `X_pert` layer.\n",
    "# adata_pert = mdata[\"rna\"].copy()\n",
    "# adata_pert.X = adata_pert.layers[\"X_pert\"]\n",
    "# sc.pp.pca(adata_pert)\n",
    "# sc.pp.neighbors(adata_pert, metric=\"cosine\")\n",
    "# sc.tl.umap(adata_pert)\n",
    "# sc.pl.umap(adata_pert, color=[\"replicate\", \"Phase\", \"perturbation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3197d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ms.mixscape(adata=mdata[\"rna\"], control=\"NT\", labels=\"gene_target\", layer=\"X_pert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ffa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdata[\"rna\"].obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d8bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt.pl.ms.perturbscore(\n",
    "#     adata=mdata[\"rna\"], labels=\"gene_target\", target_gene=\"IFNGR2\", color=\"orange\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf75ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.settings.set_figure_params(figsize=(10, 10))\n",
    "# pt.pl.ms.violin(\n",
    "#     adata=mdata[\"rna\"],\n",
    "#     target_gene_idents=[\"NT\", \"IFNGR2 NP\", \"IFNGR2 KO\"],\n",
    "#     groupby=\"mixscape_class\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee147bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt.pl.ms.heatmap(\n",
    "#     adata=mdata[\"rna\"],\n",
    "#     labels=\"gene_target\",\n",
    "#     target_gene=\"IFNGR2\",\n",
    "#     layer=\"X_pert\",\n",
    "#     control=\"NT\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d44870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdata[\"adt\"].obs[\"mixscape_class_global\"] = mdata[\"rna\"].obs[\"mixscape_class_global\"]\n",
    "# pt.pl.ms.violin(\n",
    "#     adata=mdata[\"adt\"],\n",
    "#     target_gene_idents=[\"NT\", \"JAK2\", \"STAT1\", \"IFNGR1\", \"IFNGR2\", \"IRF1\"],\n",
    "#     keys=\"PDL1\",\n",
    "#     groupby=\"gene_target\",\n",
    "#     hue=\"mixscape_class_global\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4dfcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ms.lda(adata=mdata[\"rna\"], labels=\"gene_target\", layer=\"X_pert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b5cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt.pl.ms.lda(adata=mdata[\"rna\"])\n",
    "#https://zenodo.org/record/7058382\n",
    "# folders = '/home/awahab/llm-testing/data_sets/'\n",
    "# fp1 = 'SC3_v3_NextGem_DI_CRISPR_A549_5K_Multiplex_count_raw_feature_bc_matrix.h5'\n",
    "# fp2 = 'SC3_v3_NextGem_DI_CRISPR_A549_5K_Multiplex_count_raw_molecule_info.h5'\n",
    "\n",
    "\n",
    "# import h5py\n",
    "# import anndata\n",
    "\n",
    "# # Read the .h5 File\n",
    "# def explore_h5py_group(group, indent=0):\n",
    "#     \"\"\"Recursively print the contents of an h5py group/dataset.\"\"\"\n",
    "#     print(group.data.name)\n",
    "#     items = sorted(group.items())\n",
    "#     for name, item in items:\n",
    "#         if isinstance(item, h5py.Dataset):  # Check if item is a dataset\n",
    "#             print(\"  \" * indent + f\"Dataset: {name} (Shape: {item.shape}, Dtype: {item.dtype})\")\n",
    "#         elif isinstance(item, h5py.Group):  # Check if item is a group\n",
    "#             print(\"  \" * indent + f\"Group: {name}\")\n",
    "#             explore_h5py_group(item, indent + 1)  # Recursive call to explore subgroups\n",
    "\n",
    "# # Open your HDF5 file\n",
    "# with h5py.File(folders + fp1, 'r') as f:\n",
    "#     explore_h5py_group(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
